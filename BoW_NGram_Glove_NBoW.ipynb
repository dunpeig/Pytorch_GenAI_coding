{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "from typing import List, Union, Optional, Dict\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 1. BAG OF WORDS (BoW) - Enhanced Implementation\n",
        "# ============================================================================\n",
        "\n",
        "# Imagine you have a bag (like a shopping bag) and you throw all the words from a\n",
        "# document into it:\n",
        "\n",
        "# Word order is lost - you can't tell what came first\n",
        "# You can only count how many times each word appears\n",
        "# The \"bag\" represents the document as word frequencies\n",
        "\n",
        "class BagOfWords:\n",
        "    \"\"\"\n",
        "    Represents documents as word count vectors.\n",
        "\n",
        "    Features:\n",
        "    - Ignores word order completely (bag assumption)\n",
        "    - Supports binary (presence/absence) or count mode\n",
        "    - Vocabulary size limiting\n",
        "    - Proper handling of unknown words\n",
        "    - Dense or sparse output options\n",
        "\n",
        "    Example:\n",
        "        >>> bow = BagOfWords(max_features=1000, binary=False)\n",
        "        >>> docs = [[\"hello\", \"world\"], [\"hello\", \"python\"]]\n",
        "        >>> X = bow.fit_transform(docs)\n",
        "        >>> print(X.shape)  # (2, 3) - 2 docs, 3 unique words\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        binary: bool = False,\n",
        "        max_features: Optional[int] = None,\n",
        "        min_df: int = 1,\n",
        "        dtype: torch.dtype = torch.float32\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize Bag of Words vectorizer.\n",
        "\n",
        "        Args:\n",
        "            binary: If True, use 1/0 (presence/absence) instead of counts\n",
        "            max_features: Maximum vocabulary size (keeps most frequent words)\n",
        "            min_df: Minimum document frequency (ignore rare words)\n",
        "            dtype: PyTorch dtype for output tensors\n",
        "        \"\"\"\n",
        "        self.binary = binary\n",
        "        self.max_features = max_features\n",
        "        self.min_df = min_df\n",
        "        self.dtype = dtype\n",
        "\n",
        "        # Will be set during fit()\n",
        "        self.vocab: Dict[str, int] = {}\n",
        "        self.vocab_size_: int = 0\n",
        "        self.is_fitted_: bool = False\n",
        "\n",
        "    def fit(self, documents: List[List[str]]) -> 'BagOfWords':\n",
        "        \"\"\"\n",
        "        Learn vocabulary from training documents.\n",
        "\n",
        "        Args:\n",
        "            documents: List of tokenized documents (each doc is list of words)\n",
        "\n",
        "        Returns:\n",
        "            self (for method chaining)\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If documents is empty or invalid\n",
        "        \"\"\"\n",
        "        # Input validation\n",
        "        if not documents:\n",
        "            raise ValueError(\"Cannot fit on empty document list\")\n",
        "\n",
        "        if not all(isinstance(doc, (list, tuple)) for doc in documents):\n",
        "            raise ValueError(\"All documents must be lists or tuples of tokens\")\n",
        "\n",
        "        # Step 1: Count document frequencies (how many docs contain each word)\n",
        "        doc_freq = Counter()\n",
        "        for doc in documents:\n",
        "            unique_words = set(doc)  # Only count once per document\n",
        "            doc_freq.update(unique_words)\n",
        "\n",
        "        # Step 2: Filter by minimum document frequency\n",
        "        valid_words = {\n",
        "            word for word, freq in doc_freq.items()\n",
        "            if freq >= self.min_df\n",
        "        }\n",
        "\n",
        "        # Step 3: Limit vocabulary size if specified\n",
        "        if self.max_features is not None and len(valid_words) > self.max_features:\n",
        "            # Keep most frequent words\n",
        "            word_counts = Counter()\n",
        "            for doc in documents:\n",
        "                word_counts.update(doc)\n",
        "\n",
        "            # Get top max_features words\n",
        "            most_common = word_counts.most_common(self.max_features)\n",
        "            valid_words = {word for word, _ in most_common if word in valid_words}\n",
        "\n",
        "        # Step 4: Create vocabulary mapping (sorted for reproducibility)\n",
        "        self.vocab = {\n",
        "            word: idx\n",
        "            for idx, word in enumerate(sorted(valid_words))\n",
        "        }\n",
        "        self.vocab_size_ = len(self.vocab)\n",
        "        self.is_fitted_ = True\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, documents: List[List[str]]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Transform documents to BoW representation.\n",
        "\n",
        "        Args:\n",
        "            documents: List of tokenized documents\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape (n_documents, vocab_size)\n",
        "\n",
        "        Raises:\n",
        "            RuntimeError: If called before fit()\n",
        "            ValueError: If documents is invalid\n",
        "        \"\"\"\n",
        "        if not self.is_fitted_:\n",
        "            raise RuntimeError(\"BagOfWords must be fitted before transform()\")\n",
        "\n",
        "        if not documents:\n",
        "            raise ValueError(\"Cannot transform empty document list\")\n",
        "\n",
        "        # Initialize output matrix\n",
        "        bow_matrix = torch.zeros(\n",
        "            len(documents),\n",
        "            self.vocab_size_,\n",
        "            dtype=self.dtype\n",
        "        )\n",
        "\n",
        "        # Fill matrix\n",
        "        for doc_idx, doc in enumerate(documents):\n",
        "            if not doc:  # Handle empty documents\n",
        "                continue\n",
        "\n",
        "            counts = Counter(doc)\n",
        "\n",
        "            for word, count in counts.items():\n",
        "                # Only process words in vocabulary (ignore unknown words)\n",
        "                if word in self.vocab:\n",
        "                    word_idx = self.vocab[word]\n",
        "                    if self.binary:\n",
        "                        bow_matrix[doc_idx, word_idx] = 1.0\n",
        "                    else:\n",
        "                        bow_matrix[doc_idx, word_idx] = float(count)\n",
        "\n",
        "        return bow_matrix\n",
        "\n",
        "    def fit_transform(self, documents: List[List[str]]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Fit vocabulary and transform documents in one step.\n",
        "\n",
        "        Args:\n",
        "            documents: List of tokenized documents\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape (n_documents, vocab_size)\n",
        "        \"\"\"\n",
        "        return self.fit(documents).transform(documents)\n",
        "\n",
        "    def get_feature_names(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Get list of feature names (words in vocabulary).\n",
        "\n",
        "        Returns:\n",
        "            List of words, ordered by their index\n",
        "\n",
        "        Raises:\n",
        "            RuntimeError: If called before fit()\n",
        "        \"\"\"\n",
        "        if not self.is_fitted_:\n",
        "            raise RuntimeError(\"BagOfWords must be fitted before getting feature names\")\n",
        "\n",
        "        # Sort by index to get correct order\n",
        "        return [word for word, _ in sorted(self.vocab.items(), key=lambda x: x[1])]\n",
        "\n",
        "    def inverse_transform(self, bow_matrix: torch.Tensor) -> List[List[str]]:\n",
        "        \"\"\"\n",
        "        Convert BoW vectors back to approximate word lists.\n",
        "        Note: Order is lost, counts are preserved.\n",
        "\n",
        "        Args:\n",
        "            bow_matrix: Tensor of shape (n_documents, vocab_size)\n",
        "\n",
        "        Returns:\n",
        "            List of word lists (repeated according to counts)\n",
        "        \"\"\"\n",
        "        if not self.is_fitted_:\n",
        "            raise RuntimeError(\"BagOfWords must be fitted before inverse_transform()\")\n",
        "\n",
        "        idx_to_word = {idx: word for word, idx in self.vocab.items()}\n",
        "        documents = []\n",
        "\n",
        "        for doc_vector in bow_matrix:\n",
        "            words = []\n",
        "            for idx, count in enumerate(doc_vector):\n",
        "                if count > 0:\n",
        "                    word = idx_to_word[idx]\n",
        "                    # Repeat word according to count (or just once if binary)\n",
        "                    repetitions = 1 if self.binary else int(count.item())\n",
        "                    words.extend([word] * repetitions)\n",
        "            documents.append(words)\n",
        "\n",
        "        return documents\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        \"\"\"String representation.\"\"\"\n",
        "        params = f\"binary={self.binary}, max_features={self.max_features}, min_df={self.min_df}\"\n",
        "        if self.is_fitted_:\n",
        "            params += f\", vocab_size={self.vocab_size_}\"\n",
        "        return f\"BagOfWords({params})\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# DEMONSTRATION & TESTING\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# 1. PREPARE DATA (already tokenized)\n",
        "# train_docs = [\n",
        "#     [\"i\", \"love\", \"machine\", \"learning\"],\n",
        "#     [\"i\", \"love\", \"deep\", \"learning\"],\n",
        "#     [\"machine\", \"learning\", \"is\", \"fun\"],\n",
        "#     [\"deep\", \"learning\", \"is\", \"powerful\"]\n",
        "# ]\n",
        "\n",
        "# test_docs = [\n",
        "#     [\"i\", \"love\", \"learning\"],\n",
        "#     [\"quantum\", \"computing\"]  # New words!\n",
        "# ]\n",
        "\n",
        "# # 2. CREATE AND FIT MODEL\n",
        "# bow = BagOfWords(binary=False, max_features=5)\n",
        "# X_train = bow.fit_transform(train_docs)\n",
        "\n",
        "# print(\"Vocabulary:\", bow.get_feature_names())\n",
        "# # Output: ['deep', 'i', 'is', 'learning', 'love']\n",
        "# # (Only 5 words kept due to max_features=5)\n",
        "\n",
        "# print(\"Training matrix:\")\n",
        "# print(X_train)\n",
        "# # Output:\n",
        "# #         deep  i  is  learning  love\n",
        "# # Doc 0   [0    1   0    1        1]  \"i love machine learning\"\n",
        "# # Doc 1   [1    1   0    1        1]  \"i love deep learning\"\n",
        "# # Doc 2   [0    0   1    1        0]  \"machine learning is fun\"\n",
        "# # Doc 3   [1    0   1    1        0]  \"deep learning is powerful\"\n",
        "\n",
        "# # 3. TRANSFORM NEW DATA\n",
        "# X_test = bow.transform(test_docs)\n",
        "# print(\"Test matrix:\")\n",
        "# print(X_test)\n",
        "# # Output:\n",
        "# #         deep  i  is  learning  love\n",
        "# # Doc 0   [0    1   0    1        1]  \"i love learning\" ✓\n",
        "# # Doc 1   [0    0   0    0        0]  \"quantum computing\" (all unknown!)\n",
        "\n",
        "\n",
        "# Limitations of BoW\n",
        "# 1. Word Order Lost\n",
        "# python\"not good\" vs \"good not\" → SAME vector!\n",
        "# \"dog bites man\" vs \"man bites dog\" → SAME vector!\n",
        "# 2. No Semantic Understanding\n",
        "# python\"car\" and \"automobile\" → Treated as completely different words\n",
        "# \"happy\" and \"joyful\" → No relationship captured\n",
        "# 3. Sparse High-Dimensional Vectors\n",
        "# pythonVocabulary size: 10,000 words\n",
        "# Most documents use: 50-100 words\n",
        "# Result: 99% of vector is zeros! (very sparse)\n",
        "# 4. Context Ignored\n",
        "# python\"bank\" in \"river bank\" vs \"bank account\" → Same representation!\n",
        "\n",
        "\n",
        "# When to Use BoW\n",
        "# ✅ Use BoW for:\n",
        "\n",
        "# Quick baseline models\n",
        "# Text classification (spam detection, sentiment analysis)\n",
        "# Document similarity with simple metrics\n",
        "# Small to medium vocabularies (<10,000 words)\n",
        "# When word order doesn't matter much\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 70)\n",
        "    print(\"BAG OF WORDS DEMONSTRATION\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    # Sample documents (already tokenized)\n",
        "    train_docs = [\n",
        "        [\"cat\", \"dog\", \"dog\"],\n",
        "        [\"cat\", \"bird\"],\n",
        "        [\"dog\", \"bird\", \"bird\"],\n",
        "        [\"fish\"]\n",
        "    ]\n",
        "\n",
        "    test_docs = [\n",
        "        [\"cat\", \"cat\", \"dog\"],\n",
        "        [\"bird\", \"fish\"],\n",
        "        [\"unknown\", \"word\", \"test\"]  # Contains unknown words\n",
        "    ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Test 1: Basic count mode\n",
        "    print(\"\\n1. COUNT MODE (default)\")\n",
        "    print(\"-\" * 70)\n",
        "    bow_count = BagOfWords()\n",
        "    X_train = bow_count.fit_transform(train_docs)\n",
        "    X_test = bow_count.transform(test_docs)\n",
        "\n",
        "    print(f\"Vocabulary: {bow_count.get_feature_names()}\")\n",
        "    print(f\"Training matrix shape: {X_train.shape}\")\n",
        "    print(f\"Training matrix:\\n{X_train}\")\n",
        "    print(f\"\\nTest matrix:\\n{X_test}\")\n",
        "\n",
        "    # Test 2: Binary mode\n",
        "    print(\"\\n2. BINARY MODE (presence/absence)\")\n",
        "    print(\"-\" * 70)\n",
        "    bow_binary = BagOfWords(binary=True)\n",
        "    X_binary = bow_binary.fit_transform(train_docs)\n",
        "    print(f\"Binary matrix:\\n{X_binary}\")\n",
        "\n",
        "    # Test 3: Max features\n",
        "    print(\"\\n3. MAX FEATURES (vocabulary size limit)\")\n",
        "    print(\"-\" * 70)\n",
        "    bow_limited = BagOfWords(max_features=3)\n",
        "    X_limited = bow_limited.fit_transform(train_docs)\n",
        "    print(f\"Limited vocabulary: {bow_limited.get_feature_names()}\")\n",
        "    print(f\"Limited matrix:\\n{X_limited}\")\n",
        "\n",
        "    # Test 4: Min document frequency\n",
        "    print(\"\\n4. MIN DOCUMENT FREQUENCY (filter rare words)\")\n",
        "    print(\"-\" * 70)\n",
        "    bow_filtered = BagOfWords(min_df=2)  # Word must appear in at least 2 docs\n",
        "    X_filtered = bow_filtered.fit_transform(train_docs)\n",
        "    print(f\"Filtered vocabulary: {bow_filtered.get_feature_names()}\")\n",
        "    print(f\"Filtered matrix:\\n{X_filtered}\")\n",
        "\n",
        "    # Test 5: Inverse transform\n",
        "    print(\"\\n5. INVERSE TRANSFORM (reconstruct documents)\")\n",
        "    print(\"-\" * 70)\n",
        "    reconstructed = bow_count.inverse_transform(X_train)\n",
        "    print(\"Original:      \", train_docs[0])\n",
        "    print(\"Reconstructed: \", reconstructed[0])\n",
        "    print(\"Note: Order is lost, but counts are preserved\")\n",
        "\n",
        "    # Test 6: Edge cases\n",
        "    print(\"\\n6. EDGE CASES\")\n",
        "    print(\"-\" * 70)\n",
        "    bow_edge = BagOfWords()\n",
        "\n",
        "    # Empty document\n",
        "    docs_with_empty = [[\"hello\", \"world\"], [], [\"test\"]]\n",
        "    X_edge = bow_edge.fit_transform(docs_with_empty)\n",
        "    print(f\"Documents with empty doc: {docs_with_empty}\")\n",
        "    print(f\"Result:\\n{X_edge}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"All tests completed successfully!\")\n",
        "    print(\"=\" * 70)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gL-j_ZQjghII",
        "outputId": "6a40004e-a85e-4582-f357-f25d9b9295f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "BAG OF WORDS DEMONSTRATION\n",
            "======================================================================\n",
            "\n",
            "1. COUNT MODE (default)\n",
            "----------------------------------------------------------------------\n",
            "Vocabulary: ['bird', 'cat', 'dog', 'fish']\n",
            "Training matrix shape: torch.Size([4, 4])\n",
            "Training matrix:\n",
            "tensor([[0., 1., 2., 0.],\n",
            "        [1., 1., 0., 0.],\n",
            "        [2., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.]])\n",
            "\n",
            "Test matrix:\n",
            "tensor([[0., 2., 1., 0.],\n",
            "        [1., 0., 0., 1.],\n",
            "        [0., 0., 0., 0.]])\n",
            "\n",
            "2. BINARY MODE (presence/absence)\n",
            "----------------------------------------------------------------------\n",
            "Binary matrix:\n",
            "tensor([[0., 1., 1., 0.],\n",
            "        [1., 1., 0., 0.],\n",
            "        [1., 0., 1., 0.],\n",
            "        [0., 0., 0., 1.]])\n",
            "\n",
            "3. MAX FEATURES (vocabulary size limit)\n",
            "----------------------------------------------------------------------\n",
            "Limited vocabulary: ['bird', 'cat', 'dog']\n",
            "Limited matrix:\n",
            "tensor([[0., 1., 2.],\n",
            "        [1., 1., 0.],\n",
            "        [2., 0., 1.],\n",
            "        [0., 0., 0.]])\n",
            "\n",
            "4. MIN DOCUMENT FREQUENCY (filter rare words)\n",
            "----------------------------------------------------------------------\n",
            "Filtered vocabulary: ['bird', 'cat', 'dog']\n",
            "Filtered matrix:\n",
            "tensor([[0., 1., 2.],\n",
            "        [1., 1., 0.],\n",
            "        [2., 0., 1.],\n",
            "        [0., 0., 0.]])\n",
            "\n",
            "5. INVERSE TRANSFORM (reconstruct documents)\n",
            "----------------------------------------------------------------------\n",
            "Original:       ['cat', 'dog', 'dog']\n",
            "Reconstructed:  ['cat', 'dog', 'dog']\n",
            "Note: Order is lost, but counts are preserved\n",
            "\n",
            "6. EDGE CASES\n",
            "----------------------------------------------------------------------\n",
            "Documents with empty doc: [['hello', 'world'], [], ['test']]\n",
            "Result:\n",
            "tensor([[1., 0., 1.],\n",
            "        [0., 0., 0.],\n",
            "        [0., 1., 0.]])\n",
            "\n",
            "======================================================================\n",
            "All tests completed successfully!\n",
            "======================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Union, Optional, Dict, Set\n",
        "import itertools\n",
        "\n",
        "# ============================================================================\n",
        "# 2. N-GRAMS - Enhanced Implementation with Local Word Order\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# N-grams are sequences of N consecutive tokens (words or characters). Unlike Bag\n",
        "# of Words, they capture local word order.\n",
        "\n",
        "\n",
        "class NGramVectorizer:\n",
        "    \"\"\"\n",
        "    Extract n-grams (sequences of n consecutive words/characters).\n",
        "    Captures local word order unlike basic Bag of Words.\n",
        "\n",
        "    Features:\n",
        "    - Support for multiple n-gram ranges (e.g., unigrams + bigrams)\n",
        "    - Word-level or character-level n-grams\n",
        "    - Binary or count mode\n",
        "    - Vocabulary size limiting\n",
        "    - Proper boundary handling\n",
        "\n",
        "    Examples:\n",
        "        >>> # Bigrams only\n",
        "        >>> ngram = NGramVectorizer(n=2)\n",
        "        >>> docs = [[\"the\", \"cat\", \"sat\"], [\"the\", \"dog\", \"ran\"]]\n",
        "        >>> X = ngram.fit_transform(docs)\n",
        "\n",
        "        >>> # Unigrams + Bigrams + Trigrams\n",
        "        >>> ngram = NGramVectorizer(ngram_range=(1, 3))\n",
        "        >>> X = ngram.fit_transform(docs)\n",
        "\n",
        "        >>> # Character-level n-grams\n",
        "        >>> ngram = NGramVectorizer(n=3, analyzer='char')\n",
        "        >>> docs = [\"hello\", \"world\"]\n",
        "        >>> X = ngram.fit_transform(docs)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n: Optional[int] = None,\n",
        "        ngram_range: Optional[Tuple[int, int]] = None,\n",
        "        analyzer: str = 'word',\n",
        "        binary: bool = False,\n",
        "        max_features: Optional[int] = None,\n",
        "        min_df: int = 1,\n",
        "        token_separator: str = ' ',\n",
        "        dtype: torch.dtype = torch.float32\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize N-Gram Vectorizer.\n",
        "\n",
        "        Args:\n",
        "            n: Size of n-grams (2=bigrams, 3=trigrams).\n",
        "               Cannot be used with ngram_range.\n",
        "            ngram_range: Tuple (min_n, max_n) for multiple n-gram sizes.\n",
        "                        E.g., (1,2) = unigrams + bigrams.\n",
        "                        Cannot be used with n.\n",
        "            analyzer: 'word' for word-level or 'char' for character-level n-grams\n",
        "            binary: If True, use presence/absence instead of counts\n",
        "            max_features: Maximum vocabulary size (keeps most frequent n-grams)\n",
        "            min_df: Minimum document frequency (ignore rare n-grams)\n",
        "            token_separator: String to join tokens in n-gram representation\n",
        "            dtype: PyTorch dtype for output tensors\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If both n and ngram_range are specified, or neither\n",
        "        \"\"\"\n",
        "        # Validate parameters\n",
        "        if n is not None and ngram_range is not None:\n",
        "            raise ValueError(\"Cannot specify both 'n' and 'ngram_range'\")\n",
        "        if n is None and ngram_range is None:\n",
        "            raise ValueError(\"Must specify either 'n' or 'ngram_range'\")\n",
        "\n",
        "        # Set n-gram range\n",
        "        if n is not None:\n",
        "            self.ngram_range = (n, n)\n",
        "        else:\n",
        "            self.ngram_range = ngram_range\n",
        "\n",
        "        if self.ngram_range[0] < 1:\n",
        "            raise ValueError(f\"min n must be >= 1, got {self.ngram_range[0]}\")\n",
        "        if self.ngram_range[0] > self.ngram_range[1]:\n",
        "            raise ValueError(f\"min n ({self.ngram_range[0]}) must be <= max n ({self.ngram_range[1]})\")\n",
        "\n",
        "        if analyzer not in ['word', 'char']:\n",
        "            raise ValueError(f\"analyzer must be 'word' or 'char', got '{analyzer}'\")\n",
        "\n",
        "        self.analyzer = analyzer\n",
        "        self.binary = binary\n",
        "        self.max_features = max_features\n",
        "        self.min_df = min_df\n",
        "        self.token_separator = token_separator\n",
        "        self.dtype = dtype\n",
        "\n",
        "        # Will be set during fit()\n",
        "        self.vocab: Dict[str, int] = {}\n",
        "        self.vocab_size_: int = 0\n",
        "        self.is_fitted_: bool = False\n",
        "\n",
        "    def _get_ngrams(self, tokens: Union[List[str], str]) -> List[str]:\n",
        "        \"\"\"\n",
        "        Extract n-grams from token list or string.\n",
        "\n",
        "        Args:\n",
        "            tokens: List of words or string (for character n-grams)\n",
        "\n",
        "        Returns:\n",
        "            List of n-gram strings\n",
        "        \"\"\"\n",
        "        ngrams = []\n",
        "        min_n, max_n = self.ngram_range\n",
        "\n",
        "        # Handle character-level n-grams\n",
        "        if self.analyzer == 'char':\n",
        "            # Convert to string if needed\n",
        "            if isinstance(tokens, list):\n",
        "                tokens = ''.join(tokens)\n",
        "\n",
        "            for n in range(min_n, max_n + 1):\n",
        "                for i in range(len(tokens) - n + 1):\n",
        "                    ngram = tokens[i:i+n]\n",
        "                    ngrams.append(ngram)\n",
        "\n",
        "        # Handle word-level n-grams\n",
        "        else:\n",
        "            if isinstance(tokens, str):\n",
        "                raise ValueError(\"For word analyzer, input must be list of tokens\")\n",
        "\n",
        "            for n in range(min_n, max_n + 1):\n",
        "                # Skip if document is too short for this n\n",
        "                if len(tokens) < n:\n",
        "                    continue\n",
        "\n",
        "                for i in range(len(tokens) - n + 1):\n",
        "                    ngram = self.token_separator.join(tokens[i:i+n])\n",
        "                    ngrams.append(ngram)\n",
        "\n",
        "        return ngrams\n",
        "\n",
        "    def fit(self, documents: Union[List[List[str]], List[str]]) -> 'NGramVectorizer':\n",
        "        \"\"\"\n",
        "        Learn vocabulary from training documents.\n",
        "\n",
        "        Args:\n",
        "            documents: List of tokenized documents (word analyzer) or\n",
        "                      list of strings (char analyzer)\n",
        "\n",
        "        Returns:\n",
        "            self (for method chaining)\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If documents is empty or invalid\n",
        "        \"\"\"\n",
        "        if not documents:\n",
        "            raise ValueError(\"Cannot fit on empty document list\")\n",
        "\n",
        "        # Validate input format\n",
        "        if self.analyzer == 'word':\n",
        "            if not all(isinstance(doc, (list, tuple)) for doc in documents):\n",
        "                raise ValueError(\"For word analyzer, documents must be lists of tokens\")\n",
        "        elif self.analyzer == 'char':\n",
        "            if not all(isinstance(doc, str) for doc in documents):\n",
        "                raise ValueError(\"For char analyzer, documents must be strings\")\n",
        "\n",
        "        # Step 1: Count document frequencies\n",
        "        doc_freq = Counter()\n",
        "        all_ngrams = []  # For total frequency counting\n",
        "\n",
        "        for doc in documents:\n",
        "            ngrams = self._get_ngrams(doc)\n",
        "            unique_ngrams = set(ngrams)\n",
        "            doc_freq.update(unique_ngrams)\n",
        "            all_ngrams.extend(ngrams)\n",
        "\n",
        "        # Step 2: Filter by minimum document frequency\n",
        "        valid_ngrams = {\n",
        "            ngram for ngram, freq in doc_freq.items()\n",
        "            if freq >= self.min_df\n",
        "        }\n",
        "\n",
        "        # Step 3: Limit vocabulary size if specified\n",
        "        if self.max_features is not None and len(valid_ngrams) > self.max_features:\n",
        "            # Count total frequencies\n",
        "            ngram_counts = Counter(all_ngrams)\n",
        "\n",
        "            # Keep only valid n-grams\n",
        "            valid_counts = {\n",
        "                ngram: count for ngram, count in ngram_counts.items()\n",
        "                if ngram in valid_ngrams\n",
        "            }\n",
        "\n",
        "            # Get top max_features\n",
        "            most_common = Counter(valid_counts).most_common(self.max_features)\n",
        "            valid_ngrams = {ngram for ngram, _ in most_common}\n",
        "\n",
        "        # Step 4: Create vocabulary mapping (sorted for reproducibility)\n",
        "        self.vocab = {\n",
        "            ngram: idx\n",
        "            for idx, ngram in enumerate(sorted(valid_ngrams))\n",
        "        }\n",
        "        self.vocab_size_ = len(self.vocab)\n",
        "        self.is_fitted_ = True\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, documents: Union[List[List[str]], List[str]]) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Transform documents to n-gram representation.\n",
        "\n",
        "        Args:\n",
        "            documents: List of tokenized documents or strings\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape (n_documents, vocab_size)\n",
        "\n",
        "        Raises:\n",
        "            RuntimeError: If called before fit()\n",
        "            ValueError: If documents is invalid\n",
        "        \"\"\"\n",
        "        if not self.is_fitted_:\n",
        "            raise RuntimeError(\"NGramVectorizer must be fitted before transform()\")\n",
        "\n",
        "        if not documents:\n",
        "            raise ValueError(\"Cannot transform empty document list\")\n",
        "\n",
        "        # Initialize output matrix\n",
        "        ngram_matrix = torch.zeros(\n",
        "            len(documents),\n",
        "            self.vocab_size_,\n",
        "            dtype=self.dtype\n",
        "        )\n",
        "\n",
        "        # Fill matrix\n",
        "        for doc_idx, doc in enumerate(documents):\n",
        "            # Handle empty documents\n",
        "            if not doc:\n",
        "                continue\n",
        "\n",
        "            ngrams = self._get_ngrams(doc)\n",
        "            counts = Counter(ngrams)\n",
        "\n",
        "            for ngram, count in counts.items():\n",
        "                # Only process n-grams in vocabulary\n",
        "                if ngram in self.vocab:\n",
        "                    ngram_idx = self.vocab[ngram]\n",
        "                    if self.binary:\n",
        "                        ngram_matrix[doc_idx, ngram_idx] = 1.0\n",
        "                    else:\n",
        "                        ngram_matrix[doc_idx, ngram_idx] = float(count)\n",
        "\n",
        "        return ngram_matrix\n",
        "\n",
        "    def fit_transform(\n",
        "        self,\n",
        "        documents: Union[List[List[str]], List[str]]\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Fit vocabulary and transform documents in one step.\n",
        "\n",
        "        Args:\n",
        "            documents: List of tokenized documents or strings\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape (n_documents, vocab_size)\n",
        "        \"\"\"\n",
        "        return self.fit(documents).transform(documents)\n",
        "\n",
        "    def get_feature_names(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Get list of feature names (n-grams in vocabulary).\n",
        "\n",
        "        Returns:\n",
        "            List of n-gram strings, ordered by their index\n",
        "\n",
        "        Raises:\n",
        "            RuntimeError: If called before fit()\n",
        "        \"\"\"\n",
        "        if not self.is_fitted_:\n",
        "            raise RuntimeError(\"NGramVectorizer must be fitted before getting feature names\")\n",
        "\n",
        "        return [ngram for ngram, _ in sorted(self.vocab.items(), key=lambda x: x[1])]\n",
        "\n",
        "    def inverse_transform(self, ngram_matrix: torch.Tensor) -> List[List[str]]:\n",
        "        \"\"\"\n",
        "        Convert n-gram vectors back to approximate token lists.\n",
        "        Note: Original text cannot be perfectly reconstructed from n-grams.\n",
        "\n",
        "        Args:\n",
        "            ngram_matrix: Tensor of shape (n_documents, vocab_size)\n",
        "\n",
        "        Returns:\n",
        "            List of n-gram lists\n",
        "        \"\"\"\n",
        "        if not self.is_fitted_:\n",
        "            raise RuntimeError(\"NGramVectorizer must be fitted before inverse_transform()\")\n",
        "\n",
        "        idx_to_ngram = {idx: ngram for ngram, idx in self.vocab.items()}\n",
        "        documents = []\n",
        "\n",
        "        for doc_vector in ngram_matrix:\n",
        "            ngrams = []\n",
        "            for idx, count in enumerate(doc_vector):\n",
        "                if count > 0:\n",
        "                    ngram = idx_to_ngram[idx]\n",
        "                    repetitions = 1 if self.binary else int(count.item())\n",
        "                    ngrams.extend([ngram] * repetitions)\n",
        "            documents.append(ngrams)\n",
        "\n",
        "        return documents\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        \"\"\"String representation.\"\"\"\n",
        "        params = (\n",
        "            f\"ngram_range={self.ngram_range}, \"\n",
        "            f\"analyzer='{self.analyzer}', \"\n",
        "            f\"binary={self.binary}, \"\n",
        "            f\"max_features={self.max_features}, \"\n",
        "            f\"min_df={self.min_df}\"\n",
        "        )\n",
        "        if self.is_fitted_:\n",
        "            params += f\", vocab_size={self.vocab_size_}\"\n",
        "        return f\"NGramVectorizer({params})\"\n",
        "\n",
        "\n",
        "\n",
        "# N-grams improve upon Bag of Words by:\n",
        "\n",
        "# ✅ Capturing local word order\n",
        "# ✅ Representing phrases and idioms\n",
        "# ✅ Distinguishing word sequences\n",
        "\n",
        "# But with trade-offs:\n",
        "\n",
        "# ❌ Much larger vocabulary (exponential growth)\n",
        "# ❌ Still no long-range dependencies\n",
        "# ❌ Still no semantic understanding\n",
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# DEMONSTRATION & TESTING\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 80)\n",
        "    print(\"N-GRAM VECTORIZER DEMONSTRATION\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Sample documents (already tokenized)\n",
        "    train_docs = [\n",
        "        [\"the\", \"cat\", \"sat\", \"on\", \"mat\"],\n",
        "        [\"the\", \"dog\", \"sat\", \"on\", \"log\"],\n",
        "        [\"the\", \"cat\", \"and\", \"dog\"],\n",
        "    ]\n",
        "\n",
        "    test_docs = [\n",
        "        [\"the\", \"cat\", \"sat\"],\n",
        "        [\"dog\", \"on\", \"mat\"]\n",
        "    ]\n",
        "\n",
        "    # ========================================================================\n",
        "    # Test 1: Basic Bigrams\n",
        "    # ========================================================================\n",
        "    print(\"\\n1. BIGRAMS ONLY (n=2)\")\n",
        "    print(\"-\" * 80)\n",
        "    bigram = NGramVectorizer(n=2)\n",
        "    X_train = bigram.fit_transform(train_docs)\n",
        "    X_test = bigram.transform(test_docs)\n",
        "\n",
        "    print(f\"Vocabulary ({bigram.vocab_size_} bigrams):\")\n",
        "    for i, ngram in enumerate(bigram.get_feature_names()):\n",
        "        print(f\"  [{i}] '{ngram}'\")\n",
        "\n",
        "    print(f\"\\nTraining matrix shape: {X_train.shape}\")\n",
        "    print(\"Training matrix:\")\n",
        "    print(X_train)\n",
        "\n",
        "    print(\"\\nExample: Document 'the cat sat on mat'\")\n",
        "    print(\"Bigrams: ['the cat', 'cat sat', 'sat on', 'on mat']\")\n",
        "    print(f\"Vector: {X_train[0]}\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # Test 2: Unigrams + Bigrams (captures both individual words and pairs)\n",
        "    # ========================================================================\n",
        "    print(\"\\n2. UNIGRAMS + BIGRAMS (ngram_range=(1,2))\")\n",
        "    print(\"-\" * 80)\n",
        "    combined = NGramVectorizer(ngram_range=(1, 2))\n",
        "    X_combined = combined.fit_transform(train_docs)\n",
        "\n",
        "    print(f\"Vocabulary ({combined.vocab_size_} features):\")\n",
        "    features = combined.get_feature_names()\n",
        "    unigrams = [f for f in features if ' ' not in f]\n",
        "    bigrams = [f for f in features if ' ' in f]\n",
        "    print(f\"  Unigrams ({len(unigrams)}): {unigrams[:5]}...\")\n",
        "    print(f\"  Bigrams ({len(bigrams)}): {bigrams[:5]}...\")\n",
        "\n",
        "    print(f\"\\nCombined matrix shape: {X_combined.shape}\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # Test 3: Trigrams (3-word sequences)\n",
        "    # ========================================================================\n",
        "    print(\"\\n3. TRIGRAMS (n=3)\")\n",
        "    print(\"-\" * 80)\n",
        "    trigram = NGramVectorizer(n=3)\n",
        "    X_trigram = trigram.fit_transform(train_docs)\n",
        "\n",
        "    print(f\"Vocabulary ({trigram.vocab_size_} trigrams):\")\n",
        "    for ngram in trigram.get_feature_names():\n",
        "        print(f\"  '{ngram}'\")\n",
        "\n",
        "    print(\"\\nTrigram matrix:\")\n",
        "    print(X_trigram)\n",
        "\n",
        "    # ========================================================================\n",
        "    # Test 4: Character-level n-grams\n",
        "    # ========================================================================\n",
        "    print(\"\\n4. CHARACTER-LEVEL N-GRAMS (analyzer='char', n=3)\")\n",
        "    print(\"-\" * 80)\n",
        "    char_docs = [\"hello\", \"helloworld\", \"world\"]\n",
        "\n",
        "    char_ngram = NGramVectorizer(n=3, analyzer='char')\n",
        "    X_char = char_ngram.fit_transform(char_docs)\n",
        "\n",
        "    print(f\"Vocabulary ({char_ngram.vocab_size_} character trigrams):\")\n",
        "    for ngram in char_ngram.get_feature_names():\n",
        "        print(f\"  '{ngram}'\")\n",
        "\n",
        "    print(\"\\nCharacter n-gram matrix:\")\n",
        "    print(X_char)\n",
        "\n",
        "    print(\"\\nExample: 'hello' character trigrams:\")\n",
        "    print(\"  ['hel', 'ell', 'llo']\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # Test 5: Binary mode (presence/absence)\n",
        "    # ========================================================================\n",
        "    print(\"\\n5. BINARY MODE (binary=True)\")\n",
        "    print(\"-\" * 80)\n",
        "    docs_repeated = [\n",
        "        [\"cat\", \"cat\", \"dog\"],\n",
        "        [\"cat\", \"dog\", \"dog\", \"dog\"]\n",
        "    ]\n",
        "\n",
        "    binary_ngram = NGramVectorizer(n=2, binary=True)\n",
        "    X_binary = binary_ngram.fit_transform(docs_repeated)\n",
        "\n",
        "    print(\"Documents with repeated n-grams:\")\n",
        "    for i, doc in enumerate(docs_repeated):\n",
        "        print(f\"  Doc {i}: {doc}\")\n",
        "\n",
        "    print(f\"\\nVocabulary: {binary_ngram.get_feature_names()}\")\n",
        "    print(\"Binary matrix (1 = present, 0 = absent):\")\n",
        "    print(X_binary)\n",
        "\n",
        "    # ========================================================================\n",
        "    # Test 6: Vocabulary limiting\n",
        "    # ========================================================================\n",
        "    print(\"\\n6. VOCABULARY LIMITING (max_features=3)\")\n",
        "    print(\"-\" * 80)\n",
        "    limited = NGramVectorizer(n=2, max_features=3)\n",
        "    X_limited = limited.fit_transform(train_docs)\n",
        "\n",
        "    print(f\"All possible bigrams from training: ~{bigram.vocab_size_}\")\n",
        "    print(f\"Limited vocabulary (top 3): {limited.get_feature_names()}\")\n",
        "    print(f\"Limited matrix shape: {X_limited.shape}\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # Test 7: Comparison with Bag of Words\n",
        "    # ========================================================================\n",
        "    print(\"\\n7. COMPARISON: BAG OF WORDS vs N-GRAMS\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    test_doc = [\"not\", \"good\"]\n",
        "    opposite_doc = [\"good\", \"not\"]  # Same words, different order\n",
        "\n",
        "    # Unigrams (equivalent to BoW)\n",
        "    unigram = NGramVectorizer(n=1)\n",
        "    unigram.fit([test_doc, opposite_doc])\n",
        "\n",
        "    vec1 = unigram.transform([test_doc])\n",
        "    vec2 = unigram.transform([opposite_doc])\n",
        "\n",
        "    print(f\"Document 1: {test_doc}\")\n",
        "    print(f\"Document 2: {opposite_doc}\")\n",
        "    print(f\"\\nUnigrams (BoW) - SAME vectors:\")\n",
        "    print(f\"  Doc 1: {vec1[0]}\")\n",
        "    print(f\"  Doc 2: {vec2[0]}\")\n",
        "    print(f\"  Equal? {torch.equal(vec1, vec2)}\")\n",
        "\n",
        "    # Bigrams (captures order)\n",
        "    bigram = NGramVectorizer(n=2)\n",
        "    bigram.fit([test_doc, opposite_doc])\n",
        "\n",
        "    vec1_bi = bigram.transform([test_doc])\n",
        "    vec2_bi = bigram.transform([opposite_doc])\n",
        "\n",
        "    print(f\"\\nBigrams - DIFFERENT vectors:\")\n",
        "    print(f\"  Vocabulary: {bigram.get_feature_names()}\")\n",
        "    print(f\"  Doc 1: {vec1_bi[0]}\")\n",
        "    print(f\"  Doc 2: {vec2_bi[0]}\")\n",
        "    print(f\"  Equal? {torch.equal(vec1_bi, vec2_bi)}\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # Test 8: Edge cases\n",
        "    # ========================================================================\n",
        "    print(\"\\n8. EDGE CASES\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Document shorter than n\n",
        "    short_docs = [[\"hi\"], [\"hello\", \"world\"], [\"a\", \"b\", \"c\"]]\n",
        "    safe_ngram = NGramVectorizer(n=2)\n",
        "    X_safe = safe_ngram.fit_transform(short_docs)\n",
        "\n",
        "    print(f\"Documents: {short_docs}\")\n",
        "    print(f\"Bigrams vocabulary: {safe_ngram.get_feature_names()}\")\n",
        "    print(f\"Matrix (note: 'hi' produces no bigrams):\")\n",
        "    print(X_safe)\n",
        "\n",
        "    # Empty document\n",
        "    docs_with_empty = [[\"hello\", \"world\"], [], [\"test\"]]\n",
        "    ngram_empty = NGramVectorizer(n=2)\n",
        "    X_empty = ngram_empty.fit_transform(docs_with_empty)\n",
        "    print(f\"\\nDocuments with empty: {docs_with_empty}\")\n",
        "    print(f\"Result matrix:\")\n",
        "    print(X_empty)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"All tests completed successfully!\")\n",
        "    print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cuvAWov2jsbh",
        "outputId": "4856e148-83a4-44ab-9d79-8318862594d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "N-GRAM VECTORIZER DEMONSTRATION\n",
            "================================================================================\n",
            "\n",
            "1. BIGRAMS ONLY (n=2)\n",
            "--------------------------------------------------------------------------------\n",
            "Vocabulary (9 bigrams):\n",
            "  [0] 'and dog'\n",
            "  [1] 'cat and'\n",
            "  [2] 'cat sat'\n",
            "  [3] 'dog sat'\n",
            "  [4] 'on log'\n",
            "  [5] 'on mat'\n",
            "  [6] 'sat on'\n",
            "  [7] 'the cat'\n",
            "  [8] 'the dog'\n",
            "\n",
            "Training matrix shape: torch.Size([3, 9])\n",
            "Training matrix:\n",
            "tensor([[0., 0., 1., 0., 0., 1., 1., 1., 0.],\n",
            "        [0., 0., 0., 1., 1., 0., 1., 0., 1.],\n",
            "        [1., 1., 0., 0., 0., 0., 0., 1., 0.]])\n",
            "\n",
            "Example: Document 'the cat sat on mat'\n",
            "Bigrams: ['the cat', 'cat sat', 'sat on', 'on mat']\n",
            "Vector: tensor([0., 0., 1., 0., 0., 1., 1., 1., 0.])\n",
            "\n",
            "2. UNIGRAMS + BIGRAMS (ngram_range=(1,2))\n",
            "--------------------------------------------------------------------------------\n",
            "Vocabulary (17 features):\n",
            "  Unigrams (8): ['and', 'cat', 'dog', 'log', 'mat']...\n",
            "  Bigrams (9): ['and dog', 'cat and', 'cat sat', 'dog sat', 'on log']...\n",
            "\n",
            "Combined matrix shape: torch.Size([3, 17])\n",
            "\n",
            "3. TRIGRAMS (n=3)\n",
            "--------------------------------------------------------------------------------\n",
            "Vocabulary (8 trigrams):\n",
            "  'cat and dog'\n",
            "  'cat sat on'\n",
            "  'dog sat on'\n",
            "  'sat on log'\n",
            "  'sat on mat'\n",
            "  'the cat and'\n",
            "  'the cat sat'\n",
            "  'the dog sat'\n",
            "\n",
            "Trigram matrix:\n",
            "tensor([[0., 1., 0., 0., 1., 0., 1., 0.],\n",
            "        [0., 0., 1., 1., 0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0., 0., 1., 0., 0.]])\n",
            "\n",
            "4. CHARACTER-LEVEL N-GRAMS (analyzer='char', n=3)\n",
            "--------------------------------------------------------------------------------\n",
            "Vocabulary (8 character trigrams):\n",
            "  'ell'\n",
            "  'hel'\n",
            "  'llo'\n",
            "  'low'\n",
            "  'orl'\n",
            "  'owo'\n",
            "  'rld'\n",
            "  'wor'\n",
            "\n",
            "Character n-gram matrix:\n",
            "tensor([[1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 1., 1., 1.],\n",
            "        [0., 0., 0., 0., 1., 0., 1., 1.]])\n",
            "\n",
            "Example: 'hello' character trigrams:\n",
            "  ['hel', 'ell', 'llo']\n",
            "\n",
            "5. BINARY MODE (binary=True)\n",
            "--------------------------------------------------------------------------------\n",
            "Documents with repeated n-grams:\n",
            "  Doc 0: ['cat', 'cat', 'dog']\n",
            "  Doc 1: ['cat', 'dog', 'dog', 'dog']\n",
            "\n",
            "Vocabulary: ['cat cat', 'cat dog', 'dog dog']\n",
            "Binary matrix (1 = present, 0 = absent):\n",
            "tensor([[1., 1., 0.],\n",
            "        [0., 1., 1.]])\n",
            "\n",
            "6. VOCABULARY LIMITING (max_features=3)\n",
            "--------------------------------------------------------------------------------\n",
            "All possible bigrams from training: ~9\n",
            "Limited vocabulary (top 3): ['cat sat', 'sat on', 'the cat']\n",
            "Limited matrix shape: torch.Size([3, 3])\n",
            "\n",
            "7. COMPARISON: BAG OF WORDS vs N-GRAMS\n",
            "--------------------------------------------------------------------------------\n",
            "Document 1: ['not', 'good']\n",
            "Document 2: ['good', 'not']\n",
            "\n",
            "Unigrams (BoW) - SAME vectors:\n",
            "  Doc 1: tensor([1., 1.])\n",
            "  Doc 2: tensor([1., 1.])\n",
            "  Equal? True\n",
            "\n",
            "Bigrams - DIFFERENT vectors:\n",
            "  Vocabulary: ['good not', 'not good']\n",
            "  Doc 1: tensor([0., 1.])\n",
            "  Doc 2: tensor([1., 0.])\n",
            "  Equal? False\n",
            "\n",
            "8. EDGE CASES\n",
            "--------------------------------------------------------------------------------\n",
            "Documents: [['hi'], ['hello', 'world'], ['a', 'b', 'c']]\n",
            "Bigrams vocabulary: ['a b', 'b c', 'hello world']\n",
            "Matrix (note: 'hi' produces no bigrams):\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 1.],\n",
            "        [1., 1., 0.]])\n",
            "\n",
            "Documents with empty: [['hello', 'world'], [], ['test']]\n",
            "Result matrix:\n",
            "tensor([[1.],\n",
            "        [0.],\n",
            "        [0.]])\n",
            "\n",
            "================================================================================\n",
            "All tests completed successfully!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "from typing import List, Optional, Dict, Tuple\n",
        "import math\n",
        "\n",
        "# ============================================================================\n",
        "# 4. GloVe-style Co-occurrence Matrix - Enhanced Implementation\n",
        "# ============================================================================\n",
        "\n",
        "\n",
        "# A co-occurrence matrix captures how often words appear near each other in text.\n",
        "# It's based on the distributional hypothesis: \"You shall know a word by the company it keeps\" (J.R. Firth, 1957).\n",
        "# Core Concept\n",
        "# Words that appear in similar contexts tend to have similar meanings:\n",
        "\n",
        "# \"cat\" and \"dog\" both appear near: \"pet\", \"sat\", \"furry\"\n",
        "# \"king\" and \"queen\" both appear near: \"throne\", \"crown\", \"rules\"\n",
        "\n",
        "# Corpus:\n",
        "#   \"the cat sat on mat\"\n",
        "#   \"the dog sat on log\"\n",
        "\n",
        "# Window size = 2 (look 2 words left/right)\n",
        "\n",
        "# For \"sat\":\n",
        "#   Context words: [cat, on] (distance 1 each) + [the, mat] (distance 2)\n",
        "\n",
        "# For \"cat\":\n",
        "#   Context words: [the, sat] (distance 1 each) + [on] (distance 2)\n",
        "\n",
        "# Co-occurrence Matrix:\n",
        "#            cat  dog  log  mat  on   sat  the\n",
        "#     cat  [  0    0    0    1   1    1    1  ]\n",
        "#     dog  [  0    0    1    0   1    1    1  ]\n",
        "#     log  [  0    1    0    0   1    1    1  ]\n",
        "#     mat  [  1    0    0    0   1    1    1  ]\n",
        "#     on   [  1    1    1    1   0    2    2  ]\n",
        "#     sat  [  1    1    1    1   2    0    2  ]\n",
        "#     the  [  1    1    1    1   2    2    0  ]\n",
        "\n",
        "# Reading: Row = center word, Column = context word\n",
        "# Example: sat[on] = 2 means \"on\" appears near \"sat\" 2 times\n",
        "\n",
        "\n",
        "# Co-occurrence Matrix captures:\n",
        "\n",
        "# ✅ Distributional semantics (context = meaning)\n",
        "# ✅ Word associations\n",
        "# ✅ Semantic similarity\n",
        "\n",
        "# Improvements in this implementation:\n",
        "\n",
        "# ✅ Fixed double-counting bug in symmetric mode\n",
        "# ✅ Multiple weighting schemes\n",
        "# ✅ PPMI transformation for better quality\n",
        "# ✅ Vocabulary limiting for scalability\n",
        "# ✅ Similarity search functionality\n",
        "# ✅ Proper normalization options\n",
        "\n",
        "# Limitations:\n",
        "\n",
        "# ❌ Dense matrix (memory intensive for large vocab)\n",
        "# ❌ No dimensionality reduction (unlike GloVe embeddings)\n",
        "# ❌ Linear relationships only (no non-linear patterns)\n",
        "\n",
        "\n",
        "\n",
        "class CooccurrenceMatrix:\n",
        "    \"\"\"\n",
        "    Build word co-occurrence matrix (foundation of GloVe embeddings).\n",
        "    Counts how often words appear near each other in a context window.\n",
        "\n",
        "    Features:\n",
        "    - Distance-based weighting (closer words = higher weight)\n",
        "    - Symmetric or directed co-occurrence\n",
        "    - PPMI transformation option\n",
        "    - Vocabulary size limiting\n",
        "    - Efficient computation\n",
        "\n",
        "    The co-occurrence matrix captures distributional semantics:\n",
        "    \"You shall know a word by the company it keeps\" - J.R. Firth\n",
        "\n",
        "    Example:\n",
        "        >>> docs = [[\"cat\", \"sat\", \"on\", \"mat\"], [\"dog\", \"sat\", \"on\", \"log\"]]\n",
        "        >>> cooc = CooccurrenceMatrix(window_size=2)\n",
        "        >>> cooc.fit(docs)\n",
        "        >>> matrix = cooc.get_matrix()\n",
        "        >>> similar = cooc.most_similar(\"sat\", k=3)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        window_size: int = 5,\n",
        "        weighting: str = 'harmonic',\n",
        "        symmetric: bool = True,\n",
        "        max_vocab_size: Optional[int] = None,\n",
        "        min_count: int = 1,\n",
        "        normalize: bool = False,\n",
        "        use_ppmi: bool = False,\n",
        "        dtype: torch.dtype = torch.float32\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize Co-occurrence Matrix builder.\n",
        "\n",
        "        Args:\n",
        "            window_size: Context window size (words left/right of target)\n",
        "            weighting: How to weight context words by distance:\n",
        "                      - 'uniform': All context words weighted equally (1.0)\n",
        "                      - 'harmonic': 1/distance (GloVe default)\n",
        "                      - 'distance': 1 - (distance/window_size)\n",
        "            symmetric: If True, treat (w1, w2) same as (w2, w1)\n",
        "            max_vocab_size: Limit vocabulary to most frequent words\n",
        "            min_count: Minimum word frequency to include in vocabulary\n",
        "            normalize: If True, normalize rows to sum to 1 (probability)\n",
        "            use_ppmi: If True, apply PPMI transformation (recommended)\n",
        "            dtype: PyTorch dtype for matrix\n",
        "        \"\"\"\n",
        "        if window_size < 1:\n",
        "            raise ValueError(f\"window_size must be >= 1, got {window_size}\")\n",
        "\n",
        "        if weighting not in ['uniform', 'harmonic', 'distance']:\n",
        "            raise ValueError(f\"weighting must be 'uniform', 'harmonic', or 'distance', got '{weighting}'\")\n",
        "\n",
        "        self.window_size = window_size\n",
        "        self.weighting = weighting\n",
        "        self.symmetric = symmetric\n",
        "        self.max_vocab_size = max_vocab_size\n",
        "        self.min_count = min_count\n",
        "        self.normalize = normalize\n",
        "        self.use_ppmi = use_ppmi\n",
        "        self.dtype = dtype\n",
        "\n",
        "        # Will be set during fit()\n",
        "        self.vocab: Dict[str, int] = {}\n",
        "        self.vocab_size_: int = 0\n",
        "        self.word_counts: Counter = Counter()\n",
        "        self.cooccur: Optional[torch.Tensor] = None\n",
        "        self.is_fitted_: bool = False\n",
        "\n",
        "    def _get_weight(self, distance: int) -> float:\n",
        "        \"\"\"\n",
        "        Calculate weight based on distance from center word.\n",
        "\n",
        "        Args:\n",
        "            distance: Distance from center word (1, 2, 3, ...)\n",
        "\n",
        "        Returns:\n",
        "            Weight value (higher = more important)\n",
        "        \"\"\"\n",
        "        if self.weighting == 'uniform':\n",
        "            return 1.0\n",
        "        elif self.weighting == 'harmonic':\n",
        "            return 1.0 / distance\n",
        "        elif self.weighting == 'distance':\n",
        "            return 1.0 - (distance / self.window_size)\n",
        "        else:\n",
        "            return 1.0\n",
        "\n",
        "    def fit(self, documents: List[List[str]]) -> 'CooccurrenceMatrix':\n",
        "        \"\"\"\n",
        "        Build co-occurrence matrix from documents.\n",
        "\n",
        "        Args:\n",
        "            documents: List of tokenized documents\n",
        "\n",
        "        Returns:\n",
        "            self (for method chaining)\n",
        "        \"\"\"\n",
        "        if not documents:\n",
        "            raise ValueError(\"Cannot fit on empty document list\")\n",
        "\n",
        "        if not all(isinstance(doc, (list, tuple)) for doc in documents):\n",
        "            raise ValueError(\"All documents must be lists or tuples of tokens\")\n",
        "\n",
        "        # Step 1: Count word frequencies\n",
        "        for doc in documents:\n",
        "            self.word_counts.update(doc)\n",
        "\n",
        "        # Step 2: Filter by minimum count\n",
        "        valid_words = {\n",
        "            word for word, count in self.word_counts.items()\n",
        "            if count >= self.min_count\n",
        "        }\n",
        "\n",
        "        # Step 3: Limit vocabulary size if needed\n",
        "        if self.max_vocab_size is not None and len(valid_words) > self.max_vocab_size:\n",
        "            most_common = self.word_counts.most_common(self.max_vocab_size)\n",
        "            valid_words = {word for word, _ in most_common}\n",
        "\n",
        "        # Step 4: Create vocabulary mapping\n",
        "        self.vocab = {\n",
        "            word: idx\n",
        "            for idx, word in enumerate(sorted(valid_words))\n",
        "        }\n",
        "        self.vocab_size_ = len(self.vocab)\n",
        "\n",
        "        if self.vocab_size_ == 0:\n",
        "            raise ValueError(\"No words passed min_count threshold\")\n",
        "\n",
        "        # Step 5: Initialize co-occurrence matrix\n",
        "        self.cooccur = torch.zeros(\n",
        "            self.vocab_size_,\n",
        "            self.vocab_size_,\n",
        "            dtype=self.dtype\n",
        "        )\n",
        "\n",
        "        # Step 6: Count co-occurrences\n",
        "        for doc in documents:\n",
        "            # Filter document to only include vocabulary words\n",
        "            filtered_doc = [word for word in doc if word in self.vocab]\n",
        "\n",
        "            if len(filtered_doc) < 2:\n",
        "                continue\n",
        "\n",
        "            for i, center_word in enumerate(filtered_doc):\n",
        "                center_idx = self.vocab[center_word]\n",
        "\n",
        "                # Define context window\n",
        "                start = max(0, i - self.window_size)\n",
        "                end = min(len(filtered_doc), i + self.window_size + 1)\n",
        "\n",
        "                for j in range(start, end):\n",
        "                    if i == j:  # Skip the center word itself\n",
        "                        continue\n",
        "\n",
        "                    context_word = filtered_doc[j]\n",
        "                    context_idx = self.vocab[context_word]\n",
        "\n",
        "                    # Calculate weight based on distance\n",
        "                    distance = abs(i - j)\n",
        "                    weight = self._get_weight(distance)\n",
        "\n",
        "                    # Add to co-occurrence matrix\n",
        "                    # CRITICAL FIX: Only add once, not twice for symmetric\n",
        "                    self.cooccur[center_idx, context_idx] += weight\n",
        "\n",
        "                    # For symmetric, we'll symmetrize AFTER counting\n",
        "\n",
        "        # Step 7: Symmetrize if needed (do this ONCE at the end)\n",
        "        if self.symmetric:\n",
        "            self.cooccur = (self.cooccur + self.cooccur.T) / 2.0\n",
        "\n",
        "        # Step 8: Apply PPMI transformation if requested\n",
        "        if self.use_ppmi:\n",
        "            self.cooccur = self._compute_ppmi(self.cooccur)\n",
        "\n",
        "        # Step 9: Normalize if requested\n",
        "        if self.normalize:\n",
        "            row_sums = self.cooccur.sum(dim=1, keepdim=True)\n",
        "            # Avoid division by zero\n",
        "            row_sums = torch.where(row_sums > 0, row_sums, torch.ones_like(row_sums))\n",
        "            self.cooccur = self.cooccur / row_sums\n",
        "\n",
        "        self.is_fitted_ = True\n",
        "        return self\n",
        "\n",
        "    def _compute_ppmi(self, cooccur_matrix: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Compute Positive Pointwise Mutual Information (PPMI).\n",
        "\n",
        "        PPMI measures how much more likely two words co-occur than expected by chance.\n",
        "        PMI(w1, w2) = log(P(w1, w2) / (P(w1) * P(w2)))\n",
        "        PPMI = max(0, PMI)  # Only keep positive values\n",
        "\n",
        "        Args:\n",
        "            cooccur_matrix: Raw co-occurrence counts\n",
        "\n",
        "        Returns:\n",
        "            PPMI matrix\n",
        "        \"\"\"\n",
        "        # Total number of co-occurrences\n",
        "        total = cooccur_matrix.sum()\n",
        "\n",
        "        if total == 0:\n",
        "            return cooccur_matrix\n",
        "\n",
        "        # P(w1, w2) - joint probability\n",
        "        p_joint = cooccur_matrix / total\n",
        "\n",
        "        # P(w1) and P(w2) - marginal probabilities\n",
        "        p_word = cooccur_matrix.sum(dim=1) / total  # Row sums\n",
        "\n",
        "        # P(w1) * P(w2) - expected probability under independence\n",
        "        # Outer product: p_word[:, None] * p_word[None, :]\n",
        "        p_expected = p_word.unsqueeze(1) * p_word.unsqueeze(0)\n",
        "\n",
        "        # PMI = log(P(w1,w2) / (P(w1) * P(w2)))\n",
        "        # Avoid log(0) by adding small epsilon\n",
        "        epsilon = 1e-10\n",
        "        pmi = torch.log((p_joint + epsilon) / (p_expected + epsilon))\n",
        "\n",
        "        # PPMI = max(0, PMI)\n",
        "        ppmi = torch.clamp(pmi, min=0.0)\n",
        "\n",
        "        return ppmi\n",
        "\n",
        "    def get_matrix(self) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Get the co-occurrence matrix.\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape (vocab_size, vocab_size)\n",
        "        \"\"\"\n",
        "        if not self.is_fitted_:\n",
        "            raise RuntimeError(\"CooccurrenceMatrix must be fitted first\")\n",
        "        return self.cooccur\n",
        "\n",
        "    def get_vocabulary(self) -> List[str]:\n",
        "        \"\"\"Get vocabulary words in index order.\"\"\"\n",
        "        if not self.is_fitted_:\n",
        "            raise RuntimeError(\"CooccurrenceMatrix must be fitted first\")\n",
        "        return [word for word, _ in sorted(self.vocab.items(), key=lambda x: x[1])]\n",
        "\n",
        "    def most_similar(\n",
        "        self,\n",
        "        word: str,\n",
        "        k: int = 10,\n",
        "        metric: str = 'cosine'\n",
        "    ) -> List[Tuple[str, float]]:\n",
        "        \"\"\"\n",
        "        Find most similar words based on co-occurrence patterns.\n",
        "\n",
        "        Args:\n",
        "            word: Query word\n",
        "            k: Number of similar words to return\n",
        "            metric: Similarity metric ('cosine' or 'correlation')\n",
        "\n",
        "        Returns:\n",
        "            List of (word, similarity_score) tuples, sorted by similarity\n",
        "        \"\"\"\n",
        "        if not self.is_fitted_:\n",
        "            raise RuntimeError(\"CooccurrenceMatrix must be fitted first\")\n",
        "\n",
        "        if word not in self.vocab:\n",
        "            raise ValueError(f\"Word '{word}' not in vocabulary\")\n",
        "\n",
        "        word_idx = self.vocab[word]\n",
        "        word_vector = self.cooccur[word_idx]\n",
        "\n",
        "        if metric == 'cosine':\n",
        "            # Cosine similarity\n",
        "            similarities = self._cosine_similarity(word_vector, self.cooccur)\n",
        "        elif metric == 'correlation':\n",
        "            # Pearson correlation\n",
        "            similarities = self._correlation(word_vector, self.cooccur)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown metric: {metric}\")\n",
        "\n",
        "        # Get top k (excluding the word itself)\n",
        "        similarities[word_idx] = -float('inf')  # Exclude self\n",
        "        top_k_indices = torch.topk(similarities, k=min(k, self.vocab_size_)).indices\n",
        "\n",
        "        # Convert to word list with scores\n",
        "        idx_to_word = {idx: word for word, idx in self.vocab.items()}\n",
        "        results = [\n",
        "            (idx_to_word[idx.item()], similarities[idx].item())\n",
        "            for idx in top_k_indices\n",
        "        ]\n",
        "\n",
        "        return results\n",
        "\n",
        "    def _cosine_similarity(\n",
        "        self,\n",
        "        vector: torch.Tensor,\n",
        "        matrix: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Compute cosine similarity between vector and all rows of matrix.\"\"\"\n",
        "        # Normalize\n",
        "        vector_norm = vector / (torch.norm(vector) + 1e-10)\n",
        "        matrix_norm = matrix / (torch.norm(matrix, dim=1, keepdim=True) + 1e-10)\n",
        "\n",
        "        # Dot product\n",
        "        similarities = torch.matmul(matrix_norm, vector_norm)\n",
        "        return similarities\n",
        "\n",
        "    def _correlation(\n",
        "        self,\n",
        "        vector: torch.Tensor,\n",
        "        matrix: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"Compute Pearson correlation between vector and all rows of matrix.\"\"\"\n",
        "        # Center the data (subtract mean)\n",
        "        vector_centered = vector - vector.mean()\n",
        "        matrix_centered = matrix - matrix.mean(dim=1, keepdim=True)\n",
        "\n",
        "        # Compute correlation\n",
        "        numerator = torch.matmul(matrix_centered, vector_centered)\n",
        "        vector_std = torch.sqrt(torch.sum(vector_centered ** 2))\n",
        "        matrix_std = torch.sqrt(torch.sum(matrix_centered ** 2, dim=1))\n",
        "\n",
        "        correlations = numerator / (matrix_std * vector_std + 1e-10)\n",
        "        return correlations\n",
        "\n",
        "    def get_context_words(\n",
        "        self,\n",
        "        word: str,\n",
        "        k: int = 10\n",
        "    ) -> List[Tuple[str, float]]:\n",
        "        \"\"\"\n",
        "        Get words that most frequently appear in context of given word.\n",
        "\n",
        "        Args:\n",
        "            word: Query word\n",
        "            k: Number of context words to return\n",
        "\n",
        "        Returns:\n",
        "            List of (context_word, co-occurrence_count) tuples\n",
        "        \"\"\"\n",
        "        if not self.is_fitted_:\n",
        "            raise RuntimeError(\"CooccurrenceMatrix must be fitted first\")\n",
        "\n",
        "        if word not in self.vocab:\n",
        "            raise ValueError(f\"Word '{word}' not in vocabulary\")\n",
        "\n",
        "        word_idx = self.vocab[word]\n",
        "        counts = self.cooccur[word_idx]\n",
        "\n",
        "        # Get top k\n",
        "        top_k_indices = torch.topk(counts, k=min(k, self.vocab_size_)).indices\n",
        "\n",
        "        idx_to_word = {idx: word for word, idx in self.vocab.items()}\n",
        "        results = [\n",
        "            (idx_to_word[idx.item()], counts[idx].item())\n",
        "            for idx in top_k_indices\n",
        "        ]\n",
        "\n",
        "        return results\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        \"\"\"String representation.\"\"\"\n",
        "        params = (\n",
        "            f\"window_size={self.window_size}, \"\n",
        "            f\"weighting='{self.weighting}', \"\n",
        "            f\"symmetric={self.symmetric}, \"\n",
        "            f\"use_ppmi={self.use_ppmi}\"\n",
        "        )\n",
        "        if self.is_fitted_:\n",
        "            params += f\", vocab_size={self.vocab_size_}\"\n",
        "        return f\"CooccurrenceMatrix({params})\"\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# DEMONSTRATION & TESTING\n",
        "# ============================================================================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\" * 80)\n",
        "    print(\"CO-OCCURRENCE MATRIX DEMONSTRATION\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # Sample documents\n",
        "    train_docs = [\n",
        "        [\"the\", \"cat\", \"sat\", \"on\", \"the\", \"mat\"],\n",
        "        [\"the\", \"dog\", \"sat\", \"on\", \"the\", \"log\"],\n",
        "        [\"the\", \"cat\", \"and\", \"dog\", \"are\", \"friends\"],\n",
        "        [\"cats\", \"and\", \"dogs\", \"like\", \"to\", \"play\"],\n",
        "    ]\n",
        "\n",
        "    # ========================================================================\n",
        "    # Test 1: Basic Co-occurrence Matrix\n",
        "    # ========================================================================\n",
        "    print(\"\\n1. BASIC CO-OCCURRENCE MATRIX\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    cooc = CooccurrenceMatrix(window_size=2, weighting='uniform')\n",
        "    cooc.fit(train_docs)\n",
        "\n",
        "    print(f\"Vocabulary ({cooc.vocab_size_} words): {cooc.get_vocabulary()}\")\n",
        "    print(f\"\\nCo-occurrence matrix shape: {cooc.get_matrix().shape}\")\n",
        "    print(f\"Matrix (first 5x5):\")\n",
        "    print(cooc.get_matrix()[:5, :5])\n",
        "\n",
        "    # ========================================================================\n",
        "    # Test 2: Weighting Schemes\n",
        "    # ========================================================================\n",
        "    print(\"\\n2. DIFFERENT WEIGHTING SCHEMES\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    test_doc = [[\"cat\", \"sat\", \"on\", \"mat\"]]\n",
        "\n",
        "    # Uniform weighting\n",
        "    cooc_uniform = CooccurrenceMatrix(window_size=2, weighting='uniform')\n",
        "    cooc_uniform.fit(test_doc)\n",
        "\n",
        "    # Harmonic weighting (GloVe default)\n",
        "    cooc_harmonic = CooccurrenceMatrix(window_size=2, weighting='harmonic')\n",
        "    cooc_harmonic.fit(test_doc)\n",
        "\n",
        "    # Distance weighting\n",
        "    cooc_distance = CooccurrenceMatrix(window_size=2, weighting='distance')\n",
        "    cooc_distance.fit(test_doc)\n",
        "\n",
        "    print(\"Document: 'cat sat on mat'\")\n",
        "    print(\"Context of 'sat': [cat (distance=1), on (distance=1), mat (distance=2)]\")\n",
        "    print()\n",
        "\n",
        "    cat_idx = cooc_uniform.vocab['cat']\n",
        "    sat_idx = cooc_uniform.vocab['sat']\n",
        "    on_idx = cooc_uniform.vocab['on']\n",
        "    mat_idx = cooc_uniform.vocab['mat']\n",
        "\n",
        "    print(f\"Uniform:  cat-sat={cooc_uniform.cooccur[sat_idx, cat_idx]:.2f}, \"\n",
        "          f\"sat-on={cooc_uniform.cooccur[sat_idx, on_idx]:.2f}, \"\n",
        "          f\"sat-mat={cooc_uniform.cooccur[sat_idx, mat_idx]:.2f}\")\n",
        "\n",
        "    print(f\"Harmonic: cat-sat={cooc_harmonic.cooccur[sat_idx, cat_idx]:.2f}, \"\n",
        "          f\"sat-on={cooc_harmonic.cooccur[sat_idx, on_idx]:.2f}, \"\n",
        "          f\"sat-mat={cooc_harmonic.cooccur[sat_idx, mat_idx]:.2f}\")\n",
        "\n",
        "    print(f\"Distance: cat-sat={cooc_distance.cooccur[sat_idx, cat_idx]:.2f}, \"\n",
        "          f\"sat-on={cooc_distance.cooccur[sat_idx, on_idx]:.2f}, \"\n",
        "          f\"sat-mat={cooc_distance.cooccur[sat_idx, mat_idx]:.2f}\")\n",
        "\n",
        "    print(\"\\nNote: Harmonic (1/distance) gives more weight to closer words\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # Test 3: Symmetric vs Directed\n",
        "    # ========================================================================\n",
        "    print(\"\\n3. SYMMETRIC vs DIRECTED\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    cooc_sym = CooccurrenceMatrix(window_size=2, symmetric=True)\n",
        "    cooc_sym.fit([[\"cat\", \"dog\", \"bird\"]])\n",
        "\n",
        "    cooc_dir = CooccurrenceMatrix(window_size=2, symmetric=False)\n",
        "    cooc_dir.fit([[\"cat\", \"dog\", \"bird\"]])\n",
        "\n",
        "    cat_idx = cooc_sym.vocab['cat']\n",
        "    dog_idx = cooc_sym.vocab['dog']\n",
        "\n",
        "    print(\"Document: 'cat dog bird'\")\n",
        "    print(f\"\\nSymmetric: cat-dog = {cooc_sym.cooccur[cat_idx, dog_idx]:.2f}, \"\n",
        "          f\"dog-cat = {cooc_sym.cooccur[dog_idx, cat_idx]:.2f}\")\n",
        "    print(f\"Directed:  cat-dog = {cooc_dir.cooccur[cat_idx, dog_idx]:.2f}, \"\n",
        "          f\"dog-cat = {cooc_dir.cooccur[dog_idx, cat_idx]:.2f}\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # Test 4: PPMI Transformation\n",
        "    # ========================================================================\n",
        "    print(\"\\n4. PPMI TRANSFORMATION\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    cooc_raw = CooccurrenceMatrix(window_size=2, use_ppmi=False)\n",
        "    cooc_raw.fit(train_docs)\n",
        "\n",
        "    cooc_ppmi = CooccurrenceMatrix(window_size=2, use_ppmi=True)\n",
        "    cooc_ppmi.fit(train_docs)\n",
        "\n",
        "    print(\"PPMI reduces effect of common word pairs\")\n",
        "    print(f\"\\nRaw counts (first 3x3):\")\n",
        "    print(cooc_raw.get_matrix()[:3, :3])\n",
        "    print(f\"\\nPPMI values (first 3x3):\")\n",
        "    print(cooc_ppmi.get_matrix()[:3, :3])\n",
        "\n",
        "    # ========================================================================\n",
        "    # Test 5: Finding Similar Words\n",
        "    # ========================================================================\n",
        "    print(\"\\n5. FINDING SIMILAR WORDS\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Larger corpus for better examples\n",
        "    larger_docs = [\n",
        "        [\"cat\", \"sat\", \"on\", \"mat\"],\n",
        "        [\"cat\", \"slept\", \"on\", \"mat\"],\n",
        "        [\"dog\", \"sat\", \"on\", \"log\"],\n",
        "        [\"dog\", \"slept\", \"on\", \"log\"],\n",
        "        [\"cat\", \"and\", \"dog\", \"are\", \"pets\"],\n",
        "        [\"mat\", \"and\", \"log\", \"are\", \"objects\"],\n",
        "    ]\n",
        "\n",
        "    cooc_large = CooccurrenceMatrix(window_size=3, use_ppmi=True)\n",
        "    cooc_large.fit(larger_docs)\n",
        "\n",
        "    print(\"Query: 'cat'\")\n",
        "    similar_to_cat = cooc_large.most_similar(\"cat\", k=3)\n",
        "    for word, score in similar_to_cat:\n",
        "        print(f\"  {word}: {score:.3f}\")\n",
        "\n",
        "    print(\"\\nQuery: 'sat'\")\n",
        "    similar_to_sat = cooc_large.most_similar(\"sat\", k=3)\n",
        "    for word, score in similar_to_sat:\n",
        "        print(f\"  {word}: {score:.3f}\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # Test 6: Context Words\n",
        "    # ========================================================================\n",
        "    print(\"\\n6. MOST FREQUENT CONTEXT WORDS\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    print(\"Words that appear near 'cat':\")\n",
        "    context_cat = cooc_large.get_context_words(\"cat\", k=5)\n",
        "    for word, count in context_cat:\n",
        "        print(f\"  {word}: {count:.2f}\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # Test 7: Vocabulary Limiting\n",
        "    # ========================================================================\n",
        "    print(\"\\n7. VOCABULARY LIMITING\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    cooc_unlimited = CooccurrenceMatrix(window_size=2)\n",
        "    cooc_unlimited.fit(train_docs)\n",
        "\n",
        "    cooc_limited = CooccurrenceMatrix(window_size=2, max_vocab_size=5)\n",
        "    cooc_limited.fit(train_docs)\n",
        "\n",
        "    print(f\"Unlimited vocab: {cooc_unlimited.get_vocabulary()}\")\n",
        "    print(f\"Limited vocab (top 5): {cooc_limited.get_vocabulary()}\")\n",
        "\n",
        "    # ========================================================================\n",
        "    # Test 8: Practical Example - Distributional Semantics\n",
        "    # ========================================================================\n",
        "    print(\"\\n8. DISTRIBUTIONAL SEMANTICS EXAMPLE\")\n",
        "    print(\"-\" * 80)\n",
        "    print(\"'You shall know a word by the company it keeps' - J.R. Firth\")\n",
        "    print()\n",
        "\n",
        "    semantic_docs = [\n",
        "        [\"king\", \"rules\", \"kingdom\", \"crown\", \"throne\"],\n",
        "        [\"queen\", \"rules\", \"kingdom\", \"crown\", \"throne\"],\n",
        "        [\"man\", \"works\", \"in\", \"office\"],\n",
        "        [\"woman\", \"works\", \"in\", \"office\"],\n",
        "        [\"king\", \"and\", \"queen\", \"are\", \"royalty\"],\n",
        "        [\"man\", \"and\", \"woman\", \"are\", \"people\"],\n",
        "    ]\n",
        "\n",
        "    cooc_semantic = CooccurrenceMatrix(window_size=3, use_ppmi=True)\n",
        "    cooc_semantic.fit(semantic_docs)\n",
        "\n",
        "    print(\"Similar to 'king':\")\n",
        "    for word, score in cooc_semantic.most_similar(\"king\", k=3):\n",
        "        print(f\"  {word}: {score:.3f}\")\n",
        "\n",
        "    print(\"\\nSimilar to 'man':\")\n",
        "    for word, score in cooc_semantic.most_similar(\"man\", k=3):\n",
        "        print(f\"  {word}: {score:.3f}\")\n",
        "\n",
        "    print(\"\\nNote: Words with similar contexts are semantically related!\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"All tests completed successfully!\")\n",
        "    print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kv4AAukGlbpY",
        "outputId": "7f1b24af-4499-488b-ddad-7b6ebfa09fce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "CO-OCCURRENCE MATRIX DEMONSTRATION\n",
            "================================================================================\n",
            "\n",
            "1. BASIC CO-OCCURRENCE MATRIX\n",
            "--------------------------------------------------------------------------------\n",
            "Vocabulary (15 words): ['and', 'are', 'cat', 'cats', 'dog', 'dogs', 'friends', 'like', 'log', 'mat', 'on', 'play', 'sat', 'the', 'to']\n",
            "\n",
            "Co-occurrence matrix shape: torch.Size([15, 15])\n",
            "Matrix (first 5x5):\n",
            "tensor([[0., 1., 1., 1., 1.],\n",
            "        [1., 0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0., 1.],\n",
            "        [1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0.]])\n",
            "\n",
            "2. DIFFERENT WEIGHTING SCHEMES\n",
            "--------------------------------------------------------------------------------\n",
            "Document: 'cat sat on mat'\n",
            "Context of 'sat': [cat (distance=1), on (distance=1), mat (distance=2)]\n",
            "\n",
            "Uniform:  cat-sat=1.00, sat-on=1.00, sat-mat=1.00\n",
            "Harmonic: cat-sat=1.00, sat-on=1.00, sat-mat=0.50\n",
            "Distance: cat-sat=0.50, sat-on=0.50, sat-mat=0.00\n",
            "\n",
            "Note: Harmonic (1/distance) gives more weight to closer words\n",
            "\n",
            "3. SYMMETRIC vs DIRECTED\n",
            "--------------------------------------------------------------------------------\n",
            "Document: 'cat dog bird'\n",
            "\n",
            "Symmetric: cat-dog = 1.00, dog-cat = 1.00\n",
            "Directed:  cat-dog = 1.00, dog-cat = 1.00\n",
            "\n",
            "4. PPMI TRANSFORMATION\n",
            "--------------------------------------------------------------------------------\n",
            "PPMI reduces effect of common word pairs\n",
            "\n",
            "Raw counts (first 3x3):\n",
            "tensor([[0.0000, 0.5000, 1.0000],\n",
            "        [0.5000, 0.0000, 0.0000],\n",
            "        [1.0000, 0.0000, 0.0000]])\n",
            "\n",
            "PPMI values (first 3x3):\n",
            "tensor([[0.0000, 0.7112, 0.7112],\n",
            "        [0.7112, 0.0000, 0.0000],\n",
            "        [0.7112, 0.0000, 0.0000]])\n",
            "\n",
            "5. FINDING SIMILAR WORDS\n",
            "--------------------------------------------------------------------------------\n",
            "Query: 'cat'\n",
            "  on: 0.762\n",
            "  dog: 0.667\n",
            "  mat: 0.436\n",
            "\n",
            "Query: 'sat'\n",
            "  slept: 1.000\n",
            "  mat: 0.644\n",
            "  and: 0.397\n",
            "\n",
            "6. MOST FREQUENT CONTEXT WORDS\n",
            "--------------------------------------------------------------------------------\n",
            "Words that appear near 'cat':\n",
            "  slept: 0.77\n",
            "  sat: 0.77\n",
            "  and: 0.64\n",
            "  mat: 0.27\n",
            "  on: 0.08\n",
            "\n",
            "7. VOCABULARY LIMITING\n",
            "--------------------------------------------------------------------------------\n",
            "Unlimited vocab: ['and', 'are', 'cat', 'cats', 'dog', 'dogs', 'friends', 'like', 'log', 'mat', 'on', 'play', 'sat', 'the', 'to']\n",
            "Limited vocab (top 5): ['cat', 'dog', 'on', 'sat', 'the']\n",
            "\n",
            "8. DISTRIBUTIONAL SEMANTICS EXAMPLE\n",
            "--------------------------------------------------------------------------------\n",
            "'You shall know a word by the company it keeps' - J.R. Firth\n",
            "\n",
            "Similar to 'king':\n",
            "  queen: 0.566\n",
            "  kingdom: 0.430\n",
            "  royalty: 0.423\n",
            "\n",
            "Similar to 'man':\n",
            "  in: 0.623\n",
            "  office: 0.613\n",
            "  woman: 0.608\n",
            "\n",
            "Note: Words with similar contexts are semantically related!\n",
            "\n",
            "================================================================================\n",
            "All tests completed successfully!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MQ_7_cHK6MMX"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 3. CHARACTER N-GRAMS - Subword information\n",
        "# ============================================================================\n",
        "\n",
        "class CharNGramVectorizer:\n",
        "    \"\"\"\n",
        "    Extract character-level n-grams\n",
        "    Useful for: morphology, typos, out-of-vocabulary words\n",
        "    E.g., \"cat\" with n=3 → [\"<ca\", \"cat\", \"at>\"]\n",
        "    \"\"\"\n",
        "    def __init__(self, n=3, word_level=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            n: Character n-gram size\n",
        "            word_level: If True, add word boundaries <>, else do character stream\n",
        "        \"\"\"\n",
        "        self.n = n\n",
        "        self.word_level = word_level\n",
        "        self.vocab = {}\n",
        "\n",
        "    def _get_char_ngrams(self, text):\n",
        "        \"\"\"Extract character n-grams\"\"\"\n",
        "        if self.word_level:\n",
        "            # Add boundaries for each word\n",
        "            words = text if isinstance(text, list) else text.split()\n",
        "            text = ' '.join(f'<{word}>' for word in words)\n",
        "\n",
        "        ngrams = []\n",
        "        for i in range(len(text) - self.n + 1):\n",
        "            ngrams.append(text[i:i+self.n])\n",
        "        return ngrams\n",
        "\n",
        "    def fit(self, documents):\n",
        "        vocab_set = set()\n",
        "        for doc in documents:\n",
        "            if isinstance(doc, list):\n",
        "                doc = ' '.join(doc)\n",
        "            ngrams = self._get_char_ngrams(doc)\n",
        "            vocab_set.update(ngrams)\n",
        "\n",
        "        self.vocab = {ngram: idx for idx, ngram in enumerate(sorted(vocab_set))}\n",
        "        return self\n",
        "\n",
        "    def transform(self, documents):\n",
        "        vocab_size = len(self.vocab)\n",
        "        char_ngram_matrix = torch.zeros(len(documents), vocab_size)\n",
        "\n",
        "        for doc_idx, doc in enumerate(documents):\n",
        "            if isinstance(doc, list):\n",
        "                doc = ' '.join(doc)\n",
        "            ngrams = self._get_char_ngrams(doc)\n",
        "            counts = Counter(ngrams)\n",
        "\n",
        "            for ngram, count in counts.items():\n",
        "                if ngram in self.vocab:\n",
        "                    char_ngram_matrix[doc_idx, self.vocab[ngram]] = count\n",
        "\n",
        "        return char_ngram_matrix\n",
        "\n",
        "    def fit_transform(self, documents):\n",
        "        return self.fit(documents).transform(documents)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 4. GloVe-style Co-occurrence Matrix\n",
        "# ============================================================================\n",
        "\n",
        "class CooccurrenceMatrix:\n",
        "    \"\"\"\n",
        "    Build word co-occurrence matrix (foundation of GloVe)\n",
        "    Counts how often words appear near each other\n",
        "    \"\"\"\n",
        "    def __init__(self, window_size=5, symmetric=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            window_size: How many words left/right to consider as context\n",
        "            symmetric: If True, (w1, w2) and (w2, w1) count equally\n",
        "        \"\"\"\n",
        "        self.window_size = window_size\n",
        "        self.symmetric = symmetric\n",
        "        self.vocab = {}\n",
        "        self.cooccur = None\n",
        "\n",
        "    def fit(self, documents):\n",
        "        # Build vocabulary\n",
        "        vocab_set = set()\n",
        "        for doc in documents:\n",
        "            vocab_set.update(doc)\n",
        "        self.vocab = {word: idx for idx, word in enumerate(sorted(vocab_set))}\n",
        "        vocab_size = len(self.vocab)\n",
        "\n",
        "        # Initialize co-occurrence matrix\n",
        "        self.cooccur = torch.zeros(vocab_size, vocab_size)\n",
        "\n",
        "        # Count co-occurrences\n",
        "        for doc in documents:\n",
        "            for i, center_word in enumerate(doc):\n",
        "                if center_word not in self.vocab:\n",
        "                    continue\n",
        "                center_idx = self.vocab[center_word]\n",
        "\n",
        "                # Look at context window\n",
        "                start = max(0, i - self.window_size)\n",
        "                end = min(len(doc), i + self.window_size + 1)\n",
        "\n",
        "                for j in range(start, end):\n",
        "                    if i == j:\n",
        "                        continue\n",
        "\n",
        "                    context_word = doc[j]\n",
        "                    if context_word not in self.vocab:\n",
        "                        continue\n",
        "                    context_idx = self.vocab[context_word]\n",
        "\n",
        "                    # Weight by distance (optional)\n",
        "                    distance = abs(i - j)\n",
        "                    weight = 1.0 / distance\n",
        "\n",
        "                    self.cooccur[center_idx, context_idx] += weight\n",
        "\n",
        "                    if self.symmetric:\n",
        "                        self.cooccur[context_idx, center_idx] += weight\n",
        "\n",
        "        return self\n",
        "\n",
        "    def get_matrix(self):\n",
        "        \"\"\"Return the co-occurrence matrix\"\"\"\n",
        "        return self.cooccur\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 5. SIMPLE NEURAL BAG OF WORDS (NBoW)\n",
        "# ============================================================================\n",
        "\n",
        "class NeuralBagOfWords(nn.Module):\n",
        "    \"\"\"\n",
        "    Learnable word embeddings averaged to represent document\n",
        "    - Each word gets a learned embedding\n",
        "    - Document = average of its word embeddings\n",
        "    - Can be trained end-to-end for classification\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embedding_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, lengths=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [batch_size, max_seq_len] - padded word indices\n",
        "            lengths: [batch_size] - actual lengths (optional, for masking)\n",
        "        \"\"\"\n",
        "        # Get embeddings\n",
        "        embedded = self.embedding(x)  # [batch, seq_len, embed_dim]\n",
        "\n",
        "        # Average pooling (ignoring padding if lengths provided)\n",
        "        if lengths is not None:\n",
        "            # Create mask\n",
        "            mask = torch.arange(x.size(1)).unsqueeze(0) < lengths.unsqueeze(1)\n",
        "            mask = mask.unsqueeze(2).float()  # [batch, seq_len, 1]\n",
        "\n",
        "            # Masked average\n",
        "            embedded = embedded * mask\n",
        "            doc_embedding = embedded.sum(dim=1) / lengths.unsqueeze(1).float()\n",
        "        else:\n",
        "            # Simple average\n",
        "            doc_embedding = embedded.mean(dim=1)  # [batch, embed_dim]\n",
        "\n",
        "        # Classification\n",
        "        logits = self.fc(doc_embedding)  # [batch, num_classes]\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# 6. FASTTEXT-STYLE (Subword embeddings approximation)\n",
        "# ============================================================================\n",
        "\n",
        "class SubwordEmbedding:\n",
        "    \"\"\"\n",
        "    Approximate FastText by representing words as sum of character n-grams\n",
        "    Handles out-of-vocabulary words better than pure word embeddings\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim=100, n=3):\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.n = n\n",
        "        self.ngram_embeddings = {}\n",
        "\n",
        "    def _get_char_ngrams(self, word):\n",
        "        \"\"\"Extract character n-grams with boundaries\"\"\"\n",
        "        word = f'<{word}>'\n",
        "        ngrams = [word[i:i+self.n] for i in range(len(word) - self.n + 1)]\n",
        "        return ngrams\n",
        "\n",
        "    def fit(self, words):\n",
        "        \"\"\"Build vocabulary of character n-grams\"\"\"\n",
        "        ngram_set = set()\n",
        "        for word in words:\n",
        "            ngrams = self._get_char_ngrams(word)\n",
        "            ngram_set.update(ngrams)\n",
        "\n",
        "        # Initialize random embeddings for each n-gram\n",
        "        for ngram in ngram_set:\n",
        "            self.ngram_embeddings[ngram] = torch.randn(self.embedding_dim)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def get_word_embedding(self, word):\n",
        "        \"\"\"Get word embedding as sum of its character n-gram embeddings\"\"\"\n",
        "        ngrams = self._get_char_ngrams(word)\n",
        "\n",
        "        embeddings = []\n",
        "        for ngram in ngrams:\n",
        "            if ngram in self.ngram_embeddings:\n",
        "                embeddings.append(self.ngram_embeddings[ngram])\n",
        "\n",
        "        if len(embeddings) == 0:\n",
        "            # Fallback for unknown word\n",
        "            return torch.zeros(self.embedding_dim)\n",
        "\n",
        "        # Average n-gram embeddings\n",
        "        return torch.stack(embeddings).mean(dim=0)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EXAMPLE USAGE & COMPARISON\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample documents\n",
        "    documents = [\n",
        "        ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
        "        ['the', 'dog', 'sat', 'on', 'the', 'log'],\n",
        "        ['cats', 'and', 'dogs', 'are', 'pets'],\n",
        "    ]\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"1. BAG OF WORDS\")\n",
        "    print(\"=\" * 70)\n",
        "    bow = BagOfWords()\n",
        "    bow_matrix = bow.fit_transform(documents)\n",
        "    print(f\"Shape: {bow_matrix.shape}\")\n",
        "    print(f\"Doc 1: {bow_matrix[0][:10]}...\")  # First 10 dimensions\n",
        "    print(f\"Sparsity: {(bow_matrix == 0).sum().item() / bow_matrix.numel():.2%}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"2. BIGRAMS (2-grams)\")\n",
        "    print(\"=\" * 70)\n",
        "    bigram = NGramVectorizer(n=2)\n",
        "    bigram_matrix = bigram.fit_transform(documents)\n",
        "    print(f\"Shape: {bigram_matrix.shape}\")\n",
        "    print(f\"Sample bigrams: {list(bigram.vocab.keys())[:5]}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"3. CHARACTER TRIGRAMS\")\n",
        "    print(\"=\" * 70)\n",
        "    char_trigram = CharNGramVectorizer(n=3)\n",
        "    char_matrix = char_trigram.fit_transform(documents)\n",
        "    print(f\"Shape: {char_matrix.shape}\")\n",
        "    print(f\"Sample char-3grams: {list(char_trigram.vocab.keys())[:10]}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"4. CO-OCCURRENCE MATRIX\")\n",
        "    print(\"=\" * 70)\n",
        "    cooccur = CooccurrenceMatrix(window_size=2)\n",
        "    cooccur.fit(documents)\n",
        "    cooccur_matrix = cooccur.get_matrix()\n",
        "    print(f\"Shape: {cooccur_matrix.shape}\")\n",
        "\n",
        "    # Show co-occurrence for \"cat\"\n",
        "    if 'cat' in cooccur.vocab:\n",
        "        cat_idx = cooccur.vocab['cat']\n",
        "        print(f\"\\nWords co-occurring with 'cat':\")\n",
        "        vocab_reverse = {v: k for k, v in cooccur.vocab.items()}\n",
        "        for idx in torch.topk(cooccur_matrix[cat_idx], k=5).indices:\n",
        "            word = vocab_reverse[idx.item()]\n",
        "            count = cooccur_matrix[cat_idx, idx].item()\n",
        "            print(f\"  {word}: {count:.2f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"5. NEURAL BAG OF WORDS (Architecture only)\")\n",
        "    print(\"=\" * 70)\n",
        "    vocab_size = 100\n",
        "    nbow = NeuralBagOfWords(vocab_size=vocab_size, embedding_dim=50, num_classes=3)\n",
        "\n",
        "    # Dummy input\n",
        "    dummy_input = torch.randint(0, vocab_size, (2, 10))  # 2 docs, max len 10\n",
        "    lengths = torch.tensor([6, 8])\n",
        "    output = nbow(dummy_input, lengths)\n",
        "    print(f\"Input shape: {dummy_input.shape}\")\n",
        "    print(f\"Output shape: {output.shape}\")\n",
        "    print(\"(This would be trained end-to-end with a classification loss)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 70)\n",
        "    print(\"6. SUBWORD EMBEDDINGS (FastText-style)\")\n",
        "    print(\"=\" * 70)\n",
        "    words = ['cat', 'cats', 'catlike', 'dog']\n",
        "    subword = SubwordEmbedding(embedding_dim=50, n=3)\n",
        "    subword.fit(words)\n",
        "\n",
        "    cat_embed = subword.get_word_embedding('cat')\n",
        "    cats_embed = subword.get_word_embedding('cats')\n",
        "\n",
        "    # Similarity between 'cat' and 'cats'\n",
        "    similarity = F.cosine_similarity(cat_embed.unsqueeze(0), cats_embed.unsqueeze(0))\n",
        "    print(f\"Cosine similarity between 'cat' and 'cats': {similarity.item():.4f}\")\n",
        "    print(\"(Should be high because they share many character n-grams)\")\n",
        "\n",
        "    # Out-of-vocabulary word\n",
        "    unknown_embed = subword.get_word_embedding('category')\n",
        "    sim_unknown = F.cosine_similarity(cat_embed.unsqueeze(0), unknown_embed.unsqueeze(0))\n",
        "    print(f\"Similarity between 'cat' and 'category': {sim_unknown.item():.4f}\")\n",
        "    print(\"(Non-zero even though 'category' wasn't in training!)\")"
      ]
    }
  ]
}