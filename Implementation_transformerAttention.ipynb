{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-09T19:16:20.822048Z",
     "start_time": "2025-11-09T19:16:20.816563Z"
    }
   },
   "source": [
    "# import torch\n",
    "# import math\n",
    "# from torch import nn\n",
    "# import copy\n",
    "# import torch.nn.functional as F\n",
    "# DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#\n",
    "# def attention(query, key, value, mask=None, dropout=None):\n",
    "#     \"\"\"\n",
    "#     Scaled dot product attention\n",
    "#     query: (B, h, T_q, d_k)\n",
    "#     key: (B, h, T_k, d_k)\n",
    "#     value: (B, h, T_k, d_k)\n",
    "#     mask: broadcastable to (B, h, T_q, T_k) with True where allowed (or 1)\n",
    "#     returns: (B, h, T_q, d_k), attn (B, h, T_q, T_k)\n",
    "#     \"\"\"\n",
    "#     d_k = query.size(-1)\n",
    "#     scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "#     if mask is not None:\n",
    "#         scores = scores.masked_fill(mask==0,-1e9)\n",
    "#     p_attn = nn.functional.softmax(scores, dim=-1)\n",
    "#     if dropout is not None:\n",
    "#         p_attn = dropout(p_attn)\n",
    "#     return torch.matmul(p_attn, value), p_attn\n",
    "#\n",
    "# from copy import deepcopy\n",
    "# class MultiHeadedAttention(nn.Module):\n",
    "#     def __init__(self, h, d_model, dropout=0.1):\n",
    "#         \"\"\"\n",
    "#         h: number of heads\n",
    "#         d_model: model dimensionality (must be divisible by h)\n",
    "#         \"\"\"\n",
    "#         super().__init__()\n",
    "#         assert d_model % h == 0\n",
    "#         self.d_k = d_model // h\n",
    "#         self.h = h\n",
    "#         self.linears = nn.ModuleList([deepcopy(\n",
    "#             nn.Linear(d_model, d_model)) for i in range(4\n",
    "#         )])\n",
    "#         self.attn = None\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "#\n",
    "#     def forward(self, query, key, value, mask=None):\n",
    "#         if mask is not None:\n",
    "#             mask = mask.unsqueeze(1)\n",
    "#         nbatches = query.size(0)\n",
    "#         query, key, value = [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1,2) for\n",
    "#             l,x in zip(self.linears, (query, key, value))]\n",
    "#         x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "#         x = x.transpose(1,2).contiguous().view(nbatches, -1, self.h*self.d_k)\n",
    "#         output = self.dropout(x)\n",
    "#         return output\n",
    "#\n",
    "# class PositionwiseFeedForward(nn.Module):\n",
    "#     def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "#         super().__init__()\n",
    "#         self.w_1 = nn.Linear(d_model, d_ff)\n",
    "#         self.w_2 = nn.Linear(d_ff, d_model)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#     def forward(self, x):\n",
    "#         h1 = self.w_1(x)\n",
    "#         h2 = self.dropout(h1)\n",
    "#         return self.w_2(h2)\n",
    "#\n",
    "# class EncoderLayer(nn.Module):\n",
    "#     def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "#         super().__init__()\n",
    "#         self.self_attn = self_attn\n",
    "#         self.feed_forward = feed_forward\n",
    "#         self.sublayer = nn.ModuleList([deepcopy(\n",
    "#             SublayerConnection(size, dropout)) for i in range(2)\n",
    "#         ])\n",
    "#         self.size = size\n",
    "#     def forward(self, x, mask):\n",
    "#         x = self.sublayer[0](\n",
    "#             x, lambda x: self.self_attn(x, x, x, mask)\n",
    "#         )\n",
    "#         output = self.sublayer[1](x, self.feed_forward)\n",
    "#         return output\n",
    "#\n",
    "# class SublayerConnection(nn.Module):\n",
    "#     def __init__(self, size, dropout):\n",
    "#         super().__init__()\n",
    "#         self.norm = LayerNorm(size)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "#     def forward(self, x, sublayer):\n",
    "#         output = x + self.dropout(sublayer(self.norm(x)))\n",
    "#         return output\n",
    "#\n",
    "# class LayerNorm(nn.Module):\n",
    "#     def __init__(self, features, eps=1e-6):\n",
    "#         super().__init__()\n",
    "#         self.a_2 = nn.Parameter(torch.ones(features))\n",
    "#         self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "#         self.eps = eps\n",
    "#     def forward(self, x):\n",
    "#         mean = x.mean(-1, keepdim=True)\n",
    "#         std = x.std(-1, keepdim=True)\n",
    "#         x_zscore = (x-mean) / torch.sqrt(std**2 + self.eps)\n",
    "#         output = self.a_2 * x_zscore + self.b_2\n",
    "#         return output\n",
    "#\n",
    "# from copy import deepcopy\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, layer, N):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.ModuleList([deepcopy(layer) for i in range(N)])\n",
    "#         self.norm = LayerNorm(layer.size)\n",
    "#     def forward(self, x, mask):\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x, mask)\n",
    "#             output = self.norm(x)\n",
    "#         return output\n",
    "#\n",
    "# class DecoderLayer(nn.Module):\n",
    "#     def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "#         super().__init__()\n",
    "#         self.size = size\n",
    "#         self.self_attn = self_attn\n",
    "#         self.src_attn = src_attn\n",
    "#         self.feed_forward = feed_forward\n",
    "#         self.sublayer = nn.ModuleList([deepcopy(SublayerConnection(size, dropout)) for\n",
    "#                                        i in range(3)])\n",
    "#     def forward(self, x, memory, src_mask, tgt_mask):\n",
    "#         x = self.sublayer[0](x, lambda x:\n",
    "#                              self.self_attn(x, x, x, tgt_mask))\n",
    "#         x = self.sublayer[1](x, lambda x:\n",
    "#                              self.src_attn(x, memory, memory, src_mask))\n",
    "#         output = self.sublayer[2](x, self.feed_forward)\n",
    "#         return output\n",
    "#\n",
    "# class Decoder(nn.Module):\n",
    "#     def __init__(self, layer, N):\n",
    "#         super().__init__()\n",
    "#         self.layers = nn.ModuleList([deepcopy(layer) for i in range(N)])\n",
    "#         self.norm = LayerNorm(layer.size)\n",
    "#     def forward(self, x, memory, src_mask, tgt_mask):\n",
    "#         for layer in self.layers:\n",
    "#             x = layer(x, memory, memory, src_mask, tgt_mask)\n",
    "#         output = self.norm(x)\n",
    "#         return output\n",
    "#\n",
    "#\n",
    "# class Transformer(nn.Module):\n",
    "#     def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "#         super().__init__()\n",
    "#         self.encoder = encoder\n",
    "#         self.decoder = decoder\n",
    "#         self.src_embed = src_embed\n",
    "#         self.tgt_embed = tgt_embed\n",
    "#         self.generator = generator\n",
    "#     def encode(self, src, src_mask):\n",
    "#         return self.encoder(self.src_embed(src), src_mask)\n",
    "#     def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "#         return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "#     def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "#         memory = self.encode(src, src_mask)\n",
    "#         output = self.decode(memory, src_mask, tgt, tgt_mask)\n",
    "#         return output\n",
    "#\n",
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, d_model, vocab):\n",
    "#         super().__init__()\n",
    "#         self.proj = nn.Linear(d_model, vocab)\n",
    "#     def forward(self, x):\n",
    "#         out = self.proj(x)\n",
    "#         probs = nn.functional.log_softmax(out, dim=-1)\n",
    "#         return probs\n",
    "#\n",
    "# def create_model(src_vocab, tgt_vocab, N, d_model, d_ff, h, dropout=0.1):\n",
    "#     attn = MultiHeadedAttention(h, d_model).to(DEVICE)\n",
    "#     ff = PositionwiseFeedForward(d_model, d_ff, dropout).to(DEVICE)\n",
    "#     pos = PositionalEncoding(d_model, dropout).to(DEVICE)\n",
    "#     model = Transformer(\n",
    "#         Encoder(EncoderLayer(d_model, deepcopy(attn), deepcopy(ff),\n",
    "#                              dropout).to(DEVICE), N).to(DEVICE)\n",
    "#         Decoder(DecoderLayer(d_model, deepcopy(attn),\n",
    "#                              deepcopy(attn), deepcopy(ff), dropout).to(DEVICE),N\n",
    "#     ).to(DEVICE),\n",
    "#         nn.Sequential(Embeddings(d_model, src_vocab).to(DEVICE), deepcopy(pos)),\n",
    "#         nn.Sequential(Embeddings(d_model, tgt_vocab).to(DEVICE), deepcopy(pos)),\n",
    "#         Generator(d_model, tgt_vocab).to(DEVICE)\n",
    "#     for p in model.parameters():\n",
    "#         if p.dim() > 1:\n",
    "#             nn.init.xvavier_uniform_(p)\n",
    "#     return model.to(DEVICE)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T19:16:20.826622Z",
     "start_time": "2025-11-09T19:16:20.825116Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "91176e25c777f07b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T19:16:20.851061Z",
     "start_time": "2025-11-09T19:16:20.829135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# transformer_model.py\n",
    "import math, copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        # ensure boolean mask where True means \"allowed\"\n",
    "        if mask.dtype != torch.bool:\n",
    "            mask = mask != 0\n",
    "        # mask shape should broadcast to (B, h, T_q, T_k); use ~mask because masked_fill expects True where to fill\n",
    "        print(\"mask matrix\")\n",
    "        print(mask)\n",
    "        scores = scores.masked_fill(~mask, -1e9)\n",
    "        print(scores)\n",
    "    p_attn = F.softmax(scores, dim=-1)\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn\n",
    "\n",
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.h = h\n",
    "        self.d_k = d_model // h\n",
    "        self.linears = nn.ModuleList([copy.deepcopy(nn.Linear(d_model, d_model)) for _ in range(4)])\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        breakpoint()\n",
    "        if mask is not None:\n",
    "            # if mask is (B, T_q, T_k) -> make it (B, 1, T_q, T_k) for broadcasting\n",
    "            if mask.dim() == 3:\n",
    "                # unsqueeze returns a tensor with a dimension one inserted at specified position\n",
    "                mask = mask.unsqueeze(1)\n",
    "        B = query.size(0)\n",
    "        # Project and split heads\n",
    "        # T is time steps or sequence length, \"Hello World\" T = 2\n",
    "        # h is number of attention heads, common values 8, 12, 16, BERT is 12\n",
    "        # T_q is length of query sequence\n",
    "        # T_k is length of key sequence\n",
    "        # d_k is dimension per head, d_model / h\n",
    "        # d_model is the model dimension\n",
    "        query, key, value = [\n",
    "            # linear projection of x\n",
    "            # B, T, h, d_k\n",
    "            # B, h, T, d_k\n",
    "            lin(x).view(B, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears[:3], (query, key, value))\n",
    "        ]\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        # after attention is B, h, T, d_k\n",
    "        # transpose B, T, h, d_k\n",
    "        # d_model is h * d_k , and output B, T, d_model\n",
    "        x = x.transpose(1, 2).contiguous().view(B, -1, self.h * self.d_k)\n",
    "        return self.linears[3](x)\n",
    "\n",
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        var = x.var(-1, keepdim=True, unbiased=False)\n",
    "        x_norm = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.a_2 * x_norm + self.b_2\n",
    "\n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super().__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = nn.ModuleList([copy.deepcopy(SublayerConnection(size, dropout)) for _ in range(2)])\n",
    "        self.size = size\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super().__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = nn.ModuleList([copy.deepcopy(SublayerConnection(size, dropout)) for _ in range(3)])\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, memory, memory, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(layer) for _ in range(N)])\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        memory = self.encode(src, src_mask)\n",
    "        return self.decode(memory, src_mask, tgt, tgt_mask)\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "    def forward(self, x):\n",
    "        return F.log_softmax(self.proj(x), dim=-1)\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super().__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :].to(x.device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "def make_model(src_vocab, tgt_vocab, N=2, d_model=32, d_ff=64, h=4, dropout=0.1):\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model, dropout)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    pos = PositionalEncoding(d_model, dropout)\n",
    "    model = Transformer(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(pos)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(pos)),\n",
    "        Generator(d_model, tgt_vocab)\n",
    "    ).to(DEVICE)\n",
    "\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model\n"
   ],
   "id": "f993804935618c85",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T19:16:20.868910Z",
     "start_time": "2025-11-09T19:16:20.856066Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# make_dummy_data.py\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "B = 2           # batch size\n",
    "T_src = 5       # source sequence length\n",
    "T_tgt = 6       # target sequence length\n",
    "SRC_VOCAB = 11\n",
    "TGT_VOCAB = 11\n",
    "\n",
    "# tokens 0..vocab-1 (we reserve 0 for padding in general)\n",
    "src = torch.randint(1, SRC_VOCAB, (B, T_src), dtype=torch.long)\n",
    "tgt = torch.randint(1, TGT_VOCAB, (B, T_tgt), dtype=torch.long)\n",
    "\n",
    "# masks: 1 where real token, 0 where padding (here no padding)\n",
    "src_mask = (src != 0).unsqueeze(-2)       # (B, 1, T_src)\n",
    "tgt_mask = (tgt != 0).unsqueeze(-2)      # (B, 1, T_tgt)\n",
    "\n",
    "# subsequent_mask for causal decoding\n",
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent = torch.triu(torch.ones(attn_shape), diagonal=1).bool()\n",
    "    return ~subsequent\n",
    "\n",
    "sub = subsequent_mask(T_tgt)              # (1, T_tgt, T_tgt)\n",
    "tgt_mask = tgt_mask.unsqueeze(1) & sub   # (B, 1, T_tgt, T_tgt)\n",
    "\n",
    "torch.save({\n",
    "    'src': src,\n",
    "    'tgt': tgt,\n",
    "    'src_mask': src_mask,\n",
    "    'tgt_mask': tgt_mask,\n",
    "    'src_vocab': SRC_VOCAB,\n",
    "    'tgt_vocab': TGT_VOCAB\n",
    "}, 'data.pt')\n",
    "\n",
    "print(\"Saved data.pt with shapes:\")\n",
    "print(\"src\", src.shape, \"tgt\", tgt.shape, \"src_mask\", src_mask.shape, \"tgt_mask\", tgt_mask.shape)\n"
   ],
   "id": "93c9c983304d6593",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved data.pt with shapes:\n",
      "src torch.Size([2, 5]) tgt torch.Size([2, 6]) src_mask torch.Size([2, 1, 5]) tgt_mask torch.Size([2, 1, 6, 6])\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T19:16:20.958991Z",
     "start_time": "2025-11-09T19:16:20.872915Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# run_smoketest.py\n",
    "import torch\n",
    "# from transformer_model import make_model, PositionalEncoding, Embeddings, Generator, DEVICE\n",
    "\n",
    "data = torch.load('data.pt')\n",
    "src = data['src'].to(DEVICE)\n",
    "tgt = data['tgt'].to(DEVICE)\n",
    "src_mask = data['src_mask'].to(DEVICE)\n",
    "tgt_mask = data['tgt_mask'].to(DEVICE)\n",
    "src_vocab = data['src_vocab']\n",
    "tgt_vocab = data['tgt_vocab']\n",
    "\n",
    "print(\"Device:\", DEVICE)\n",
    "print(\"Loaded src shape:\", src.shape, \"tgt shape:\", tgt.shape)\n",
    "print(\"src_mask\", src_mask.shape, \"tgt_mask\", tgt_mask.shape)\n",
    "\n",
    "# Build the small model (matching sizes from transformer_model.py make_model defaults)\n",
    "model = make_model(src_vocab, tgt_vocab, N=2, d_model=32, d_ff=64, h=4, dropout=0.1)\n",
    "model.train()\n",
    "\n",
    "out = model(src, tgt, src_mask, tgt_mask)  # (B, T_tgt, d_model)\n",
    "print(\"Output shape:\", out.shape)\n",
    "\n",
    "# generator to log-probabilities\n",
    "log_probs = model.generator(out)  # (B, T_tgt, V)\n",
    "print(\"Log probs shape:\", log_probs.shape)\n",
    "\n",
    "# quick numerical checks\n",
    "print(\"NaN in outputs:\", torch.isnan(log_probs).any().item())\n",
    "print(\"Inf in outputs:\", torch.isinf(log_probs).any().item())\n",
    "\n",
    "# toy target and loss\n",
    "targets = torch.randint(0, tgt_vocab, (src.size(0), tgt.size(1))).to(DEVICE)\n",
    "loss_fn = torch.nn.NLLLoss()\n",
    "loss = loss_fn(log_probs.view(-1, tgt_vocab), targets.view(-1))\n",
    "print(\"Loss:\", loss.item())\n",
    "loss.backward()\n",
    "print(\"Backward OK. Gradient present on first param:\", next(model.parameters()).grad is not None)\n",
    "\n",
    "\n"
   ],
   "id": "d61ab855e9d864fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Loaded src shape: torch.Size([2, 5]) tgt shape: torch.Size([2, 6])\n",
      "src_mask torch.Size([2, 1, 5]) tgt_mask torch.Size([2, 1, 6, 6])\n",
      "mask matrix\n",
      "tensor([[[[True, True, True, True, True]]],\n",
      "\n",
      "\n",
      "        [[[True, True, True, True, True]]]])\n",
      "tensor([[[[-0.4995,  0.1880, -3.3225,  0.0419, -2.6722],\n",
      "          [-0.1723,  0.4228, -0.7855, -0.8842, -1.0246],\n",
      "          [-0.3008,  0.5816, -1.0868,  0.4911, -0.6556],\n",
      "          [ 1.0010, -0.2941, -0.3300, -0.8776, -0.1582],\n",
      "          [-0.3688,  0.8469, -2.0946,  0.2840, -1.4589]],\n",
      "\n",
      "         [[ 0.0201,  0.6344, -0.8660,  0.9146, -1.4362],\n",
      "          [ 1.9139, -0.6253, -0.3232, -1.5265,  1.0153],\n",
      "          [ 1.0377,  0.2003, -0.0792,  0.2176,  0.1688],\n",
      "          [ 1.1756,  0.8227, -0.7845,  0.7713, -0.2874],\n",
      "          [ 1.0643,  1.4644, -1.2174, -0.0057, -0.5145]],\n",
      "\n",
      "         [[ 0.0073, -0.1851,  1.0104, -0.2575,  1.1681],\n",
      "          [-1.0207,  1.3500, -1.1273, -0.1228, -1.5513],\n",
      "          [ 0.7092, -0.4316,  0.6800, -0.6082,  0.7532],\n",
      "          [ 0.8228,  0.1915, -0.6694, -0.3251, -0.8717],\n",
      "          [ 0.8856,  0.3552,  0.1188, -1.1206, -0.0887]],\n",
      "\n",
      "         [[ 0.8341, -0.1414,  0.6895, -0.4183,  0.1845],\n",
      "          [-0.4652,  1.3636,  0.6289, -1.1199, -0.6917],\n",
      "          [-0.1292, -0.1867,  0.4069, -0.3309, -0.3078],\n",
      "          [-0.6120,  0.1911,  0.6051, -0.2847, -0.5618],\n",
      "          [-0.3320, -0.7615,  0.5907, -0.6423, -0.2313]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0070,  2.3519,  0.1191,  2.3678,  0.9141],\n",
      "          [ 0.5314, -1.5883,  0.1151, -1.0994, -0.1726],\n",
      "          [ 0.5023,  0.5008, -0.5450,  0.5925, -0.0205],\n",
      "          [-0.2351, -0.5075,  0.2756, -0.0534,  0.0936],\n",
      "          [ 0.8134, -0.9204,  0.1073, -0.6623, -0.1071]],\n",
      "\n",
      "         [[-0.6524, -0.6557,  0.5387, -0.8762, -0.1150],\n",
      "          [ 1.1604, -0.1984, -0.4168, -0.3246, -1.5185],\n",
      "          [ 0.5927, -2.0474,  1.2862, -1.7404, -1.3720],\n",
      "          [ 0.7254,  0.1001, -0.9574,  0.1439, -0.9149],\n",
      "          [-0.9829,  0.5154, -2.0120,  0.7677,  1.1072]],\n",
      "\n",
      "         [[-0.9302,  0.3550, -0.3370,  0.2459,  0.2503],\n",
      "          [ 1.5973, -0.6434,  1.2929, -0.5366, -0.5075],\n",
      "          [-0.0531, -0.0854,  0.2818, -0.2886, -0.5041],\n",
      "          [ 1.3330, -0.6138,  1.3827, -0.3811, -0.2693],\n",
      "          [-0.4931,  0.1825, -0.8866, -0.1991,  0.1165]],\n",
      "\n",
      "         [[-0.7464,  1.3632, -0.7921,  0.6138,  0.1013],\n",
      "          [ 0.2046,  2.0127,  0.0678,  2.0094,  1.4721],\n",
      "          [-0.3663,  1.0567, -0.3221, -0.1324, -0.1260],\n",
      "          [ 0.0085,  2.0282, -0.4500,  1.6355,  1.2611],\n",
      "          [ 0.4499, -0.4466,  1.4239, -1.0279,  0.1813]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "mask matrix\n",
      "tensor([[[[True, True, True, True, True]]],\n",
      "\n",
      "\n",
      "        [[[True, True, True, True, True]]]])\n",
      "tensor([[[[-6.5073e-01,  5.4134e-01, -1.7733e-01,  5.1854e-01, -1.2567e-01],\n",
      "          [ 4.7600e-01, -3.8017e-01, -2.9868e-01, -6.5340e-02, -6.8729e-01],\n",
      "          [-1.2248e+00,  9.5264e-01, -5.7635e-01,  3.7269e-01,  1.1057e-01],\n",
      "          [-9.8418e-01, -3.3056e-01, -8.8751e-01, -3.6558e-02, -2.4911e-01],\n",
      "          [-1.3047e+00,  1.1691e+00, -6.0636e-01,  1.4409e-01,  2.1125e-02]],\n",
      "\n",
      "         [[-2.7514e-01,  9.1972e-01,  3.6794e-01, -4.1425e-01,  1.2392e-01],\n",
      "          [-5.2785e-01, -3.6276e-01, -6.1528e-01, -1.8359e+00, -7.8887e-01],\n",
      "          [ 2.2891e-01,  5.4568e-01, -9.5627e-01,  3.9843e-01, -1.0373e+00],\n",
      "          [-6.1190e-01, -3.2167e-01, -1.2759e+00, -1.2706e+00, -1.5193e+00],\n",
      "          [ 1.9718e-02,  5.0866e-01, -7.5007e-01,  1.3017e-01, -9.6051e-01]],\n",
      "\n",
      "         [[ 1.6179e+00, -2.6804e-01,  1.9076e+00,  5.1683e-01,  2.3604e+00],\n",
      "          [ 5.7121e-01,  2.4088e-01,  5.8679e-01,  3.3627e-01,  9.5350e-01],\n",
      "          [ 8.1988e-01, -2.0355e-01,  5.7264e-01, -2.6066e-01,  7.8865e-01],\n",
      "          [-1.0066e+00, -1.4619e+00, -1.0535e+00, -2.8560e+00, -4.0656e-01],\n",
      "          [ 1.3491e+00, -5.8957e-02,  9.9176e-01, -2.9974e-01,  9.5817e-01]],\n",
      "\n",
      "         [[ 5.4935e-01, -1.0197e+00,  2.7853e-02,  8.4553e-01,  1.5609e-01],\n",
      "          [-7.8383e-01, -2.5142e-01,  6.5584e-01, -2.4483e-01,  5.2758e-01],\n",
      "          [-1.3056e-01, -9.1444e-01,  3.1436e-01,  6.8830e-02,  2.7067e-01],\n",
      "          [ 1.2903e+00, -1.8009e+00,  1.5267e+00, -3.4499e-01,  1.5730e+00],\n",
      "          [ 3.3781e-01, -8.2471e-01,  5.6519e-01, -4.6932e-01,  2.8095e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.5737e-01, -7.9729e-01, -4.8104e-01, -1.1379e-01, -8.7199e-01],\n",
      "          [ 1.9219e-01, -1.1188e+00,  5.9671e-01, -6.8864e-01, -1.5236e+00],\n",
      "          [ 3.3157e-01, -8.9226e-01, -1.4435e-01, -1.6156e+00, -9.2973e-01],\n",
      "          [ 4.9672e-01, -1.1990e+00,  9.1664e-01, -5.2877e-01, -1.3378e+00],\n",
      "          [ 1.1352e+00, -4.7091e-01,  5.7406e-01,  8.0215e-02, -1.0194e+00]],\n",
      "\n",
      "         [[ 8.2410e-02,  1.6479e+00,  1.1014e+00,  1.5317e+00,  1.0900e+00],\n",
      "          [ 2.2598e-01,  6.0603e-01,  2.7869e-01,  6.4176e-01, -5.6440e-01],\n",
      "          [ 5.4392e-01, -1.5143e-01, -3.5452e-01, -1.0914e-01, -1.1616e+00],\n",
      "          [ 4.1700e-01,  3.6650e-01, -1.6684e-01,  3.8796e-01, -7.5477e-01],\n",
      "          [ 3.4335e-01,  2.5549e-01, -6.5635e-02,  3.3121e-01, -8.9815e-01]],\n",
      "\n",
      "         [[ 1.1704e+00,  1.5878e+00,  1.1745e+00,  1.1954e+00,  1.2344e+00],\n",
      "          [ 1.0564e-03, -1.0481e+00, -1.2674e+00, -1.2906e+00, -3.7117e-01],\n",
      "          [ 7.6461e-01, -2.6769e+00, -8.9846e-01, -3.0319e+00, -8.9729e-01],\n",
      "          [ 2.9206e-01, -1.0782e+00, -5.6535e-01, -1.4949e+00, -2.3521e-01],\n",
      "          [-6.8132e-01,  1.6119e-01, -9.6961e-01, -1.6874e-01,  4.8684e-01]],\n",
      "\n",
      "         [[-3.2326e-01,  7.1997e-01,  8.4156e-01,  4.1079e-01,  4.2544e-01],\n",
      "          [ 9.4303e-01, -8.3025e-01, -6.1694e-01, -2.1508e+00,  3.5322e-02],\n",
      "          [-7.4488e-02,  6.4657e-01,  2.6119e-01, -3.0845e-01,  4.6536e-01],\n",
      "          [ 1.1840e+00, -8.3929e-01, -2.7772e-01, -1.9788e+00,  2.1255e-01],\n",
      "          [ 5.8827e-01, -6.8744e-01, -1.1250e+00, -2.1775e+00,  3.4773e-02]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "mask matrix\n",
      "tensor([[[[ True, False, False, False, False, False],\n",
      "          [ True,  True, False, False, False, False],\n",
      "          [ True,  True,  True, False, False, False],\n",
      "          [ True,  True,  True,  True, False, False],\n",
      "          [ True,  True,  True,  True,  True, False],\n",
      "          [ True,  True,  True,  True,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[ True, False, False, False, False, False],\n",
      "          [ True,  True, False, False, False, False],\n",
      "          [ True,  True,  True, False, False, False],\n",
      "          [ True,  True,  True,  True, False, False],\n",
      "          [ True,  True,  True,  True,  True, False],\n",
      "          [ True,  True,  True,  True,  True,  True]]]])\n",
      "tensor([[[[-4.1190e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 1.0104e+00,  2.1603e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 1.9007e-01,  1.6371e+00,  4.5742e-02, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 1.1605e+00,  1.7656e+00,  1.3729e+00,  2.3558e+00, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-2.6325e-01,  1.8750e+00,  3.1245e-01,  2.6288e+00,  1.4586e+00,\n",
      "           -1.0000e+09],\n",
      "          [ 3.8232e-01, -2.2422e-01, -1.5471e+00, -1.0992e-01, -6.1675e-01,\n",
      "            4.0031e-01]],\n",
      "\n",
      "         [[-4.0060e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 7.0728e-02, -7.0449e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-1.0913e+00, -2.3569e+00, -1.5818e+00, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-3.5584e-01, -1.4531e+00, -9.1273e-01, -1.1530e+00, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-2.8469e-01, -7.1950e-01,  3.2318e-01, -7.4085e-01, -6.0340e-01,\n",
      "           -1.0000e+09],\n",
      "          [ 4.3729e-01, -5.8015e-02,  1.1751e+00, -8.1631e-02,  8.1362e-01,\n",
      "           -5.0776e-01]],\n",
      "\n",
      "         [[ 9.6387e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-2.9029e-01, -4.2610e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-4.8412e-01,  1.7535e-01,  1.2107e+00, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-3.3280e-01, -4.8809e-01,  2.3549e+00, -7.1108e-02, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 2.3457e-01, -4.6850e-01,  1.2510e+00,  7.6040e-02,  7.5335e-01,\n",
      "           -1.0000e+09],\n",
      "          [-8.6329e-01,  1.5640e+00,  6.5876e-01,  1.5609e+00, -4.6127e-01,\n",
      "            1.1832e+00]],\n",
      "\n",
      "         [[ 1.1871e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-2.5377e-01, -1.0154e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-6.1243e-01, -5.5241e-01, -4.1934e-01, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-2.2128e-01,  4.9747e-02, -6.1087e-02,  1.2285e-01, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 6.9749e-01, -4.1153e-01, -6.2868e-01, -5.3177e-01,  9.8831e-01,\n",
      "           -1.0000e+09],\n",
      "          [-1.5178e+00,  1.7084e+00,  1.3176e-02,  2.2787e+00, -1.7685e+00,\n",
      "            6.3836e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 9.4139e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 6.3457e-01,  4.6061e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-9.6065e-01, -1.7519e+00,  4.9715e-01, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-2.5088e-01, -3.8635e-01, -7.2727e-01, -1.1078e+00, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-2.1564e-01, -9.8761e-01, -8.4584e-02,  1.7065e+00,  2.4443e+00,\n",
      "           -1.0000e+09],\n",
      "          [-4.4316e-01, -6.6267e-01, -8.6876e-01, -8.4986e-01,  1.8201e+00,\n",
      "           -5.6187e-01]],\n",
      "\n",
      "         [[ 1.5283e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 9.1896e-01,  1.6472e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 1.0232e+00,  4.5373e-01, -6.8025e-02, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 7.2823e-01,  3.2866e-01,  4.3366e-01, -6.4819e-01, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 8.7419e-01,  6.0573e-01, -6.8892e-01,  1.6065e+00, -2.4354e-01,\n",
      "           -1.0000e+09],\n",
      "          [ 1.9935e-01, -1.6096e-01,  6.6566e-02,  3.1781e-01,  1.9564e-01,\n",
      "            2.0677e-01]],\n",
      "\n",
      "         [[-4.0222e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-6.8725e-01, -1.4853e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 1.1517e-01, -3.5894e-01, -9.2925e-01, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-8.9612e-02,  1.4269e-01,  2.6148e-01, -5.6703e-02, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-2.6179e-01, -1.8489e-01, -4.9506e-01,  9.8557e-01, -6.2446e-01,\n",
      "           -1.0000e+09],\n",
      "          [-7.7482e-02,  1.6221e-01, -1.7197e-01,  2.0068e-01, -1.8259e+00,\n",
      "            1.0651e-01]],\n",
      "\n",
      "         [[ 3.1163e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 3.0880e-01,  5.1480e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 2.3668e-01,  3.3539e-01, -1.1654e+00, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-1.1326e+00, -9.5303e-01, -1.9828e-01,  1.7188e+00, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-6.4033e-02, -5.0346e-01, -7.4982e-01, -3.7569e-01,  1.8477e+00,\n",
      "           -1.0000e+09],\n",
      "          [-1.8137e+00, -1.6157e+00, -4.5260e-01,  2.5697e+00, -8.9267e-01,\n",
      "            1.7084e+00]]]], grad_fn=<MaskedFillBackward0>)\n",
      "mask matrix\n",
      "tensor([[[[True, True, True, True, True]]],\n",
      "\n",
      "\n",
      "        [[[True, True, True, True, True]]]])\n",
      "tensor([[[[-7.0649e-01, -7.2765e-01, -2.0215e-01, -9.6844e-01, -6.8494e-01],\n",
      "          [-1.9152e+00, -7.9211e-01, -7.6053e-01,  3.5776e-01, -1.0521e+00],\n",
      "          [ 7.2946e-01, -9.9109e-01,  6.3861e-01, -7.2160e-01,  6.2408e-01],\n",
      "          [-1.4663e+00, -1.0205e+00, -1.0935e+00,  1.1830e-01, -1.2444e+00],\n",
      "          [-5.6831e-01, -8.4878e-01, -5.3122e-01,  1.4666e-01, -4.7376e-01],\n",
      "          [ 1.4514e+00, -3.2153e-01,  1.3289e+00, -7.4527e-01,  9.9461e-01]],\n",
      "\n",
      "         [[ 2.1857e+00,  8.7609e-01,  1.1152e+00,  6.9545e-01,  1.0292e+00],\n",
      "          [ 9.6820e-01,  1.2384e+00,  3.5323e-01,  4.8394e-01,  1.7220e-01],\n",
      "          [-4.4927e-01, -4.3517e-01,  4.3362e-01,  1.5831e-01,  2.8041e-01],\n",
      "          [-4.1175e-01,  4.0190e-01,  3.2324e-01,  3.9040e-02, -4.6396e-02],\n",
      "          [ 9.5430e-01,  1.5741e+00,  2.4196e-01,  2.9669e-01,  1.4424e-02],\n",
      "          [-1.5184e+00, -1.1830e+00, -4.3003e-01, -5.1637e-01, -4.9756e-01]],\n",
      "\n",
      "         [[ 8.7179e-01, -3.4030e-01,  8.2695e-01,  1.3351e+00,  8.2067e-01],\n",
      "          [-3.9939e-01, -1.0690e+00, -1.3986e+00, -1.0347e+00, -1.0080e+00],\n",
      "          [-6.2394e-01, -1.0723e-01, -1.5209e-01, -5.3709e-01, -3.0943e-01],\n",
      "          [ 2.9597e-01, -7.0315e-01, -5.3462e-01, -3.5008e-01, -1.0062e-01],\n",
      "          [-1.2871e+00, -6.1082e-01, -8.0294e-01, -2.5338e-02, -1.0373e+00],\n",
      "          [ 1.4904e+00,  5.4355e-01,  4.1966e-01,  1.1968e+00,  8.3998e-01]],\n",
      "\n",
      "         [[-6.6550e-01, -6.9533e-01, -1.2143e+00, -9.5127e-01, -1.2293e+00],\n",
      "          [-1.9523e-02, -3.6317e-01, -5.8198e-01, -2.6260e-01, -8.6808e-01],\n",
      "          [ 3.1520e-01,  4.3261e-01,  7.2872e-01,  6.3073e-01,  3.9451e-01],\n",
      "          [ 2.0410e-01, -6.1661e-01, -4.3239e-01, -5.8500e-01, -1.0090e+00],\n",
      "          [-4.4162e-02,  7.5011e-02,  4.1690e-01,  1.4227e-01,  2.6388e-01],\n",
      "          [-4.8322e-03, -3.1045e-02,  2.6208e-01,  2.6599e-01,  5.6760e-02]]],\n",
      "\n",
      "\n",
      "        [[[-9.9335e-01,  7.5694e-01, -7.1501e-01, -2.2696e-01,  1.0805e+00],\n",
      "          [ 2.3442e-01,  1.4671e+00,  4.5471e-02,  1.9078e+00,  1.1972e+00],\n",
      "          [ 2.3899e+00,  6.8802e-01,  7.2070e-01,  2.4535e+00,  3.8149e-01],\n",
      "          [ 1.6461e+00, -1.6993e+00,  1.4115e-01, -1.0494e+00, -1.4196e+00],\n",
      "          [ 3.3671e-01,  9.8171e-01,  6.8768e-01,  1.8592e+00,  9.3674e-02],\n",
      "          [ 7.4159e-01, -1.8587e+00,  1.3271e-03, -1.7753e+00, -1.7035e+00]],\n",
      "\n",
      "         [[ 2.1887e+00,  9.0728e-01,  5.2935e-01,  7.4075e-01, -1.3099e-01],\n",
      "          [ 1.5516e+00,  6.0832e-01,  9.6901e-01,  4.6530e-01,  1.4980e-01],\n",
      "          [ 7.3167e-01,  2.6133e-01,  8.1095e-01, -2.3700e-01,  4.0479e-01],\n",
      "          [-1.1817e+00, -8.9314e-01,  1.6105e-01, -1.0673e+00,  4.1440e-03],\n",
      "          [-1.7929e-01,  6.4874e-01,  5.4677e-03,  5.6149e-01,  4.6384e-01],\n",
      "          [-9.6360e-01, -4.7106e-01, -4.3794e-01, -4.9388e-01, -1.6143e-01]],\n",
      "\n",
      "         [[ 1.5495e-01, -1.5104e-01,  1.5277e-01,  9.3872e-02,  7.5452e-01],\n",
      "          [-1.4762e-01, -2.5571e-01, -4.7915e-01, -4.7320e-01, -1.9012e-01],\n",
      "          [-1.1848e-01, -3.0710e-01, -2.1464e-01, -7.5606e-01, -2.2045e-01],\n",
      "          [-4.4397e-02, -8.9964e-01, -1.6626e-01, -9.6947e-01, -3.4214e-01],\n",
      "          [-1.2567e+00, -3.3183e+00, -3.5775e+00, -3.2113e+00, -3.8717e+00],\n",
      "          [ 2.2037e-01, -1.2257e+00, -8.2642e-01, -1.3304e+00, -9.2931e-01]],\n",
      "\n",
      "         [[-8.4546e-01, -5.6420e-01, -1.2455e+00, -8.1223e-01, -5.8430e-01],\n",
      "          [-3.1578e-01,  7.4893e-01, -1.3350e-01,  8.2390e-01,  3.0207e-01],\n",
      "          [-5.1812e-01,  2.4550e+00,  7.1302e-01,  3.5383e+00,  2.2120e+00],\n",
      "          [ 1.3697e+00,  9.5714e-01,  1.2503e+00,  5.9918e-01,  8.9932e-01],\n",
      "          [-1.1907e+00, -1.3525e+00, -1.5594e+00, -1.0812e+00, -1.4688e+00],\n",
      "          [ 6.2341e-01, -1.0305e-01,  6.2666e-01, -2.9144e-01,  1.7578e-01]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "mask matrix\n",
      "tensor([[[[ True, False, False, False, False, False],\n",
      "          [ True,  True, False, False, False, False],\n",
      "          [ True,  True,  True, False, False, False],\n",
      "          [ True,  True,  True,  True, False, False],\n",
      "          [ True,  True,  True,  True,  True, False],\n",
      "          [ True,  True,  True,  True,  True,  True]]],\n",
      "\n",
      "\n",
      "        [[[ True, False, False, False, False, False],\n",
      "          [ True,  True, False, False, False, False],\n",
      "          [ True,  True,  True, False, False, False],\n",
      "          [ True,  True,  True,  True, False, False],\n",
      "          [ True,  True,  True,  True,  True, False],\n",
      "          [ True,  True,  True,  True,  True,  True]]]])\n",
      "tensor([[[[ 2.9322e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 3.4378e+00,  1.7711e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-2.4897e-01,  1.0187e-03,  5.1441e-01, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 2.7789e+00,  1.2598e+00,  9.4494e-01,  1.2103e+00, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-2.6630e+00, -1.7395e+00, -9.7307e-01, -1.4260e+00, -2.3737e+00,\n",
      "           -1.0000e+09],\n",
      "          [ 2.9912e-01,  6.4207e-02,  1.2554e+00,  3.5426e-02, -5.6799e-01,\n",
      "            1.4614e+00]],\n",
      "\n",
      "         [[ 1.1805e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-7.0371e-01, -6.3931e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 4.2307e-01,  1.0879e+00,  5.1115e-02, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-6.1933e-01, -4.0723e-01,  2.2679e-01, -6.5840e-01, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 7.6472e-01,  1.3568e+00, -5.2900e-01,  1.5008e-01, -1.0645e-01,\n",
      "           -1.0000e+09],\n",
      "          [-5.9952e-01,  6.9044e-01, -3.2278e-02,  2.7074e-01, -6.1274e-01,\n",
      "           -5.6216e-01]],\n",
      "\n",
      "         [[-5.6576e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-2.0672e-02,  1.3380e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 1.3453e-01,  4.8932e-01, -2.9565e-01, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-1.4711e+00,  1.6717e-03,  2.5915e-02,  3.1605e-01, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 3.0328e-01,  8.3759e-01, -6.9612e-02,  4.7949e-01,  1.7223e+00,\n",
      "           -1.0000e+09],\n",
      "          [ 2.3804e+00,  1.7077e+00, -1.2114e+00,  8.3254e-01,  8.0510e-01,\n",
      "           -1.3579e-01]],\n",
      "\n",
      "         [[-1.4401e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-1.0243e-01, -1.7176e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 9.1469e-01,  5.5885e-01, -1.4484e-01, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 4.7770e-01, -9.8827e-01,  7.3898e-01, -5.0238e-02, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-1.8239e+00, -1.6712e+00,  4.8862e-01, -1.8316e+00, -2.7833e-01,\n",
      "           -1.0000e+09],\n",
      "          [ 6.8032e-01,  9.3134e-01, -7.3844e-01,  1.0026e+00, -1.7199e-01,\n",
      "           -1.2851e+00]]],\n",
      "\n",
      "\n",
      "        [[[-1.2744e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-1.4126e+00, -1.5614e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 7.7267e-02, -1.3035e+00, -5.1183e-01, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 2.0198e+00,  1.2802e+00,  1.1349e+00,  7.3123e-01, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-5.8993e-01, -5.6463e-01,  2.9854e-01,  2.7992e-01, -6.8432e-01,\n",
      "           -1.0000e+09],\n",
      "          [ 2.2251e+00,  2.2184e+00,  1.9965e+00,  5.3950e-01, -1.0475e+00,\n",
      "           -5.3014e-01]],\n",
      "\n",
      "         [[-2.1018e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-1.1099e-01, -6.4928e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-2.4048e-01, -1.2949e-01,  6.0867e-01, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 9.2317e-01,  9.2916e-01,  5.8471e-01, -6.2017e-01, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-1.2230e+00, -2.2382e-01,  6.3964e-01,  5.2151e-01,  1.2560e+00,\n",
      "           -1.0000e+09],\n",
      "          [-4.3483e-01, -6.4240e-02, -5.3464e-01, -6.3393e-01,  5.9322e-02,\n",
      "           -1.1114e-01]],\n",
      "\n",
      "         [[ 3.3236e-01, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-4.0738e-01, -1.0381e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 1.5100e+00,  3.2316e-01,  3.7006e-01, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 4.0745e+00,  3.0289e+00,  2.7446e+00,  1.2463e+00, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-8.4249e-01, -1.3889e+00, -5.7873e-01,  5.9620e-01,  2.7676e-01,\n",
      "           -1.0000e+09],\n",
      "          [ 2.6790e+00,  2.2371e+00,  1.8367e+00,  1.4799e-01,  1.2308e+00,\n",
      "           -4.3496e-01]],\n",
      "\n",
      "         [[ 1.4332e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 1.1970e+00,  1.7095e+00, -1.0000e+09, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 4.4594e-01,  3.0240e-01,  8.5681e-02, -1.0000e+09, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [-4.6754e-01, -3.9143e-01,  4.2298e-01,  1.1469e+00, -1.0000e+09,\n",
      "           -1.0000e+09],\n",
      "          [ 2.3404e+00, -3.2276e-01, -1.2496e+00, -2.0818e-01,  1.4247e+00,\n",
      "           -1.0000e+09],\n",
      "          [-1.0317e+00, -8.6881e-01,  1.7390e-01,  1.5528e+00,  1.0660e-01,\n",
      "            1.3231e+00]]]], grad_fn=<MaskedFillBackward0>)\n",
      "mask matrix\n",
      "tensor([[[[True, True, True, True, True]]],\n",
      "\n",
      "\n",
      "        [[[True, True, True, True, True]]]])\n",
      "tensor([[[[-1.7306,  1.1937,  0.2218, -0.9389,  0.0527],\n",
      "          [ 0.7234,  0.5282,  0.9977,  0.8382,  0.5997],\n",
      "          [-0.8164, -1.7512, -0.2964, -0.2633, -0.6471],\n",
      "          [ 0.5543, -0.0716,  0.2046,  0.4347, -0.2201],\n",
      "          [-1.2208, -0.8186, -1.1397, -0.7748, -1.6500],\n",
      "          [-0.4559, -0.9039, -0.6484, -0.5465, -1.0047]],\n",
      "\n",
      "         [[ 1.3672,  0.7263, -0.2468,  1.1561, -0.1489],\n",
      "          [ 0.4409, -0.7205,  0.6802, -0.2140,  0.0913],\n",
      "          [ 0.4760,  1.2860, -0.8237,  0.2390, -0.4829],\n",
      "          [-0.0269, -0.6568,  0.4256,  0.2803,  0.2428],\n",
      "          [ 0.5464,  1.2106, -0.3711,  1.1724, -0.0255],\n",
      "          [ 1.1183,  2.0664,  0.2592,  2.4782,  0.7537]],\n",
      "\n",
      "         [[-0.4587, -0.2449,  0.6825, -0.9269,  0.5322],\n",
      "          [ 0.4580,  1.0445,  0.8227, -0.7770,  1.0042],\n",
      "          [-0.2896,  0.4436, -0.7232,  0.4609, -0.8972],\n",
      "          [-0.7381,  0.8918,  0.2550, -1.0514,  0.3024],\n",
      "          [ 0.2096, -0.1159,  0.5922, -1.1998,  0.5162],\n",
      "          [-0.2924, -0.5946,  0.1484, -1.5169, -0.2213]],\n",
      "\n",
      "         [[ 0.5862,  0.6154, -1.0970, -1.6694, -0.7215],\n",
      "          [ 0.5337,  0.2807, -1.6157, -3.0983, -1.5829],\n",
      "          [-0.3609,  0.6502, -0.2329, -0.1259, -0.4490],\n",
      "          [ 0.0874,  0.6919, -1.6364, -3.1011, -1.6519],\n",
      "          [ 0.1108,  0.9699,  0.7831, -0.9851,  0.6897],\n",
      "          [-0.5639,  0.1163, -1.2806, -1.1364, -1.4367]]],\n",
      "\n",
      "\n",
      "        [[[-0.5752, -2.9546, -0.4099, -2.2347, -1.4944],\n",
      "          [-1.2063, -0.7333, -1.3298, -0.5043, -0.4488],\n",
      "          [-2.3106, -1.1251, -2.2666, -1.3457, -2.1132],\n",
      "          [-0.3157, -0.8409, -0.8835, -0.7192, -0.4967],\n",
      "          [-1.9262, -0.7732, -1.8962, -1.0354, -1.5711],\n",
      "          [ 1.5766, -0.2381,  0.8944,  0.3081,  1.2059]],\n",
      "\n",
      "         [[ 1.5408,  1.9520,  1.8054,  1.5076,  0.4339],\n",
      "          [ 0.8268,  1.3108,  0.6008,  0.5533,  0.6011],\n",
      "          [ 3.3811,  2.5634,  3.2759,  1.8136,  0.3827],\n",
      "          [ 2.6372,  1.4948,  3.1307,  1.3432, -0.4006],\n",
      "          [ 2.9200,  0.7773,  3.2698,  1.0483,  0.0186],\n",
      "          [-0.2201,  0.1331,  0.3259,  0.2560, -0.6673]],\n",
      "\n",
      "         [[-0.0342,  0.2432,  0.2689,  0.2982,  1.1660],\n",
      "          [ 0.0387,  0.6017,  0.8185,  0.1597,  1.4702],\n",
      "          [ 0.2186,  0.8664,  0.7065,  0.5326,  1.3962],\n",
      "          [ 0.6944, -1.2570, -0.7302, -0.2122,  0.1246],\n",
      "          [ 0.2673,  1.2337,  1.0624,  0.5426,  1.6706],\n",
      "          [ 0.8792, -1.5381, -0.6835, -0.4903,  0.1600]],\n",
      "\n",
      "         [[-0.0682, -0.2606, -1.0879, -0.3054, -0.4516],\n",
      "          [-0.2535, -0.5659, -0.8682, -1.1138, -0.3839],\n",
      "          [-0.2427,  0.5781,  0.3615,  0.1591,  0.1766],\n",
      "          [ 0.0938, -0.9381, -0.9417, -0.9298, -0.6412],\n",
      "          [ 0.1902,  2.0407,  1.6095,  2.3540,  1.3168],\n",
      "          [ 0.4001, -1.0881, -0.8213, -0.8982, -0.4270]]]],\n",
      "       grad_fn=<MaskedFillBackward0>)\n",
      "Output shape: torch.Size([2, 6, 32])\n",
      "Log probs shape: torch.Size([2, 6, 11])\n",
      "NaN in outputs: False\n",
      "Inf in outputs: False\n",
      "Loss: 2.783055067062378\n",
      "Backward OK. Gradient present on first param: True\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T19:16:20.964497Z",
     "start_time": "2025-11-09T19:16:20.962498Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "2b3e3c3cc9df512c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-09T19:16:20.997760Z",
     "start_time": "2025-11-09T19:16:20.995760Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "9013338d43dbd0f1",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
