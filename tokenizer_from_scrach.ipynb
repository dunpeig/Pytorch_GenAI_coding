{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad2c7d0",
   "metadata": {},
   "source": [
    "Tokenization is at the heart of much weirdness of LLMs. Do not brush it off.\n",
    "\n",
    "Why can't LLM spell words? Tokenization.\n",
    "Why can't LLM do super simple string processing tasks like reversing a string? Tokenization.\n",
    "Why is LLM worse at non-English languages (e.g. Japanese)? Tokenization.\n",
    "Why is LLM bad at simple arithmetic? Tokenization.\n",
    "Why did GPT-2 have more than necessary trouble coding in Python? Tokenization.\n",
    "Why did my LLM abruptly halt when it sees the string \"<|endoftext|>\"? Tokenization.\n",
    "What is this weird warning I get about a \"trailing whitespace\"? Tokenization.\n",
    "Why the LLM break if I ask it about \"SolidGoldMagikarp\"? Tokenization.\n",
    "Why should I prefer to use YAML over JSON with LLMs? Tokenization.\n",
    "Why is LLM not actually end-to-end language modeling? Tokenization.\n",
    "What is the real root of suffering? Tokenization.\n",
    "\n",
    "reference: https://www.fast.ai/posts/2025-10-16-karpathy-tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a5380fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " !,Tadehilmnorstwx\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "text = 'This is some text dataset hello, and hi some words!'\n",
    "# get the unique characters that occur in this text\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(''.join(chars))\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ae0ddfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7, 8, 8, 0, 15, 7, 6, 13, 6]\n",
      "hii there\n"
     ]
    }
   ],
   "source": [
    "stoi = {ch:i for i, ch in enumerate(chars)}\n",
    "itos = {i:ch for i,ch in enumerate(chars)}\n",
    "encode = lambda s: [stoi[c] for c in s]\n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "print(encode(\"hii there\"))\n",
    "print(decode(encode(\"hii there\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3c13d49",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(encode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat if out of vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(decode(encode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhat if out of vocabulary\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m      1\u001b[0m stoi \u001b[38;5;241m=\u001b[39m {ch:i \u001b[38;5;28;01mfor\u001b[39;00m i, ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chars)}\n\u001b[0;32m      2\u001b[0m itos \u001b[38;5;241m=\u001b[39m {i:ch \u001b[38;5;28;01mfor\u001b[39;00m i,ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chars)}\n\u001b[1;32m----> 3\u001b[0m encode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m s: [stoi[c] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m s]\n\u001b[0;32m      4\u001b[0m decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m l: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([itos[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m l])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(encode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhii there\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m stoi \u001b[38;5;241m=\u001b[39m {ch:i \u001b[38;5;28;01mfor\u001b[39;00m i, ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chars)}\n\u001b[0;32m      2\u001b[0m itos \u001b[38;5;241m=\u001b[39m {i:ch \u001b[38;5;28;01mfor\u001b[39;00m i,ch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(chars)}\n\u001b[1;32m----> 3\u001b[0m encode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m s: [stoi[c] \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m s]\n\u001b[0;32m      4\u001b[0m decode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m l: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([itos[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m l])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(encode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhii there\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mKeyError\u001b[0m: 'f'"
     ]
    }
   ],
   "source": [
    "print(encode(\"what if out of vocabulary\"))\n",
    "print(decode(encode(\"what if out of vocabulary\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b00e78ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([51]) torch.int64\n",
      "tensor([ 3,  7,  8, 14,  0,  8, 14,  0, 14, 12, 10,  6,  0, 15,  6, 17, 15,  0,\n",
      "         5,  4, 15,  4, 14,  6, 15,  0,  7,  6,  9,  9, 12,  2,  0,  4, 11,  5,\n",
      "         0,  7,  8,  0, 14, 12, 10,  6,  0, 16, 12, 13,  5, 14,  1])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9be4c670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50504,\n",
       " 45397,\n",
       " 54616,\n",
       " 49464,\n",
       " 50836,\n",
       " 32,\n",
       " 128075,\n",
       " 32,\n",
       " 40,\n",
       " 104,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 32,\n",
       " 105,\n",
       " 110,\n",
       " 32,\n",
       " 75,\n",
       " 111,\n",
       " 114,\n",
       " 101,\n",
       " 97,\n",
       " 110,\n",
       " 33,\n",
       " 41]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ord(x)for x in \"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ (hello in Korean!)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e580f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# byte-pair encoding (BPE), work with character chunks \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f40d34f",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ hello world ðŸ¤—\n",
      "ord('h') = 104\n",
      "ord('ðŸ¤—') = 129303\n",
      "ord('ì•ˆ') = 50504\n"
     ]
    }
   ],
   "source": [
    "text = \"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ hello world ðŸ¤—\"\n",
    "print(text)\n",
    "\n",
    "# Python documentation defines strings as immutable sequences of Unicode code points. \n",
    "# Unicode code points are defined by the Unicode Consortium as part of the Unicode standard. \n",
    "# The standard defines roughly 150,000 characters across 161 scripts, specifying what these characters look like and what \n",
    "# integers represent them. The standard continues to evolveâ€”version 15.1 was released in September 2023.\n",
    "\n",
    "\n",
    "# Get Unicode code point for English character\n",
    "print(f\"ord('h') = {ord('h')}\")\n",
    "\n",
    "# Get Unicode code point for emoji\n",
    "print(f\"ord('ðŸ¤—') = {ord('ðŸ¤—')}\")\n",
    "\n",
    "# Get Unicode code point for Korean character\n",
    "print(f\"ord('ì•ˆ') = {ord('ì•ˆ')}\")\n",
    "\n",
    "# Get Unicode code points for each character in the string\n",
    "text = \"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ hello world ðŸ¤—\"\n",
    "L([ord(x) for x in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04070e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTF-8: [236, 149, 136, 235, 133, 149, 237, 149, 152, 236, 132, 184, 236, 154, 148, 32, 240, 159, 145, 139, 32, 104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 32, 240, 159, 164, 151]\n",
      "UTF-16: [255, 254, 72, 197, 85, 177, 88, 213, 56, 193, 148, 198, 32, 0, 61, 216, 75, 220, 32, 0, 104, 0, 101, 0, 108, 0, 108, 0, 111, 0, 32, 0, 119, 0, 111, 0, 114, 0, 108, 0, 100, 0, 32, 0, 62, 216, 23, 221]\n",
      "UTF-32: [255, 254, 0, 0, 72, 197, 0, 0, 85, 177, 0, 0, 88, 213, 0, 0, 56, 193, 0, 0, 148, 198, 0, 0, 32, 0, 0, 0, 75, 244, 1, 0, 32, 0, 0, 0, 104, 0, 0, 0, 101, 0, 0, 0, 108, 0, 0, 0, 108, 0, 0, 0, 111, 0, 0, 0, 32, 0, 0, 0, 119, 0, 0, 0, 111, 0, 0, 0, 114, 0, 0, 0, 108, 0, 0, 0, 100, 0, 0, 0, 32, 0, 0, 0, 23, 249, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "# UTF-8, UTF-16, and UTF-32. These encodings translate Unicode text into binary data \n",
    "# or byte strings. UTF-8 is by far the most common.\n",
    "\n",
    "text = \"ì•ˆë…•í•˜ì„¸ìš” ðŸ‘‹ hello world ðŸ¤—\"\n",
    "\n",
    "# UTF-8 encoding\n",
    "utf8_bytes = list(text.encode('utf-8'))\n",
    "print(f\"UTF-8: {utf8_bytes}\")\n",
    "\n",
    "# UTF-16 encoding  \n",
    "utf16_bytes = list(text.encode('utf-16'))\n",
    "print(f\"UTF-16: {utf16_bytes}\")\n",
    "\n",
    "# UTF-32 encoding  \n",
    "utf32_bytes = list(text.encode('utf-32'))\n",
    "print(f\"UTF-32: {utf32_bytes}\")\n",
    "\n",
    "# note too many zeros or zero something in 16 and 32 bit encodings, which makes it wasteful in memory.\n",
    "\n",
    "# While UTF-8 is the preferred choice, using it naively presents a challenge. The byte streams imply a \n",
    "# vocabulary of only 256 possible tokens. This vocabulary size is extremely small, resulting in text stretched\n",
    "# across very long sequences of bytes.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#  itâ€™s worth noting that feeding raw byte sequences directly into language models would be ideal. A paper from \n",
    "# summer 2023 explores this possibility.\n",
    "\n",
    "# The challenge is that the transformer architecture requires modification to handle raw bytes.\n",
    "# As mentioned earlier, attention becomes extremely expensive with such long sequences.\n",
    "\n",
    "\n",
    "# The Byte Pair Encoding algorithm is relatively straightforward, and the Wikipedia page provides a clear explanation \n",
    "# of the basic concept. The algorithm operates on an input sequenceâ€”for example, a sequence containing only four\n",
    "# vocabulary elements: a, b, c, and d. Rather than working with bytes directly, \n",
    "# consider this simplified case with a vocabulary size of four.\n",
    "\n",
    "# When a sequence becomes too long and requires compression, the algorithm iteratively identifies the most frequently \n",
    "# occurring pair of tokens. Once identified, that pair is replaced with a single new token appended to the vocabulary. \n",
    "# For instance, if the byte pair â€˜aaâ€™ occurs most often, we create a new token (call it capital Z) and replace every \n",
    "# occurrence of â€˜aaâ€™ with Z, resulting in two Zâ€™s in the sequence.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36acd6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: ï¼µï½Žï½‰ï½ƒï½ï½„ï½…! ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½ ðŸ‡ºâ€ŒðŸ‡³â€ŒðŸ‡®â€ŒðŸ‡¨â€ŒðŸ‡´â€ŒðŸ‡©â€ŒðŸ‡ª! ðŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide.\n",
      " We all know we ought to \"support Unicode\" in our software (whatever that meansâ€”like using wchar_t for all the strings, right?).\n",
      "   But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes,\n",
      "     reports, and notes can be more than a little intimidating. I don't blame programmers for still finding the whole thing\n",
      "       mysterious, even 30 years after Unicode's inception.\n",
      "Length in characters: 549\n",
      "UTF-8 encoded bytes: [239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133]...\n",
      "Length in bytes: 624\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Get the sample text from Nathan Reed's blog post\n",
    "text = \"\"\"ï¼µï½Žï½‰ï½ƒï½ï½„ï½…! ðŸ…¤ðŸ…ðŸ…˜ðŸ…’ðŸ…žðŸ…“ðŸ…”â€½ ðŸ‡ºâ€ŒðŸ‡³â€ŒðŸ‡®â€ŒðŸ‡¨â€ŒðŸ‡´â€ŒðŸ‡©â€ŒðŸ‡ª! ðŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide.\n",
    " We all know we ought to \"support Unicode\" in our software (whatever that meansâ€”like using wchar_t for all the strings, right?).\n",
    "   But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes,\n",
    "     reports, and notes can be more than a little intimidating. I don't blame programmers for still finding the whole thing\n",
    "       mysterious, even 30 years after Unicode's inception.\"\"\"\n",
    "\n",
    "print(f\"Text: {text}\")\n",
    "print(f\"Length in characters: {len(text)}\")\n",
    "\n",
    "\n",
    "# Step 2: Encode the text to UTF-8 bytes and convert to list of integers\n",
    "tokens = list(text.encode(\"utf-8\"))\n",
    "print(f\"UTF-8 encoded bytes: {tokens[:50]}...\")  # Show first 50 bytes\n",
    "print(f\"Length in bytes: {len(tokens)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51d79feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample list: [1, 2, 3, 4, 5]\n",
      "Consecutive pairs: [(1, 2), (2, 3), (3, 4), (4, 5)]\n",
      "This is the 'Pythonic way' Andrej mentions for iterating consecutive elements\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_stats(ids, counts=None):\n",
    "    \"\"\"\n",
    "    Given a list of integers, return a dictionary of counts of consecutive pairs\n",
    "    Example: [1, 2, 3, 1, 2] -> {(1, 2): 2, (2, 3): 1, (3, 1): 1}\n",
    "    Optionally allows to update an existing dictionary of counts\n",
    "    \"\"\"\n",
    "    counts = {} if counts is None else counts\n",
    "    for pair in zip(ids, ids[1:]): # iterate consecutive elements\n",
    "        counts[pair] = counts.get(pair, 0) + 1\n",
    "    return counts\n",
    "\n",
    "\n",
    "# Step 3a: Understand how zip(ids, ids[1:]) works for consecutive pairs\n",
    "sample_list = [1, 2, 3, 4, 5]\n",
    "consecutive_pairs = list(zip(sample_list, sample_list[1:]))\n",
    "print(f\"Sample list: {sample_list}\")\n",
    "print(f\"Consecutive pairs: {consecutive_pairs}\")\n",
    "print(\"This is the 'Pythonic way' Andrej mentions for iterating consecutive elements\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c9403f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique pairs: 272\n",
      "\n",
      "Top 10 most frequent pairs:\n",
      "  (101, 32): 20 times\n",
      "  (240, 159): 15 times\n",
      "  (105, 110): 12 times\n",
      "  (32, 32): 12 times\n",
      "  (115, 32): 10 times\n",
      "  (97, 110): 10 times\n",
      "  (32, 97): 10 times\n",
      "  (32, 116): 9 times\n",
      "  (226, 128): 8 times\n",
      "  (116, 104): 8 times\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 3: Find the most common consecutive pair using get_stats\n",
    "stats = get_stats(tokens)\n",
    "print(f\"Total number of unique pairs: {len(stats)}\")\n",
    "\n",
    "# Show top 10 most frequent pairs\n",
    "top_pairs = sorted([(count, pair) for pair, count in stats.items()], reverse=True)[:10]\n",
    "print(\"\\nTop 10 most frequent pairs:\")\n",
    "for count, pair in top_pairs:\n",
    "    print(f\"  {pair}: {count} times\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e56a904d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent pair: (101, 32)\n",
      "Occurs 20 times\n",
      "This represents: 'e' + ' '\n",
      "Found 20 occurrences of pair (101, 32) ('e' + ' ') at positions:\n",
      "Positions: [110, 120, 141, 150, 187, 199, 242, 270, 296, 329, 336, 366, 380, 388, 470, 475, 489, 517, 551, 557]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 4: Get the most frequent pair using max() function\n",
    "most_frequent_pair = max(stats, key=stats.get)\n",
    "print(f\"Most frequent pair: {most_frequent_pair}\")\n",
    "print(f\"Occurs {stats[most_frequent_pair]} times\")\n",
    "\n",
    "# Convert bytes back to characters to see what this pair represents\n",
    "char1 = chr(most_frequent_pair[0])\n",
    "char2 = chr(most_frequent_pair[1])\n",
    "print(f\"This represents: '{char1}' + '{char2}'\")\n",
    "\n",
    "\n",
    "# Step 4a: Verify the most frequent pair by finding its occurrences in the text\n",
    "pair_to_find = most_frequent_pair  # (101, 32) which is 'e' + ' '\n",
    "\n",
    "# Find all positions where this pair occurs\n",
    "occurrences = []\n",
    "for i in range(len(tokens) - 1):\n",
    "    if tokens[i] == pair_to_find[0] and tokens[i + 1] == pair_to_find[1]:\n",
    "        occurrences.append(i)\n",
    "\n",
    "print(f\"Found {len(occurrences)} occurrences of pair {pair_to_find} ('e' + ' ') at positions:\")\n",
    "print(f\"Positions: {occurrences}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ce1ca45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will replace pair (101, 32) with new token ID: 256\n",
      "Ready to implement merge function...\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Prepare to merge - create new token ID\n",
    "# Current tokens are 0-255 (256 possible values), so new token will be 256\n",
    "new_token_id = 256\n",
    "print(f\"Will replace pair {most_frequent_pair} with new token ID: {new_token_id}\")\n",
    "print(f\"Ready to implement merge function...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5c530ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Implement the merge function\n",
    "def merge(ids, pair, idx):\n",
    "    \"\"\"\n",
    "    In the list of integers (ids), replace all consecutive occurrences \n",
    "    of pair with the new integer token idx\n",
    "    Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
    "    \"\"\"\n",
    "    newids = []\n",
    "    i = 0\n",
    "    while i < len(ids):\n",
    "        # if not at the very last position AND the pair matches, replace it\n",
    "        if ids[i] == pair[0] and i < len(ids) - 1 and ids[i+1] == pair[1]:\n",
    "            newids.append(idx)\n",
    "            i += 2  # skip over the pair\n",
    "        else:\n",
    "            newids.append(ids[i])\n",
    "            i += 1\n",
    "    return newids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db09c5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: [5, 6, 6, 7, 9, 1]\n",
      "After merging (6, 7) -> 99: [5, 6, 99, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "# Test with simple example\n",
    "test_ids = [5, 6, 6, 7, 9, 1]\n",
    "result = merge(test_ids, (6, 7), 99)\n",
    "print(f\"Original: {test_ids}\")\n",
    "print(f\"After merging (6, 7) -> 99: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b743824d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length: 624\n",
      "After merge length: 604\n",
      "Reduction: 20 tokens\n",
      "\n",
      "Occurrences of new token 256: 20\n",
      "Occurrences of old pair in original: 20\n",
      "Occurrences of old pair in new tokens: 0\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Apply merge to our actual tokens\n",
    "# Merge the most frequent pair (101, 32) with token ID 256\n",
    "tokens2 = merge(tokens, most_frequent_pair, new_token_id)\n",
    "\n",
    "print(f\"Original length: {len(tokens)}\")\n",
    "print(f\"After merge length: {len(tokens2)}\")\n",
    "print(f\"Reduction: {len(tokens) - len(tokens2)} tokens\")\n",
    "\n",
    "# Verify the merge worked\n",
    "print(f\"\\nOccurrences of new token {new_token_id}: {tokens2.count(new_token_id)}\")\n",
    "print(f\"Occurrences of old pair in original: {sum(1 for i in range(len(tokens)-1) if (tokens[i], tokens[i+1]) == most_frequent_pair)}\")\n",
    "\n",
    "# Verify old pair is gone\n",
    "old_pair_count = sum(1 for i in range(len(tokens2)-1) if (tokens2[i], tokens2[i+1]) == most_frequent_pair)\n",
    "print(f\"Occurrences of old pair in new tokens: {old_pair_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a66cdb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPE Training Progress:\n",
      "Step 0: 624 tokens, vocab size: 256\n",
      "Step 1: 604 tokens, vocab size: 257\n",
      "Step 2: 589 tokens, vocab size: 258\n",
      "  Merged pair: (240, 159) -> 257\n",
      "Step 3: 577 tokens, vocab size: 259\n",
      "  Merged pair: (105, 110) -> 258\n",
      "Step 4: 571 tokens, vocab size: 260\n",
      "  Merged pair: (32, 32) -> 259\n",
      "Step 5: 561 tokens, vocab size: 261\n",
      "  Merged pair: (115, 32) -> 260\n",
      "\n",
      "Final: 561 tokens, vocab size: 261\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Iterate the BPE algorithm\n",
    "# Now we repeat: find most common pair, merge it, repeat...\n",
    "# Let's do a few more iterations\n",
    "\n",
    "current_tokens = tokens2\n",
    "vocab_size = 257  # Started with 256, now have 257\n",
    "\n",
    "print(\"BPE Training Progress:\")\n",
    "print(f\"Step 0: {len(tokens)} tokens, vocab size: 256\")\n",
    "print(f\"Step 1: {len(current_tokens)} tokens, vocab size: {vocab_size}\")\n",
    "\n",
    "# Do a few more iterations\n",
    "for step in range(2, 6):  # Steps 2-5\n",
    "    # Find most common pair\n",
    "    stats = get_stats(current_tokens)\n",
    "    if not stats:  # No more pairs to merge\n",
    "        break\n",
    "    \n",
    "    most_frequent_pair = max(stats, key=stats.get)\n",
    "    \n",
    "    # Merge it\n",
    "    current_tokens = merge(current_tokens, most_frequent_pair, vocab_size)\n",
    "    \n",
    "    print(f\"Step {step}: {len(current_tokens)} tokens, vocab size: {vocab_size + 1}\")\n",
    "    print(f\"  Merged pair: {most_frequent_pair} -> {vocab_size}\")\n",
    "    \n",
    "    vocab_size += 1\n",
    "\n",
    "print(f\"\\nFinal: {len(current_tokens)} tokens, vocab size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "47c42464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 256: (101, 32) -> 'e' + ' ' = 'e '\n",
      "Token 257: (100, 32) -> 'd' + ' ' = 'd '\n",
      "Token 258: (116, 101) -> 't' + 'e' = 'te'\n",
      "Token 259: (115, 32) -> 's' + ' ' = 's '\n",
      "Token 260: (105, 110) -> 'i' + 'n' = 'in'\n"
     ]
    }
   ],
   "source": [
    "# Track the merges we made\n",
    "merges = {\n",
    "    256: (101, 32),  # 'e' + ' '\n",
    "    257: (100, 32),  # 'd' + ' '  \n",
    "    258: (116, 101), # 't' + 'e'\n",
    "    259: (115, 32),  # 's' + ' '\n",
    "    260: (105, 110)  # 'i' + 'n'\n",
    "}\n",
    "\n",
    "for token_id, (byte1, byte2) in merges.items():\n",
    "    char1, char2 = chr(byte1), chr(byte2)\n",
    "    print(f\"Token {token_id}: ({byte1}, {byte2}) -> '{char1}' + '{char2}' = '{char1}{char2}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "21554864",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTF-8 encoded bytes: [239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133]...\n",
      "Length in bytes: 624\n"
     ]
    }
   ],
   "source": [
    "# text = full blog post text copied from the colab notebook\n",
    "tokens = list(text.encode(\"utf-8\"))\n",
    "print(f\"UTF-8 encoded bytes: {tokens[:50]}...\")  # Show first 50 bytes\n",
    "print(f\"Length in bytes: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c731fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merge 1/20: (101, 32) -> 256 (20 occurrences)\n",
      "merge 2/20: (240, 159) -> 257 (15 occurrences)\n",
      "merge 3/20: (105, 110) -> 258 (12 occurrences)\n",
      "merge 4/20: (32, 32) -> 259 (12 occurrences)\n",
      "merge 5/20: (115, 32) -> 260 (10 occurrences)\n",
      "merge 6/20: (97, 110) -> 261 (10 occurrences)\n",
      "merge 7/20: (226, 128) -> 262 (8 occurrences)\n",
      "merge 8/20: (116, 104) -> 263 (8 occurrences)\n",
      "merge 9/20: (257, 133) -> 264 (7 occurrences)\n",
      "merge 10/20: (257, 135) -> 265 (7 occurrences)\n",
      "merge 11/20: (97, 114) -> 266 (7 occurrences)\n",
      "merge 12/20: (239, 189) -> 267 (6 occurrences)\n",
      "merge 13/20: (262, 140) -> 268 (6 occurrences)\n",
      "merge 14/20: (268, 265) -> 269 (6 occurrences)\n",
      "merge 15/20: (101, 114) -> 270 (6 occurrences)\n",
      "merge 16/20: (111, 114) -> 271 (6 occurrences)\n",
      "merge 17/20: (116, 32) -> 272 (6 occurrences)\n",
      "merge 18/20: (258, 103) -> 273 (6 occurrences)\n",
      "merge 19/20: (115, 116) -> 274 (5 occurrences)\n",
      "merge 20/20: (261, 100) -> 275 (5 occurrences)\n"
     ]
    }
   ],
   "source": [
    "# BPE training\n",
    "vocab_size = 276  # hyperparameter: the desired final vocabulary size\n",
    "num_merges = vocab_size - 256\n",
    "tokens = list(text.encode(\"utf-8\"))\n",
    "\n",
    "for i in range(num_merges):\n",
    "    # count up all the pairs\n",
    "    stats = get_stats(tokens)\n",
    "    # find the pair with the highest count\n",
    "    pair = max(stats, key=stats.get)\n",
    "    # mint a new token: assign it the next available id\n",
    "    idx = 256 + i\n",
    "    # replace all occurrences of pair in tokens with idx\n",
    "    tokens = merge(tokens, pair, idx)\n",
    "    # print progress\n",
    "    print(f\"merge {i+1}/{num_merges}: {pair} -> {idx} ({stats[pair]} occurrences)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "427df1c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte\n",
      "ï¿½\n"
     ]
    }
   ],
   "source": [
    "# The tokenizer has its own training set of documents, potentially different from the LLMâ€™s training set.\n",
    "# Training the tokenizer uses the Byte Pair Encoding algorithm to create the vocabulary. Once \n",
    "# trained with its vocabulary and merges, the tokenizer can perform both encoding and decodingâ€”translating \n",
    "# between raw text (sequences of Unicode code points) and token sequences in both directions.\n",
    "\n",
    "# Intuitively, including substantial Japanese data in the tokenizer training set results in\n",
    "# more Japanese token merges, producing shorter token sequences for Japanese text. This benefits the large\n",
    "#  language model, which operates with finite context length in token space.\n",
    "\n",
    "\n",
    "\n",
    "# Decoding\n",
    "\n",
    "# Track the merges we made\n",
    "merges = {\n",
    "    (101, 32) : 256,  # 'e' + ' '\n",
    "    (100, 32) : 257,  # 'd' + ' '  \n",
    "    (116, 101) : 258, # 't' + 'e'\n",
    "    (115, 32) : 259,  # 's' + ' '\n",
    "    (105, 110): 260  # 'i' + 'n'\n",
    "}\n",
    "# given ids (list of integers), return Python string\n",
    "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
    "for (p0, p1), idx in merges.items():\n",
    "    vocab[idx] = vocab[p0] + vocab[p1]\n",
    "\n",
    "def decode(ids):\n",
    "    # given ids, get tokens\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    # convert from bytes to string\n",
    "    text = tokens.decode(\"utf-8\")\n",
    "    return text\n",
    "\n",
    "print(decode([97]))  # Should work fine\n",
    "try:print(decode([128]))  # This will cause UnicodeDecodeError\n",
    "except Exception as e: print(str(e))\n",
    "\n",
    "def decode(ids):\n",
    "    # given ids (list of integers), return Python string\n",
    "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
    "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
    "    return text\n",
    "\n",
    "try:print(decode([128]))  # This should now print the replacement character without error\n",
    "except Exception as e: print(str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d9a47606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111, 32, 119, 111, 114, 108, 100, 33]\n",
      "hello world!\n",
      "min() arg is an empty sequence\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[104]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def encode(text):\n",
    "    # given a string, return list of integers (the tokens)\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while True:\n",
    "        stats = get_stats(tokens)\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break # nothing else can be merged\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "print(encode(\"hello world!\"))\n",
    "print(decode(encode(\"hello world!\")))\n",
    "\n",
    "try: print(encode('h'))\n",
    "except Exception as e: print(e)\n",
    "\n",
    "def encode(text):\n",
    "    # given a string, return list of integers (the tokens)\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    while True:\n",
    "        stats = get_stats(tokens)\n",
    "        if len(tokens) < 2:\n",
    "            break  # nothing to merge\n",
    "        pair = min(stats, key=lambda p: merges.get(p, float(\"inf\")))\n",
    "        if pair not in merges:\n",
    "            break # nothing else can be merged\n",
    "        idx = merges[pair]\n",
    "        tokens = merge(tokens, pair, idx)\n",
    "    return tokens\n",
    "\n",
    "encode('h')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7e157f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: 'Hello world'\n",
      "Matches: ['Hello', ' world']\n",
      "Number of chunks: 2\n",
      "Text: 'Hello world how are you?'\n",
      "Matches: ['Hello', ' world', ' how', ' are', ' you', '?']\n",
      "Number of chunks: 6\n"
     ]
    }
   ],
   "source": [
    "# GPT-2 Encoder with regex pattern\n",
    "class Encoder:\n",
    "    def __init__(self, encoder, bpe_merges, errors='replace'):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = {v:k for k,v in self.encoder.items()}\n",
    "        self.bpe_merges = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "\n",
    "import regex as re\n",
    "\n",
    "pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "# Test the regex pattern on simple text\n",
    "text = \"Hello world\"\n",
    "matches = pat.findall(text)\n",
    "print(f\"Text: '{text}'\")\n",
    "print(f\"Matches: {matches}\")\n",
    "print(f\"Number of chunks: {len(matches)}\")\n",
    "\n",
    "# Test with more complex text including punctuation\n",
    "text2 = \"Hello world how are you?\"\n",
    "matches2 = pat.findall(text2)\n",
    "print(f\"Text: '{text2}'\")\n",
    "print(f\"Matches: {matches2}\")\n",
    "print(f\"Number of chunks: {len(matches2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f1ea8c7b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tiktoken'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtiktoken\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Compare GPT-2 vs GPT-4 tokenization\u001b[39;00m\n\u001b[0;32m      3\u001b[0m enc_gpt2 \u001b[38;5;241m=\u001b[39m tiktoken\u001b[38;5;241m.\u001b[39mget_encoding(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tiktoken'"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "# Compare GPT-2 vs GPT-4 tokenization\n",
    "enc_gpt2 = tiktoken.get_encoding(\"gpt2\")\n",
    "enc_gpt4 = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "tokens_gpt2 = enc_gpt2.encode(example)\n",
    "tokens_gpt4 = enc_gpt4.encode(example)\n",
    "\n",
    "print(f\"GPT-2 tokens: {len(tokens_gpt2)}\")\n",
    "print(f\"GPT-4 tokens: {len(tokens_gpt4)}\")\n",
    "\n",
    "decoded_gpt4 = [enc_gpt4.decode([token]) for token in tokens_gpt4] \n",
    "for i, token_str in enumerate(decoded_gpt4): \n",
    "    if token_str.strip() == '': print(f\"Token {i}: {repr(token_str)} (all whitespace)\")\n",
    "\n",
    "\n",
    "# GPT-2 tokenizer pattern from tiktoken openai_public.py\n",
    "def gpt2():\n",
    "    mergeable_ranks = data_gym_to_mergeable_bpe_ranks(\n",
    "        vocab_bpe_file=\"https://openaipublic.blob.core.windows.net/gpt-2/encodings/main/vocab.bpe\",\n",
    "        encoder_json_file=\"https://openaipublic.blob.core.windows.net/gpt-2/encodings/main/encoder.json\",\n",
    "        vocab_bpe_hash=\"1ce1664773c50f3e0cc8842619a93edc4624525b728b188a9e0be33b7726adc5\",\n",
    "        encoder_json_hash=\"196139668be63f3b5d6574427317ae82f612a97c5d1cdaf36ed2256dbf636783\",\n",
    "    )\n",
    "    return {\n",
    "        \"name\": \"gpt2\",\n",
    "        \"explicit_n_vocab\": 50257,\n",
    "        # The pattern in the original GPT-2 release is:\n",
    "        # r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        # This is equivalent, but executes faster:\n",
    "        \"pat_str\": r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}++| ?\\p{N}++| ?[^\\s\\p{L}\\p{N}]++|\\s++$|\\s+(?!\\S)|\\s\"\"\",\n",
    "        \"mergeable_ranks\": mergeable_ranks,\n",
    "        \"special_tokens\": {\"<|endoftext|>\": 50256},\n",
    "    }\n",
    "\n",
    "# GPT-4 tokenizer pattern from tiktoken openai_public.py\n",
    "def cl100k_base():\n",
    "    mergeable_ranks = load_tiktoken_bpe(\n",
    "        \"https://openaipublic.blob.core.windows.net/encodings/cl100k_base.tiktoken\"\n",
    "    )\n",
    "    special_tokens = {\n",
    "        \"<|endoftext|>\": 100257,\n",
    "        \"<|fim_prefix|>\": 100258,\n",
    "        \"<|fim_middle|>\": 100259,\n",
    "        \"<|fim_suffix|>\": 100260,\n",
    "        \"<|endofprompt|>\": 100276\n",
    "    }\n",
    "    return {\n",
    "        \"name\": \"cl100k_base\", \n",
    "        \"explicit_n_vocab\": 100277,\n",
    "        # Different pattern from GPT-2 - handles whitespace better\n",
    "        \"pat_str\": r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{2,}|[^\\r\\n\\p{L}\\p{N}]?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\",\n",
    "        \"mergeable_ranks\": mergeable_ranks,\n",
    "        \"special_tokens\": special_tokens,\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8ce547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
