{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "class Word2Vec(nn.Module):\n",
        "    \"\"\"Word2Vec implementation with Skip-gram and CBOW architectures\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, model_type='skipgram'):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.model_type = model_type\n",
        "\n",
        "        # Input embeddings (center word)\n",
        "        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # Output embeddings (context words)\n",
        "        self.out_embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Initialize embeddings\n",
        "        self.in_embed.weight.data.uniform_(-0.5/embedding_dim, 0.5/embedding_dim)\n",
        "        self.out_embed.weight.data.uniform_(-0.5/embedding_dim, 0.5/embedding_dim)\n",
        "\n",
        "    def forward(self, center, context, neg_samples):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            center: center word indices [batch_size]\n",
        "            context: context word indices [batch_size]\n",
        "            neg_samples: negative sample indices [batch_size, num_neg_samples]\n",
        "        \"\"\"\n",
        "        # Get embeddings\n",
        "        center_embed = self.in_embed(center)  # [batch_size, embed_dim]\n",
        "        context_embed = self.out_embed(context)  # [batch_size, embed_dim]\n",
        "        neg_embed = self.out_embed(neg_samples)  # [batch_size, num_neg, embed_dim]\n",
        "\n",
        "        # Positive score\n",
        "        pos_score = torch.sum(center_embed * context_embed, dim=1)  # [batch_size]\n",
        "        pos_score = torch.log(torch.sigmoid(pos_score) + 1e-10)\n",
        "\n",
        "        # Negative scores\n",
        "        neg_score = torch.bmm(neg_embed, center_embed.unsqueeze(2)).squeeze(2)  # [batch_size, num_neg]\n",
        "        neg_score = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-10), dim=1)  # [batch_size]\n",
        "\n",
        "        # Negative sampling loss\n",
        "        loss = -(pos_score + neg_score).mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class Word2VecTrainer:\n",
        "    \"\"\"Trainer for Word2Vec model\"\"\"\n",
        "\n",
        "    def __init__(self, sentences, embedding_dim=100, window_size=5,\n",
        "                 min_count=5, num_neg_samples=5, model_type='skipgram'):\n",
        "        self.sentences = sentences\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.window_size = window_size\n",
        "        self.min_count = min_count\n",
        "        self.num_neg_samples = num_neg_samples\n",
        "        self.model_type = model_type\n",
        "\n",
        "        # Build vocabulary\n",
        "        self.build_vocab()\n",
        "\n",
        "        # Initialize model\n",
        "        self.model = Word2Vec(len(self.word2idx), embedding_dim, model_type)\n",
        "\n",
        "        # Prepare negative sampling table\n",
        "        self.prepare_neg_sampling()\n",
        "\n",
        "    def build_vocab(self):\n",
        "        \"\"\"Build vocabulary from sentences\"\"\"\n",
        "        word_counts = Counter()\n",
        "        for sentence in self.sentences:\n",
        "            word_counts.update(sentence.lower().split())\n",
        "\n",
        "        # Filter by min_count\n",
        "        self.vocab = [word for word, count in word_counts.items()\n",
        "                      if count >= self.min_count]\n",
        "\n",
        "        # Create mappings\n",
        "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
        "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
        "\n",
        "        # Store word frequencies for negative sampling\n",
        "        self.word_freq = np.array([word_counts[word] for word in self.vocab])\n",
        "\n",
        "        print(f\"Vocabulary size: {len(self.vocab)}\")\n",
        "\n",
        "    def prepare_neg_sampling(self):\n",
        "        \"\"\"Prepare negative sampling distribution (raised to 3/4 power)\"\"\"\n",
        "        freq = self.word_freq ** 0.75\n",
        "        self.neg_sampling_probs = freq / freq.sum()\n",
        "        self.neg_sampling_table = np.random.choice(\n",
        "            len(self.vocab),\n",
        "            size=1000000,\n",
        "            p=self.neg_sampling_probs\n",
        "        )\n",
        "        self.neg_sample_idx = 0\n",
        "\n",
        "    def get_negative_samples(self, batch_size):\n",
        "        \"\"\"Sample negative examples\"\"\"\n",
        "        samples = []\n",
        "        for _ in range(batch_size):\n",
        "            neg = []\n",
        "            for _ in range(self.num_neg_samples):\n",
        "                neg.append(self.neg_sampling_table[self.neg_sample_idx])\n",
        "                self.neg_sample_idx = (self.neg_sample_idx + 1) % len(self.neg_sampling_table)\n",
        "            samples.append(neg)\n",
        "        return torch.LongTensor(samples)\n",
        "\n",
        "    def generate_training_data(self):\n",
        "        \"\"\"Generate training pairs for Skip-gram\"\"\"\n",
        "        pairs = []\n",
        "\n",
        "        for sentence in self.sentences:\n",
        "            words = sentence.lower().split()\n",
        "            indices = [self.word2idx[w] for w in words if w in self.word2idx]\n",
        "\n",
        "            for i, center in enumerate(indices):\n",
        "                # Get context words within window\n",
        "                start = max(0, i - self.window_size)\n",
        "                end = min(len(indices), i + self.window_size + 1)\n",
        "\n",
        "                for j in range(start, end):\n",
        "                    if i != j:\n",
        "                        context = indices[j]\n",
        "                        pairs.append((center, context))\n",
        "\n",
        "        return pairs\n",
        "\n",
        "    def train(self, epochs=5, batch_size=128, lr=0.025):\n",
        "        \"\"\"Train the Word2Vec model\"\"\"\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        # Generate training data\n",
        "        print(\"Generating training data...\")\n",
        "        training_pairs = self.generate_training_data()\n",
        "        print(f\"Training pairs: {len(training_pairs)}\")\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            np.random.shuffle(training_pairs)\n",
        "\n",
        "            for i in range(0, len(training_pairs), batch_size):\n",
        "                batch = training_pairs[i:i+batch_size]\n",
        "\n",
        "                centers = torch.LongTensor([p[0] for p in batch])\n",
        "                contexts = torch.LongTensor([p[1] for p in batch])\n",
        "                neg_samples = self.get_negative_samples(len(batch))\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss = self.model(centers, contexts, neg_samples)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            avg_loss = total_loss / (len(training_pairs) / batch_size)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    def get_word_vector(self, word):\n",
        "        \"\"\"Get embedding vector for a word\"\"\"\n",
        "        if word not in self.word2idx:\n",
        "            return None\n",
        "        idx = self.word2idx[word]\n",
        "        return self.model.in_embed.weight[idx].detach().numpy()\n",
        "\n",
        "    def most_similar(self, word, top_k=5):\n",
        "        \"\"\"Find most similar words using cosine similarity\"\"\"\n",
        "        if word not in self.word2idx:\n",
        "            return []\n",
        "\n",
        "        word_vec = self.get_word_vector(word)\n",
        "        word_vec = word_vec / np.linalg.norm(word_vec)\n",
        "\n",
        "        # Compute cosine similarity with all words\n",
        "        all_vecs = self.model.in_embed.weight.detach().numpy()\n",
        "        all_vecs = all_vecs / np.linalg.norm(all_vecs, axis=1, keepdims=True)\n",
        "\n",
        "        similarities = np.dot(all_vecs, word_vec)\n",
        "\n",
        "        # Get top-k most similar (excluding the word itself)\n",
        "        top_indices = np.argsort(similarities)[::-1][1:top_k+1]\n",
        "\n",
        "        results = [(self.idx2word[idx], similarities[idx]) for idx in top_indices]\n",
        "        return results\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample corpus\n",
        "    corpus = [\n",
        "        \"the quick brown fox jumps over the lazy dog\",\n",
        "        \"the dog is lazy and sleeps all day\",\n",
        "        \"the fox is quick and clever\",\n",
        "        \"a quick brown dog jumps high\",\n",
        "        \"the lazy cat sleeps on the mat\",\n",
        "        \"quick movements scare the lazy animals\",\n",
        "        \"brown and white dogs play together\",\n",
        "        \"the clever fox outsmarts the dog\",\n",
        "        \"lazy afternoon with sleeping animals\",\n",
        "        \"quick thinking saves the day\"\n",
        "    ] * 100  # Repeat for more training data\n",
        "\n",
        "    # Initialize and train\n",
        "    print(\"Initializing Word2Vec trainer...\")\n",
        "    trainer = Word2VecTrainer(\n",
        "        sentences=corpus,\n",
        "        embedding_dim=50,\n",
        "        window_size=3,\n",
        "        min_count=1,\n",
        "        num_neg_samples=5\n",
        "    )\n",
        "\n",
        "    print(\"\\nTraining Word2Vec model...\")\n",
        "    trainer.train(epochs=10, batch_size=32, lr=0.01)\n",
        "\n",
        "    # Test similarity\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Testing word similarities:\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    test_words = ['quick', 'lazy', 'dog', 'fox']\n",
        "    for word in test_words:\n",
        "        print(f\"\\nMost similar to '{word}':\")\n",
        "        similar = trainer.most_similar(word, top_k=3)\n",
        "        for sim_word, score in similar:\n",
        "            print(f\"  {sim_word}: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGgvoOH2PYkl",
        "outputId": "52b30614-a0a8-4b50-ea9a-996be41d7c55"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Word2Vec trainer...\n",
            "Vocabulary size: 32\n",
            "\n",
            "Training Word2Vec model...\n",
            "Generating training data...\n",
            "Training pairs: 26400\n",
            "Epoch 1/10, Loss: 2.0622\n",
            "Epoch 2/10, Loss: 1.8783\n",
            "Epoch 3/10, Loss: 1.8707\n",
            "Epoch 4/10, Loss: 1.8637\n",
            "Epoch 5/10, Loss: 1.8579\n",
            "Epoch 6/10, Loss: 1.8637\n",
            "Epoch 7/10, Loss: 1.8564\n",
            "Epoch 8/10, Loss: 1.8502\n",
            "Epoch 9/10, Loss: 1.8514\n",
            "Epoch 10/10, Loss: 1.8531\n",
            "\n",
            "==================================================\n",
            "Testing word similarities:\n",
            "==================================================\n",
            "\n",
            "Most similar to 'quick':\n",
            "  outsmarts: 0.6379\n",
            "  high: 0.5505\n",
            "  over: 0.4867\n",
            "\n",
            "Most similar to 'lazy':\n",
            "  mat: 0.5903\n",
            "  cat: 0.4333\n",
            "  on: 0.4141\n",
            "\n",
            "Most similar to 'dog':\n",
            "  clever: 0.6050\n",
            "  jumps: 0.5332\n",
            "  fox: 0.4984\n",
            "\n",
            "Most similar to 'fox':\n",
            "  clever: 0.5291\n",
            "  a: 0.5200\n",
            "  dog: 0.4984\n"
          ]
        }
      ]
    }
  ]
}