{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "import re\n",
        "\n",
        "# Word2Vec learns to represent words as dense vectors (embeddings), predict\n",
        "# which words appear near each other in text\n",
        "# skip-gram, fpr each word pair (center word, context word) that appears\n",
        "# within a window in the corpus, postive examples (words that actually appear\n",
        "# together, like \"cat\", \"meow\") . maxmize the probability that the center word\n",
        "# predicts the context word\n",
        "# negative sampling, instead of computing over all words, sample a few negative\n",
        "# examples (words that don't appear in context). Make dot products large for\n",
        "# actual context words, make dot product small for random words\n",
        "class Word2Vec(nn.Module):\n",
        "    \"\"\"Word2Vec implementation with Skip-gram and CBOW architectures\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embedding_dim, model_type='skipgram'):\n",
        "        super(Word2Vec, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.model_type = model_type\n",
        "\n",
        "        # Input embeddings (center word)\n",
        "        # nn.Embedding create a matrix of shape [vocab_size, embedding_dim]\n",
        "        # given a word index, simply retrieves row i\n",
        "        self.in_embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "        # Output embeddings (context words), differentiable\n",
        "        # after training, only in_embed is kept as final word embeddings\n",
        "        self.out_embed = nn.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # Initialize embeddings\n",
        "        self.in_embed.weight.data.uniform_(-0.5/embedding_dim, 0.5/embedding_dim)\n",
        "        self.out_embed.weight.data.uniform_(-0.5/embedding_dim, 0.5/embedding_dim)\n",
        "\n",
        "    def forward(self, center, context, neg_samples):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            center: center word indices [batch_size]\n",
        "            context: context word indices [batch_size]\n",
        "            neg_samples: negative sample indices [batch_size, num_neg_samples]\n",
        "        \"\"\"\n",
        "        # Get embeddings\n",
        "        center_embed = self.in_embed(center)  # [batch_size, embed_dim]\n",
        "        context_embed = self.out_embed(context)  # [batch_size, embed_dim]\n",
        "        neg_embed = self.out_embed(neg_samples)  # [batch_size, num_neg, embed_dim]\n",
        "\n",
        "        # Positive score\n",
        "        pos_score = torch.sum(center_embed * context_embed, dim=1)  # [batch_size]\n",
        "        pos_score = torch.log(torch.sigmoid(pos_score) + 1e-10)\n",
        "\n",
        "        # Negative scores\n",
        "        neg_score = torch.bmm(neg_embed, center_embed.unsqueeze(2)).squeeze(2)  # [batch_size, num_neg]\n",
        "        neg_score = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-10), dim=1)  # [batch_size]\n",
        "\n",
        "        # Negative sampling loss\n",
        "        loss = -(pos_score + neg_score).mean()\n",
        "\n",
        "        return loss\n",
        "\n",
        "\n",
        "class Word2VecTrainer:\n",
        "    \"\"\"Trainer for Word2Vec model\"\"\"\n",
        "\n",
        "    def __init__(self, sentences, embedding_dim=100, window_size=5,\n",
        "                 min_count=5, num_neg_samples=5, model_type='skipgram'):\n",
        "        self.sentences = sentences\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.window_size = window_size\n",
        "        self.min_count = min_count\n",
        "        self.num_neg_samples = num_neg_samples\n",
        "        self.model_type = model_type\n",
        "\n",
        "        # Build vocabulary\n",
        "        self.build_vocab()\n",
        "\n",
        "        # Initialize model\n",
        "        self.model = Word2Vec(len(self.word2idx), embedding_dim, model_type)\n",
        "\n",
        "        # Prepare negative sampling table\n",
        "        self.prepare_neg_sampling()\n",
        "\n",
        "    def build_vocab(self):\n",
        "        \"\"\"Build vocabulary from sentences\"\"\"\n",
        "        word_counts = Counter()\n",
        "        for sentence in self.sentences:\n",
        "            word_counts.update(sentence.lower().split())\n",
        "\n",
        "        # Filter by min_count\n",
        "        self.vocab = [word for word, count in word_counts.items()\n",
        "                      if count >= self.min_count]\n",
        "\n",
        "        # Create mappings\n",
        "        self.word2idx = {word: idx for idx, word in enumerate(self.vocab)}\n",
        "        self.idx2word = {idx: word for word, idx in self.word2idx.items()}\n",
        "\n",
        "        # Store word frequencies for negative sampling\n",
        "        self.word_freq = np.array([word_counts[word] for word in self.vocab])\n",
        "\n",
        "        print(f\"Vocabulary size: {len(self.vocab)}\")\n",
        "\n",
        "    def prepare_neg_sampling(self):\n",
        "        \"\"\"Prepare negative sampling distribution (raised to 3/4 power)\"\"\"\n",
        "        freq = self.word_freq ** 0.75\n",
        "        self.neg_sampling_probs = freq / freq.sum()\n",
        "        self.neg_sampling_table = np.random.choice(\n",
        "            len(self.vocab),\n",
        "            size=1000000,\n",
        "            p=self.neg_sampling_probs\n",
        "        )\n",
        "        self.neg_sample_idx = 0\n",
        "\n",
        "    def get_negative_samples(self, batch_size):\n",
        "        \"\"\"Sample negative examples\"\"\"\n",
        "        samples = []\n",
        "        for _ in range(batch_size):\n",
        "            neg = []\n",
        "            for _ in range(self.num_neg_samples):\n",
        "                neg.append(self.neg_sampling_table[self.neg_sample_idx])\n",
        "                self.neg_sample_idx = (self.neg_sample_idx + 1) % len(self.neg_sampling_table)\n",
        "            samples.append(neg)\n",
        "        return torch.LongTensor(samples)\n",
        "\n",
        "    def generate_training_data(self):\n",
        "        \"\"\"Generate training pairs for Skip-gram\"\"\"\n",
        "        pairs = []\n",
        "\n",
        "        for sentence in self.sentences:\n",
        "            words = sentence.lower().split()\n",
        "            indices = [self.word2idx[w] for w in words if w in self.word2idx]\n",
        "\n",
        "            for i, center in enumerate(indices):\n",
        "                # Get context words within window\n",
        "                start = max(0, i - self.window_size)\n",
        "                end = min(len(indices), i + self.window_size + 1)\n",
        "\n",
        "                for j in range(start, end):\n",
        "                    if i != j:\n",
        "                        context = indices[j]\n",
        "                        pairs.append((center, context))\n",
        "\n",
        "        return pairs\n",
        "\n",
        "    def train(self, epochs=5, batch_size=128, lr=0.025):\n",
        "        \"\"\"Train the Word2Vec model\"\"\"\n",
        "        optimizer = optim.Adam(self.model.parameters(), lr=lr)\n",
        "\n",
        "        # Generate training data\n",
        "        print(\"Generating training data...\")\n",
        "        training_pairs = self.generate_training_data()\n",
        "        print(f\"Training pairs: {len(training_pairs)}\")\n",
        "\n",
        "        self.model.train()\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            total_loss = 0\n",
        "            np.random.shuffle(training_pairs)\n",
        "\n",
        "            for i in range(0, len(training_pairs), batch_size):\n",
        "                batch = training_pairs[i:i+batch_size]\n",
        "\n",
        "                centers = torch.LongTensor([p[0] for p in batch])\n",
        "                contexts = torch.LongTensor([p[1] for p in batch])\n",
        "                neg_samples = self.get_negative_samples(len(batch))\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss = self.model(centers, contexts, neg_samples)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                total_loss += loss.item()\n",
        "\n",
        "            avg_loss = total_loss / (len(training_pairs) / batch_size)\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    def get_word_vector(self, word):\n",
        "        \"\"\"Get embedding vector for a word\"\"\"\n",
        "        if word not in self.word2idx:\n",
        "            return None\n",
        "        idx = self.word2idx[word]\n",
        "        return self.model.in_embed.weight[idx].detach().numpy()\n",
        "\n",
        "    def most_similar(self, word, top_k=5):\n",
        "        \"\"\"Find most similar words using cosine similarity\"\"\"\n",
        "        if word not in self.word2idx:\n",
        "            return []\n",
        "\n",
        "        word_vec = self.get_word_vector(word)\n",
        "        word_vec = word_vec / np.linalg.norm(word_vec)\n",
        "\n",
        "        # Compute cosine similarity with all words\n",
        "        all_vecs = self.model.in_embed.weight.detach().numpy()\n",
        "        all_vecs = all_vecs / np.linalg.norm(all_vecs, axis=1, keepdims=True)\n",
        "\n",
        "        similarities = np.dot(all_vecs, word_vec)\n",
        "\n",
        "        # Get top-k most similar (excluding the word itself)\n",
        "        top_indices = np.argsort(similarities)[::-1][1:top_k+1]\n",
        "\n",
        "        results = [(self.idx2word[idx], similarities[idx]) for idx in top_indices]\n",
        "        return results\n",
        "\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample corpus\n",
        "    corpus = [\n",
        "        \"the quick brown fox jumps over the lazy dog\",\n",
        "        \"the dog is lazy and sleeps all day\",\n",
        "        \"the fox is quick and clever\",\n",
        "        \"a quick brown dog jumps high\",\n",
        "        \"the lazy cat sleeps on the mat\",\n",
        "        \"quick movements scare the lazy animals\",\n",
        "        \"brown and white dogs play together\",\n",
        "        \"the clever fox outsmarts the dog\",\n",
        "        \"lazy afternoon with sleeping animals\",\n",
        "        \"quick thinking saves the day\"\n",
        "    ] * 100  # Repeat for more training data\n",
        "\n",
        "    # Initialize and train\n",
        "    print(\"Initializing Word2Vec trainer...\")\n",
        "    trainer = Word2VecTrainer(\n",
        "        sentences=corpus,\n",
        "        embedding_dim=50,\n",
        "        window_size=3,\n",
        "        min_count=1,\n",
        "        num_neg_samples=5\n",
        "    )\n",
        "\n",
        "    print(\"\\nTraining Word2Vec model...\")\n",
        "    trainer.train(epochs=10, batch_size=32, lr=0.01)\n",
        "\n",
        "    # Test similarity\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Testing word similarities:\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    test_words = ['quick', 'lazy', 'dog', 'fox']\n",
        "    for word in test_words:\n",
        "        print(f\"\\nMost similar to '{word}':\")\n",
        "        similar = trainer.most_similar(word, top_k=3)\n",
        "        for sim_word, score in similar:\n",
        "            print(f\"  {sim_word}: {score:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGgvoOH2PYkl",
        "outputId": "a7d81789-e94b-416f-ce19-ed59ddbf0d45"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Word2Vec trainer...\n",
            "Vocabulary size: 32\n",
            "\n",
            "Training Word2Vec model...\n",
            "Generating training data...\n",
            "Training pairs: 26400\n",
            "Epoch 1/10, Loss: 2.0676\n",
            "Epoch 2/10, Loss: 1.8739\n",
            "Epoch 3/10, Loss: 1.8655\n",
            "Epoch 4/10, Loss: 1.8576\n",
            "Epoch 5/10, Loss: 1.8606\n",
            "Epoch 6/10, Loss: 1.8584\n",
            "Epoch 7/10, Loss: 1.8592\n",
            "Epoch 8/10, Loss: 1.8588\n",
            "Epoch 9/10, Loss: 1.8536\n",
            "Epoch 10/10, Loss: 1.8574\n",
            "\n",
            "==================================================\n",
            "Testing word similarities:\n",
            "==================================================\n",
            "\n",
            "Most similar to 'quick':\n",
            "  outsmarts: 0.6447\n",
            "  high: 0.5565\n",
            "  fox: 0.4588\n",
            "\n",
            "Most similar to 'lazy':\n",
            "  mat: 0.5642\n",
            "  day: 0.4861\n",
            "  quick: 0.4406\n",
            "\n",
            "Most similar to 'dog':\n",
            "  clever: 0.6392\n",
            "  jumps: 0.5277\n",
            "  over: 0.4887\n",
            "\n",
            "Most similar to 'fox':\n",
            "  a: 0.5424\n",
            "  clever: 0.5423\n",
            "  high: 0.4678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O1JHgeqVy8Od"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "import math\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class TFIDF:\n",
        "    \"\"\"TF-IDF implementation from scratch using PyTorch\"\"\"\n",
        "\n",
        "    def __init__(self, norm='l2', smooth_idf=True, sublinear_tf=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            norm: 'l2' or 'l1' normalization (None for no normalization)\n",
        "            smooth_idf: Add 1 to document frequencies (prevents division by zero)\n",
        "            sublinear_tf: Use log(tf + 1) instead of raw term frequency\n",
        "        \"\"\"\n",
        "        self.norm = norm\n",
        "        self.smooth_idf = smooth_idf\n",
        "        self.sublinear_tf = sublinear_tf\n",
        "        self.vocab = {}  # word -> index mapping\n",
        "        self.idf = None  # IDF weights\n",
        "\n",
        "    def fit(self, documents):\n",
        "        \"\"\"\n",
        "        Learn vocabulary and IDF weights from documents\n",
        "\n",
        "        Args:\n",
        "            documents: List of documents, where each doc is a list of tokens\n",
        "                      e.g., [['hello', 'world'], ['hello', 'pytorch']]\n",
        "        \"\"\"\n",
        "        # Build vocabulary\n",
        "        vocab_set = set()\n",
        "        for doc in documents:\n",
        "            vocab_set.update(doc)\n",
        "\n",
        "        self.vocab = {word: idx for idx, word in enumerate(sorted(vocab_set))}\n",
        "        vocab_size = len(self.vocab)\n",
        "        n_documents = len(documents)\n",
        "\n",
        "        # Calculate document frequency for each term\n",
        "        # df[i] = number of documents containing term i\n",
        "        df = torch.zeros(vocab_size)\n",
        "\n",
        "        for doc in documents:\n",
        "            # Get unique terms in this document\n",
        "            unique_terms = set(doc)\n",
        "            for term in unique_terms:\n",
        "                if term in self.vocab:\n",
        "                    df[self.vocab[term]] += 1\n",
        "\n",
        "        # Calculate IDF: log(N / df)\n",
        "        # With smoothing: log((N + 1) / (df + 1)) + 1\n",
        "        if self.smooth_idf:\n",
        "            idf = torch.log((n_documents + 1) / (df + 1)) + 1\n",
        "        else:\n",
        "            idf = torch.log(n_documents / df)\n",
        "\n",
        "        self.idf = idf\n",
        "        return self\n",
        "\n",
        "    def transform(self, documents):\n",
        "        \"\"\"\n",
        "        Transform documents to TF-IDF matrix\n",
        "\n",
        "        Args:\n",
        "            documents: List of documents (list of tokens)\n",
        "\n",
        "        Returns:\n",
        "            TF-IDF matrix of shape [n_documents, vocab_size]\n",
        "        \"\"\"\n",
        "        if self.idf is None:\n",
        "            raise ValueError(\"Must call fit() before transform()\")\n",
        "\n",
        "        n_documents = len(documents)\n",
        "        vocab_size = len(self.vocab)\n",
        "\n",
        "        # Initialize TF matrix\n",
        "        tf_matrix = torch.zeros(n_documents, vocab_size)\n",
        "\n",
        "        # Calculate term frequencies\n",
        "        for doc_idx, doc in enumerate(documents):\n",
        "            # Count terms in document\n",
        "            term_counts = Counter(doc)\n",
        "\n",
        "            for term, count in term_counts.items():\n",
        "                if term in self.vocab:\n",
        "                    term_idx = self.vocab[term]\n",
        "\n",
        "                    if self.sublinear_tf:\n",
        "                        # Use log scaling: log(1 + tf)\n",
        "                        tf_matrix[doc_idx, term_idx] = math.log(1 + count)\n",
        "                    else:\n",
        "                        # Raw term frequency\n",
        "                        tf_matrix[doc_idx, term_idx] = count\n",
        "\n",
        "        # Apply IDF weights: TF-IDF = TF * IDF\n",
        "        tfidf_matrix = tf_matrix * self.idf.unsqueeze(0)\n",
        "\n",
        "        # Normalize rows\n",
        "        if self.norm == 'l2':\n",
        "            # L2 normalization: each row has unit L2 norm\n",
        "            norms = torch.sqrt((tfidf_matrix ** 2).sum(dim=1, keepdim=True))\n",
        "            norms = torch.clamp(norms, min=1e-10)  # Avoid division by zero\n",
        "            tfidf_matrix = tfidf_matrix / norms\n",
        "\n",
        "        elif self.norm == 'l1':\n",
        "            # L1 normalization: each row sums to 1\n",
        "            norms = tfidf_matrix.sum(dim=1, keepdim=True)\n",
        "            norms = torch.clamp(norms, min=1e-10)\n",
        "            tfidf_matrix = tfidf_matrix / norms\n",
        "\n",
        "        return tfidf_matrix\n",
        "\n",
        "    def fit_transform(self, documents):\n",
        "        \"\"\"Fit and transform in one step\"\"\"\n",
        "        return self.fit(documents).transform(documents)\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# EXAMPLE USAGE\n",
        "# ============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample documents (already tokenized)\n",
        "    documents = [\n",
        "        ['the', 'cat', 'sat', 'on', 'the', 'mat'],\n",
        "        ['the', 'dog', 'sat', 'on', 'the', 'log'],\n",
        "        ['cats', 'and', 'dogs', 'are', 'pets'],\n",
        "        ['the', 'cat', 'and', 'the', 'dog']\n",
        "    ]\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "    print(\"TF-IDF EXAMPLE\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Create and fit TF-IDF\n",
        "    tfidf = TFIDF(norm='l2', smooth_idf=True, sublinear_tf=False)\n",
        "    tfidf_matrix = tfidf.fit_transform(documents)\n",
        "\n",
        "    print(f\"\\nVocabulary size: {len(tfidf.vocab)}\")\n",
        "    print(f\"Number of documents: {len(documents)}\")\n",
        "    print(f\"\\nTF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
        "\n",
        "    # Show IDF weights\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"IDF WEIGHTS (higher = rarer words)\")\n",
        "    print(\"=\" * 60)\n",
        "    sorted_vocab = sorted(tfidf.vocab.items(), key=lambda x: -tfidf.idf[x[1]])\n",
        "    for word, idx in sorted_vocab[:10]:\n",
        "        print(f\"{word:15s} -> IDF: {tfidf.idf[idx]:.4f}\")\n",
        "\n",
        "    # Show TF-IDF for first document\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"DOCUMENT 1: {' '.join(documents[0])}\")\n",
        "    print(\"=\" * 60)\n",
        "    doc1_tfidf = tfidf_matrix[0]\n",
        "\n",
        "    # Get non-zero entries\n",
        "    nonzero_indices = torch.nonzero(doc1_tfidf).squeeze()\n",
        "    vocab_reverse = {v: k for k, v in tfidf.vocab.items()}\n",
        "\n",
        "    print(\"\\nTF-IDF scores:\")\n",
        "    for idx in nonzero_indices:\n",
        "        idx = idx.item()\n",
        "        word = vocab_reverse[idx]\n",
        "        score = doc1_tfidf[idx].item()\n",
        "        print(f\"{word:15s} -> {score:.4f}\")\n",
        "\n",
        "    # Compute similarity between documents\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"COSINE SIMILARITIES\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Cosine similarity (since we used L2 norm, it's just dot product)\n",
        "    similarity_matrix = torch.mm(tfidf_matrix, tfidf_matrix.t())\n",
        "\n",
        "    print(\"\\nDoc 0 vs Doc 1 similarity:\", similarity_matrix[0, 1].item())\n",
        "    print(\"Doc 0 vs Doc 2 similarity:\", similarity_matrix[0, 2].item())\n",
        "    print(\"Doc 0 vs Doc 3 similarity:\", similarity_matrix[0, 3].item())\n",
        "\n",
        "    print(\"\\nFull similarity matrix:\")\n",
        "    print(similarity_matrix.numpy().round(3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRU7xONurfy_",
        "outputId": "89270a1a-e4f0-46b8-cbf2-5cea0f329158"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "TF-IDF EXAMPLE\n",
            "============================================================\n",
            "\n",
            "Vocabulary size: 12\n",
            "Number of documents: 4\n",
            "\n",
            "TF-IDF matrix shape: torch.Size([4, 12])\n",
            "\n",
            "============================================================\n",
            "IDF WEIGHTS (higher = rarer words)\n",
            "============================================================\n",
            "are             -> IDF: 1.9163\n",
            "cats            -> IDF: 1.9163\n",
            "dogs            -> IDF: 1.9163\n",
            "log             -> IDF: 1.9163\n",
            "mat             -> IDF: 1.9163\n",
            "pets            -> IDF: 1.9163\n",
            "and             -> IDF: 1.5108\n",
            "cat             -> IDF: 1.5108\n",
            "dog             -> IDF: 1.5108\n",
            "on              -> IDF: 1.5108\n",
            "\n",
            "============================================================\n",
            "DOCUMENT 1: the cat sat on the mat\n",
            "============================================================\n",
            "\n",
            "TF-IDF scores:\n",
            "cat             -> 0.3719\n",
            "mat             -> 0.4717\n",
            "on              -> 0.3719\n",
            "sat             -> 0.3719\n",
            "the             -> 0.6022\n",
            "\n",
            "============================================================\n",
            "COSINE SIMILARITIES\n",
            "============================================================\n",
            "\n",
            "Doc 0 vs Doc 1 similarity: 0.6391986012458801\n",
            "Doc 0 vs Doc 2 similarity: 0.0\n",
            "Doc 0 vs Doc 3 similarity: 0.5680627226829529\n",
            "\n",
            "Full similarity matrix:\n",
            "[[1.    0.639 0.    0.568]\n",
            " [0.639 1.    0.    0.568]\n",
            " [0.    0.    1.    0.155]\n",
            " [0.568 0.568 0.155 1.   ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UkmCjz_Yrfoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "54pL4VVXrff5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}